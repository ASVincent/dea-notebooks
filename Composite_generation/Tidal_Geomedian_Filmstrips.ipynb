{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import datacube\n",
    "import itertools\n",
    "import warnings\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from otps import TimePoint\n",
    "from otps import predict_tide\n",
    "from datetime import datetime, timedelta\n",
    "from datacube.utils import geometry\n",
    "from datacube.utils.geometry import CRS\n",
    "from datacube_stats.statistics import GeoMedian\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from datacube.helpers import ga_pq_fuser\n",
    "from datacube.helpers import write_geotiff\n",
    "from datacube.storage import masking\n",
    "from shapely.geometry import Point\n",
    "from collections import defaultdict\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from skimage import exposure\n",
    "\n",
    "# Import external functions from dea-notebooks using relative link to Scripts\n",
    "sys.path.append('/g/data/r78/rt1527/dea-notebooks/Scripts')\n",
    "import DEAPlotting\n",
    "\n",
    "# For nicer notebook plotting, hide warnings (comment out for real analysis)\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "# Create datacube instance\n",
    "dc = datacube.Datacube(app='Tidal geomedian filmstrips')\n",
    "\n",
    "def date_range(start_date, end_date, increment, period):\n",
    "    \n",
    "    \"\"\"Generate dates seperated by given time increment/period\"\"\"\n",
    "    \n",
    "    result = []\n",
    "    nxt = start_date\n",
    "    delta = relativedelta(**{period:increment})\n",
    "    while nxt <= end_date:\n",
    "        result.append(nxt)\n",
    "        nxt += delta\n",
    "    return result\n",
    "\n",
    "def ds_to_rgb(ds, bands=['red', 'green', 'blue'], reflect_stand=4000):\n",
    "    \n",
    "    \"\"\"Converts an xarray dataset to a three band array\"\"\"\n",
    "    \n",
    "    rawimg = np.transpose(ds[bands].to_array().data, [1, 2, 0])\n",
    "    img_toshow = (rawimg / reflect_stand).clip(0, 1)\n",
    "    return img_toshow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geomedian filmstrip parameters\n",
    "Set the area, time period  and sensors of interest, and tide limits and epoch length used to produce each geomedian composite:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up centre of study area and buffer size in metres for data extraction\n",
    "study_area = 'gulfcarpentaria'  # name used as prefix for output files\n",
    "lat, lon = -17.58034, 139.81908  # centre of study area\n",
    "buffer = 4000  # metre units to extend region of interest on each side of centre point\n",
    "\n",
    "# Set up compositing parameters\n",
    "time_period = ('1988-01-01', '2018-01-01')  # Total date range to import data from\n",
    "sensors = ['ls5', 'ls7', 'ls8']  # Landsat sensors from which to import data from\n",
    "ls7_slc_off = False  # Whether to include striped > May 31 2003 Landsat 7 SLC-off data\n",
    "lower_tideheight = 0.40  # Minimum proportion of the observed tidal range to include \n",
    "upper_tideheight = 0.60  # Maximum proportion of the observed tidal range to include \n",
    "epoch_years = 6  # Length of each epoch used to compute geomedians (i.e. 5 years = 1988 to 1993)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Return observations for entire time series\n",
    "Use `dask` to lazily load Landsat data and PQ for each sensor for the entire time series. No data is actually loaded at this stage: this is saved until after the layers have been subsetted by date and time to save processing time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 5 epochs: 1988-01-01, 1994-01-01, 2000-01-01, 2006-01-01, 2012-01-01\n",
      "Using tidepost coordinates: 140.26 E -17.52 S\n",
      "\n",
      "Computing tidal heights for 348 ls5 observations\n",
      "Computing tidal heights for 78 ls7 observations\n",
      "Computing tidal heights for 107 ls8 observations\n"
     ]
    }
   ],
   "source": [
    "# If output data and figure directories doesn't exist, create them\n",
    "if not os.path.isdir('output_data/{}/'.format(study_area)):\n",
    "    os.makedirs('output_data/{}/'.format(study_area))\n",
    "    \n",
    "if not os.path.isdir('figures/{}/'.format(study_area)):\n",
    "    os.makedirs('figures/{}/'.format(study_area))\n",
    "\n",
    "# Create list of epochs between start and end of time_period in datetime format\n",
    "start = datetime.strptime(time_period[0], '%Y-%m-%d')\n",
    "end = datetime.strptime(time_period[1], '%Y-%m-%d')\n",
    "epochs = date_range(start, end, epoch_years, 'years')\n",
    "\n",
    "# Print list of epochs\n",
    "epochs_strings = [epoch.strftime('%Y-%m-%d') for epoch in epochs][:-1]\n",
    "print('Processing {} epochs: {}'.format(len(epochs_strings), ', '.join(epochs_strings)))\n",
    "\n",
    "# Identify tide point by importing ITEM polygons\n",
    "polygons_gpd = gpd.read_file('/g/data/r78/intertidal/GA_native_tidal_model.shp')\n",
    "polygon_gpd = polygons_gpd[polygons_gpd.intersects(Point(lon, lat))]\n",
    "tidepost_lon, tidepost_lat = polygon_gpd[['lon', 'lat']].values[0]\n",
    "print('Using tidepost coordinates: {} E {} S\\n'.format(tidepost_lon, tidepost_lat))\n",
    "\n",
    "# Set up query\n",
    "x, y = geometry.point(lon, lat, CRS('WGS84')).to_crs(CRS('EPSG:3577')).points[0]\n",
    "query = {'x': (x - buffer, x + buffer),\n",
    "         'y': (y - buffer, y + buffer),\n",
    "         'time': time_period,\n",
    "         'crs': 'EPSG:3577'}\n",
    "\n",
    "# Output dicts to hold entire time-series for each sensor\n",
    "sensor_dict = {}\n",
    "pq_dict = {}\n",
    "\n",
    "# For each sensor, dask load data and compute tide heights for each sensor\n",
    "for sensor in sensors:\n",
    "    \n",
    "    # The following code will fail if there is no data for the sensor in time_period;\n",
    "    # this try statement catches this and skips the affected sensor    \n",
    "    try:\n",
    "    \n",
    "        # Return observations matching query without actually loading them using dask\n",
    "        sensor_all = dc.load(product = '{}_nbart_albers'.format(sensor), \n",
    "                         group_by = 'solar_day', \n",
    "                         dask_chunks={'time': 1},\n",
    "                         **query)\n",
    "\n",
    "        # Load PQ data matching query without actually loading them using dask\n",
    "        pq_all = dc.load(product = '{}_pq_albers'.format(sensor),\n",
    "                        group_by = 'solar_day',\n",
    "                        fuse_func=ga_pq_fuser, \n",
    "                        dask_chunks={'time': 1},\n",
    "                        **query)\n",
    "        \n",
    "        # Remove Landsat 7 SLC-off from PQ layer if ls7_slc_off = False\n",
    "        if not ls7_slc_off and sensor == 'ls7':\n",
    "            sensor_all = sensor_all.where(sensor_all.time < np.datetime64('2003-05-30'), drop=True) \n",
    "\n",
    "        # Use the tidal mode to compute tide heights for each observation:\n",
    "        print('Computing tidal heights for {} {} observations'.format(len(sensor_all.time), sensor))\n",
    "        obs_datetimes = sensor_all.time.data.astype('M8[s]').astype('O').tolist()\n",
    "        obs_timepoints = [TimePoint(tidepost_lon, tidepost_lat, dt) for dt in obs_datetimes]\n",
    "        obs_predictedtides = predict_tide(obs_timepoints)\n",
    "        obs_tideheights = [predictedtide.tide_m for predictedtide in obs_predictedtides]\n",
    "\n",
    "        # Assign tide heights to the dataset as a new variable\n",
    "        sensor_all['tide_heights'] = xr.DataArray(obs_tideheights, [('time', sensor_all.time)]) \n",
    "\n",
    "        # Append results for each sensor to a dictionary with sensor name as the key\n",
    "        sensor_dict[sensor] = sensor_all\n",
    "        pq_dict[sensor] = pq_all \n",
    "    \n",
    "    except AttributeError:\n",
    "        \n",
    "        print('{} selection failed due to likely lack of data in time_period; skipping'.format(sensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset data by tidal height and epoch time range\n",
    "Use image time-stamps to compute tidal heights at the time of each observation, and take a subset of the entire Landsat and PQ time series based on tide height and epoch date range for each sensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysing tidal heights of -0.14 m to 0.42 m out of an observed local tidal range of -1.24 m to 1.52 m\n",
      "    Filtering from 1988-01-01 to 1994-01-01 for ls5\n",
      "    Filtering from 1988-01-01 to 1994-01-01 for ls7\n",
      "    Filtering from 1988-01-01 to 1994-01-01 for ls8\n",
      "    Filtering from 1994-01-01 to 2000-01-01 for ls5\n",
      "    Filtering from 1994-01-01 to 2000-01-01 for ls7\n",
      "    Filtering from 1994-01-01 to 2000-01-01 for ls8\n",
      "    Filtering from 2000-01-01 to 2006-01-01 for ls5\n",
      "    Filtering from 2000-01-01 to 2006-01-01 for ls7\n",
      "    Filtering from 2000-01-01 to 2006-01-01 for ls8\n",
      "    Filtering from 2006-01-01 to 2012-01-01 for ls5\n",
      "    Filtering from 2006-01-01 to 2012-01-01 for ls7\n",
      "    Filtering from 2006-01-01 to 2012-01-01 for ls8\n",
      "    Filtering from 2012-01-01 to 2018-01-01 for ls5\n",
      "    Filtering from 2012-01-01 to 2018-01-01 for ls7\n",
      "    Filtering from 2012-01-01 to 2018-01-01 for ls8\n"
     ]
    }
   ],
   "source": [
    "# If any sensor has 0 observations, remove it from the dictionary before proceeding\n",
    "sensor_dict = {key:value for key, value in sensor_dict.items() if len(value.time) > 0}\n",
    "\n",
    "# Calculate max and min tide heights for the entire time series and all sensors\n",
    "obs_min = np.min([sensor_ds.tide_heights.min() for sensor_ds in sensor_dict.values()])\n",
    "obs_max = np.max([sensor_ds.tide_heights.max() for sensor_ds in sensor_dict.values()])\n",
    "obs_range = obs_max - obs_min\n",
    "\n",
    "# Calculate tidal limits used for subsequent data selection\n",
    "sel_min = obs_min + (obs_range * lower_tideheight)\n",
    "sel_max = obs_min + (obs_range * upper_tideheight)\n",
    "print('Analysing tidal heights of {0:.2f} m to {1:.2f} m out of an observed local tidal ' \n",
    "      'range of {2:.2f} m to {3:.2f} m'.format(sel_min, sel_max, obs_min, obs_max))\n",
    "\n",
    "# Create dictionaries to hold filtered sensor data for each epoch\n",
    "sensor_epoch_dict = defaultdict(list)\n",
    "pq_epoch_dict = defaultdict(list)\n",
    "   \n",
    "for epoch, sensor in itertools.product(epochs[:-1], sensor_dict.keys()):\n",
    "                                           \n",
    "    # Select dataset\n",
    "    sensor_all = sensor_dict[sensor]  \n",
    "    pq_all = pq_dict[sensor] \n",
    "    \n",
    "    # Filter to keep only observations that have matching PQ data \n",
    "    time = (sensor_all.time - pq_all.time).time\n",
    "    sensor_subset = sensor_all.sel(time=time)\n",
    "    pq_subset = pq_all.sel(time=time)\n",
    "                                           \n",
    "    # Filter by tidal stage\n",
    "    sensor_subset = sensor_subset.where((sensor_subset.tide_heights >= sel_min) &\n",
    "                                        (sensor_subset.tide_heights <= sel_max), drop = True)\n",
    "    \n",
    "    # Filter pq to same timesteps (this avoids conversion to float by `.where`)\n",
    "    pq_subset = pq_subset.sel(time = sensor_subset.time)\n",
    "\n",
    "    # Identify from and to date strings\n",
    "    from_date = epoch.strftime('%Y-%m-%d')\n",
    "    to_date = (epoch + relativedelta(years=epoch_years)).strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Subset sensor to from and to date for epoch\n",
    "    print('    Filtering from {} to {} for {}'.format(from_date, to_date, sensor))\n",
    "    sensor_subset = sensor_subset.sel(time=slice(from_date, to_date))\n",
    "    pq_subset = pq_subset.sel(time=slice(from_date, to_date))\n",
    "\n",
    "    # Add subsetted data to dicts (one key matching a list of sensor data for each epoch)\n",
    "    sensor_epoch_dict[from_date].append(sensor_subset)\n",
    "    pq_epoch_dict[from_date].append(pq_subset)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine multiple sensors, load data and generate geomedians\n",
    "For each epoch, combine all sensors into one dataset, load the data for the first time using `dask`'s `.compute()`, then composite all timesteps into a single array using a geometric median computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and combining ls5, ls7, ls8 data for 1988-01-01\n",
      "    21 observations combined\n",
      "    Applying PQ mask and setting nodata to NaN\n",
      "    Computing geometric median\n",
      "    Exporting to output_data/gulfcarpentaria/gulfcarpentaria_1988-01-01.tif\n",
      "Loading and combining ls5, ls7, ls8 data for 1994-01-01\n",
      "    28 observations combined\n",
      "    Applying PQ mask and setting nodata to NaN\n",
      "    Computing geometric median\n",
      "    Exporting to output_data/gulfcarpentaria/gulfcarpentaria_1994-01-01.tif\n",
      "Loading and combining ls5, ls7, ls8 data for 2000-01-01\n",
      "    28 observations combined\n",
      "    Applying PQ mask and setting nodata to NaN\n",
      "    Computing geometric median\n",
      "    Exporting to output_data/gulfcarpentaria/gulfcarpentaria_2000-01-01.tif\n",
      "Loading and combining ls5, ls7, ls8 data for 2006-01-01\n"
     ]
    }
   ],
   "source": [
    "# Dict to hold output geomedian composits\n",
    "geomedian_dict = {}\n",
    "\n",
    "for from_date in list(sensor_epoch_dict.keys()):\n",
    "    \n",
    "    # For the first time, actually load data for all sensors and then combine into one dataset  \n",
    "    print('Loading and combining {} data for {}'.format(\", \".join(sensor_dict.keys()), from_date))\n",
    "    sensor_combined = xr.concat([i.compute() for i in sensor_epoch_dict[from_date]], dim='time')\n",
    "    pq_combined = xr.concat([i.compute() for i in pq_epoch_dict[from_date]], dim='time')\n",
    "    print('    {} observations combined'.format(len(sensor_combined.time)))\n",
    "    \n",
    "    # Manually add flag definition back in (it is lost during the concatenation)\n",
    "    pq_combined.pixelquality.attrs['flags_definition'] = pq_all.pixelquality.flags_definition\n",
    "    \n",
    "    # Sort output datasets by date\n",
    "    sensor_combined = sensor_combined.sortby('time')\n",
    "    pq_combined = pq_combined.sortby('time')\n",
    "    \n",
    "    # Identify pixels with no clouds/shadows in either ACCA for Fmask\n",
    "    good_quality = masking.make_mask(pq_combined.pixelquality,\n",
    "                                     cloud_acca='no_cloud',\n",
    "                                     cloud_shadow_acca='no_cloud_shadow',\n",
    "                                     cloud_shadow_fmask='no_cloud_shadow',\n",
    "                                     cloud_fmask='no_cloud',\n",
    "                                     blue_saturated=False,\n",
    "                                     green_saturated=False,\n",
    "                                     red_saturated=False,\n",
    "                                     nir_saturated=False,\n",
    "                                     swir1_saturated=False,\n",
    "                                     swir2_saturated=False,\n",
    "                                     contiguous=True)\n",
    "    \n",
    "    # Apply mask to set all PQ-affected pixels to NaN and set nodata to NaN\n",
    "    print('    Applying PQ mask and setting nodata to NaN')\n",
    "    sensor_combined = sensor_combined.where(good_quality)\n",
    "    sensor_combined = masking.mask_invalid_data(sensor_combined)\n",
    "\n",
    "    # Compute geomedian composite using all timesteps\n",
    "    print('    Computing geometric median')\n",
    "    geomedian_ds = GeoMedian().compute(sensor_combined)\n",
    "    \n",
    "    # Export to file\n",
    "    filename = 'output_data/{0}/{0}_{1}.tif'.format(study_area, from_date)\n",
    "    print('    Exporting to {}'.format(filename))\n",
    "    write_geotiff(filename=filename, dataset=geomedian_ds)\n",
    "    \n",
    "    # Assign to dict\n",
    "    geomedian_dict[from_date] = geomedian_ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot geomedians as filmstrips\n",
    "Plots geomedians for each epoch into single true and false-colour filmstrip plots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up dict of band combinations to iterate through to produce filmstrip plots\n",
    "band_dict = {'falsecolour': ['swir1', 'nir', 'green'],\n",
    "             'truecolour': ['red', 'green', 'blue']}\n",
    "\n",
    "# Load ITEM confidence layer that matches dataset and export to file\n",
    "item_data = dc.load(product='item_v2_conf', **query)\n",
    "write_geotiff(filename='output_data/{0}/{0}_itemconf.tif'.format(study_area), \n",
    "              dataset=item_data.squeeze(dim='time'),\n",
    "              profile_override={'nodata': -6666})\n",
    "\n",
    "# Create a filmstrip plot for each band combination\n",
    "for name, bands in band_dict.items():\n",
    "\n",
    "    # Compute number of epochs for figure construction\n",
    "    epochs_n = len(geomedian_dict.keys())\n",
    "    print('Generating {}-panel {} filmstrip plot'.format(1 + epochs_n, name))\n",
    "\n",
    "    # Set up plot with multiple columns (ITEM confidence + n epochs)\n",
    "    fig, axes = plt.subplots(ncols=1 + epochs_n, nrows=1, \n",
    "                             figsize=(15, 15), sharex=True, sharey=True) \n",
    "    \n",
    "    # Remove space between subplots\n",
    "    fig.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "    # Plot ITEM confidence in first column\n",
    "    axes[0].imshow(item_data.isel(time=0).stddev.values, cmap='RdYlBu_r', \n",
    "                   vmin=0.05, vmax=0.35, interpolation='bilinear')\n",
    "    axes[0].set_title(\"ITEM confidence\", fontsize=6)\n",
    "    \n",
    "    # Remove axes ticks and space on left and bottom of plot\n",
    "    axes[0].get_xaxis().set_visible(False)\n",
    "    axes[0].get_yaxis().set_visible(False)\n",
    "\n",
    "    # Progressively add epoch geomedians to remaining subplot columns\n",
    "    for i, from_date in enumerate(sensor_epoch_dict.keys()): \n",
    "        \n",
    "        # Compute upper date bound for plot title\n",
    "        to_date = (datetime.strptime(from_date, '%Y-%m-%d') + \\\n",
    "                   relativedelta(years=epoch_years)).strftime('%Y-%m-%d')\n",
    "\n",
    "        # Geomedian for given epoch \n",
    "        epoch_ds = geomedian_dict[from_date]\n",
    "        \n",
    "        # Convert ds to three band array, using a different reflect_stand\n",
    "        # value for True and False colour to produce bright/vibrant plots\n",
    "        epoch_array = ds_to_rgb(epoch_ds, bands=bands, \n",
    "                                reflect_stand={'falsecolour': 4000, \n",
    "                                               'truecolour': 2500}[name])\n",
    "\n",
    "        # Plot data into column\n",
    "        axes[i + 1].imshow(epoch_array, interpolation='bilinear')\n",
    "        axes[i + 1].set_title('{} to {}'.format(from_date, to_date), fontsize=6)\n",
    "        \n",
    "        # Remove axes ticks and space on left and bottom of plot\n",
    "        axes[i + 1].get_xaxis().set_visible(False)\n",
    "        axes[i + 1].get_yaxis().set_visible(False)\n",
    "\n",
    "    # Export combined figure to file\n",
    "    filename = 'figures/{0}/{0}_{1}.png'.format(study_area, name)\n",
    "    print('    Exporting to {}'.format(filename))\n",
    "    fig.savefig(filename, dpi=300, bbox_inches='tight', pad_inches=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
