{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to resolve driver datacube.plugins.index::s3aio_index\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import datacube\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from otps import TimePoint\n",
    "from otps import predict_tide\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "from datacube.utils import geometry\n",
    "from datacube.utils.geometry import CRS\n",
    "from datacube_stats.statistics import GeoMedian\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from datacube.helpers import ga_pq_fuser\n",
    "from datacube.storage import masking\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "\n",
    "# Import external functions from dea-notebooks using relative link to Scripts\n",
    "sys.path.append('/g/data/r78/rt1527/dea-notebooks/Scripts')\n",
    "import DEAPlotting\n",
    "\n",
    "# Create datacube instance\n",
    "dc = datacube.Datacube(app='Tidal tagging')\n",
    "\n",
    "def date_range(start_date, end_date, increment, period):\n",
    "    \n",
    "    \"\"\"Generate dates seperated by given time increment/period\"\"\"\n",
    "    \n",
    "    result = []\n",
    "    nxt = start_date\n",
    "    delta = relativedelta(**{period:increment})\n",
    "    while nxt <= end_date:\n",
    "        result.append(nxt)\n",
    "        nxt += delta\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up analysis data query\n",
    "lat, lon = -13.4225701889, 130.28851991\n",
    "\n",
    "# Set a tide post: this is the location the OTPS model uses to compute tides for the supplied datetimes\n",
    "tidepost_lat, tidepost_lon = -13.3079256385, 130.187740555\n",
    "\n",
    "# Set tidal lims\n",
    "lower_tideheight = 0.40\n",
    "upper_tideheight = 0.60\n",
    "\n",
    "# Set times\n",
    "time_period = ('1988-01-01', '2018-01-01')\n",
    "epoch_years = 5\n",
    "\n",
    "# Set sensors\n",
    "sensors = ['ls5', 'ls7', 'ls8']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "Three sensors\n",
    "X time periods\n",
    "\n",
    "Filter by tide height\n",
    "Filter by date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each epoch between start and end of timeperiod, create list of datetimes\n",
    "start = datetime.strptime(time_period[0], \"%Y-%m-%d\")\n",
    "end = datetime.strptime(time_period[1], \"%Y-%m-%d\")\n",
    "epochs = date_range(start, end, epoch_years, 'years') \n",
    "\n",
    "# Set up query\n",
    "x, y = geometry.point(lon, lat, CRS('WGS84')).to_crs(CRS('EPSG:3577')).points[0]\n",
    "query = {'x': (x - 5000, x + 5000),\n",
    "         'y': (y - 5000, y + 5000),\n",
    "         'time': time_period,\n",
    "         'crs': 'EPSG:3577'}\n",
    "\n",
    "# Output dicts to hold entire time-series for each sensor\n",
    "sensor_dict = {}\n",
    "pq_dict = {}\n",
    "\n",
    "# For each sensor, dask load data and compute tide heights for each sensor\n",
    "for sensor in sensors:\n",
    "    \n",
    "    # Return observations that match our query without actually loading them using dask\n",
    "    sensor_all = dc.load(product = '{}_nbart_albers'.format(sensor), \n",
    "                     group_by = 'solar_day', \n",
    "                     dask_chunks={'time': 1},\n",
    "                     **query)\n",
    "\n",
    "    # Load PQ data\n",
    "    pq_all = dc.load(product = '{}_pq_albers'.format(sensor),\n",
    "                    group_by = 'solar_day',\n",
    "                    fuse_func=ga_pq_fuser, \n",
    "                    dask_chunks={'time': 1},\n",
    "                    **query)\n",
    "\n",
    "    # Return Landsat observations that have matching PQ data \n",
    "    time = (sensor_all.time - pq_all.time).time\n",
    "    sensor_all = sensor_all.sel(time=time)\n",
    "    pq_all = pq_all.sel(time=time)\n",
    "    \n",
    "    # Use the tidal mode to extract tide heights for each observation:\n",
    "    obs_datetimes = sensor_all.time.data.astype('M8[s]').astype('O').tolist()\n",
    "    obs_timepoints = [TimePoint(tidepost_lon, tidepost_lat, dt) for dt in obs_datetimes]\n",
    "    obs_predictedtides = predict_tide(obs_timepoints)\n",
    "    obs_tideheights = [predictedtide.tide_m for predictedtide in obs_predictedtides]\n",
    "\n",
    "    # Assign these tide heights back into the dataset:\n",
    "    sensor_all['tide_heights'] = xr.DataArray(obs_tideheights, [('time', sensor_all.time)])  \n",
    "    \n",
    "    # Append to output datasets\n",
    "    sensor_dict[sensor] = sensor_all\n",
    "    pq_dict[sensor] = pq_all   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysing tidal heights of -0.58 m to 0.40 m out of an observed local tidal range of -2.55 m to 2.38 m\n"
     ]
    }
   ],
   "source": [
    "# Calculate max and min tide heights\n",
    "obs_min = np.min([sensor_ds.tide_heights.min() for sensor_ds in sensor_dict.values()])\n",
    "obs_max = np.max([sensor_ds.tide_heights.max() for sensor_ds in sensor_dict.values()])\n",
    "obs_range = obs_max - obs_min\n",
    "\n",
    "# Calculate tidal limits used for data selection\n",
    "sel_min = obs_min + (obs_range * lower_tideheight)\n",
    "sel_max = obs_min + (obs_range * upper_tideheight)\n",
    "print('Analysing tidal heights of {0:.2f} m to {1:.2f} m out of an observed local tidal ' \n",
    "      'range of {2:.2f} m to {3:.2f} m'.format(sel_min, sel_max, obs_min, obs_max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = defaultdict(list)\n",
    "   \n",
    "for epoch, sensor in itertools.product(epochs[:-1], sensor_dict.keys()):\n",
    "                                           \n",
    "    # Select dataset\n",
    "    sensor_ds = sensor_dict[sensor]    \n",
    "    pq_ds = pq_dict[sensor]   \n",
    "                                           \n",
    "    # Filter by tidal stage\n",
    "    sensor_subset = sensor_ds.where((sensor_ds.tide_heights >= sel_min) & \n",
    "                                       (sensor_ds.tide_heights <= sel_max), drop = True)    \n",
    "    pq_subset = pq_ds.where((sensor_ds.tide_heights >= sel_min) & \n",
    "                               (sensor_ds.tide_heights <= sel_max), drop = True)\n",
    "\n",
    "    # Identify from and to date strings\n",
    "    from_date = epoch.strftime('%Y-%m-%d')\n",
    "    to_date = (epoch + relativedelta(years=epoch_years)).strftime('%Y-%m-%d')\n",
    "    \n",
    "    print('Filtering {} from {} to {}'.format(sensor, from_date, to_date))\n",
    "    sensor_subset = sensor_subset.sel(time=slice(from_date, to_date)) \n",
    "    pq_subset = pq_subset.sel(time=slice(from_date, to_date)) \n",
    "\n",
    "    # Add to dict\n",
    "    data_dict[from_date].append(sel_ds)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for from_date in ['2013-01-01']:\n",
    "       \n",
    "    combined_ds = xr.concat([i.compute() for i in data_dict[from_date]], dim='time')\n",
    "    combined_ds = combined_ds.sortby('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict[from_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = geometry.point(lon, lat, CRS('WGS84')).to_crs(CRS('EPSG:3577')).points[0]\n",
    "query = {'x': (x - 5000, x + 5000),\n",
    "         'y': (y - 5000, y + 5000),         \n",
    "         'crs': 'EPSG:3577',\n",
    "         'time': time_period}\n",
    "\n",
    "# Return observations that match our query without actually loading them using dask\n",
    "obs_ds = dc.load(product = 'ls5_nbart_albers', \n",
    "                 group_by = 'solar_day', \n",
    "                 dask_chunks={'time': 1}, \n",
    "                 **query)\n",
    "\n",
    "# Load PQ data\n",
    "pq = dc.load(product = 'ls5_pq_albers',\n",
    "             group_by = 'solar_day',\n",
    "             fuse_func=ga_pq_fuser, \n",
    "             dask_chunks={'time': 1}, \n",
    "             **query)\n",
    "\n",
    "# Return Landsat observations that have matching PQ data \n",
    "time = (obs_ds.time - pq.time).time\n",
    "obs_ds = obs_ds.sel(time=time)\n",
    "pq = pq.sel(time=time)\n",
    "print(pq, obs_ds)\n",
    "\n",
    "# Use the tidal mode to extract tide heights for each observation:\n",
    "obs_datetimes = obs_ds.time.data.astype('M8[s]').astype('O').tolist()\n",
    "obs_timepoints = [TimePoint(tidepost_lon, tidepost_lat, dt) for dt in obs_datetimes]\n",
    "obs_predictedtides = predict_tide(obs_timepoints)\n",
    "obs_tideheights = [predictedtide.tide_m for predictedtide in obs_predictedtides]\n",
    "\n",
    "# Assign these tide heights back into the dataset:\n",
    "obs_ds['tide_heights'] = xr.DataArray(obs_tideheights, [('time', obs_ds.time)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract observed tidal height ranges\n",
    "obs_min, obs_max = obs_ds['tide_heights'].quantile(q=[0.0, 1.0]).values\n",
    "obs_range = obs_max - obs_min\n",
    "\n",
    "# Calculate tidal limits used for data selection\n",
    "sel_min = obs_min + (obs_range * lower_tideheight)\n",
    "sel_max = obs_min + (obs_range * upper_tideheight)\n",
    "print('Analysing tidal heights of {0:.2f} m to {1:.2f} m out of an observed local tidal ' \n",
    "      'range of {2:.2f} m to {3:.2f} m'.format(sel_min, sel_max, obs_min, obs_max))\n",
    "\n",
    "# Filter by tidal stage\n",
    "tide_ds = obs_ds.where((obs_ds.tide_heights >= sel_min) & \n",
    "                       (obs_ds.tide_heights <= sel_max), drop = True)\n",
    "\n",
    "tide_pq = pq.sel(time = tide_ds.time)\n",
    "print(tide_ds, tide_pq)\n",
    "\n",
    "# For each epoch between start and end of timeperiod, create list of datetimes\n",
    "start = datetime.strptime(time_period[0], \"%Y-%m-%d\")\n",
    "end = datetime.strptime(time_period[1], \"%Y-%m-%d\")\n",
    "epochs = date_range(start, end, epoch, 'years') \n",
    "\n",
    "for from_date in epochs[:-1]:\n",
    "    \n",
    "    # Filter by time period\n",
    "    to_date = from_date + relativedelta(years=epoch)\n",
    "    print('Filtering from {} to {}'.format(from_date.strftime('%Y-%m-%d'), to_date.strftime('%Y-%m-%d')))\n",
    "    sel_ds = tide_ds.sel(time=slice(from_date.strftime('%Y-%m-%d'), to_date.strftime('%Y-%m-%d'))) \n",
    "    sel_pq = tide_pq.sel(time=slice(from_date.strftime('%Y-%m-%d'), to_date.strftime('%Y-%m-%d'))) \n",
    "    \n",
    "    # Run `compute()` to load only our filtered datasets:\n",
    "    sel_ds = sel_ds.compute()\n",
    "    sel_pq = sel_pq.compute()\n",
    "    \n",
    "    # Identify pixels with no clouds/shadows in either ACCA for Fmask\n",
    "    good_quality = masking.make_mask(sel_pq.pixelquality,\n",
    "                                     cloud_acca='no_cloud',\n",
    "                                     cloud_shadow_acca='no_cloud_shadow',\n",
    "                                     cloud_shadow_fmask='no_cloud_shadow',\n",
    "                                     cloud_fmask='no_cloud',\n",
    "                                     blue_saturated=False,\n",
    "                                     green_saturated=False,\n",
    "                                     red_saturated=False,\n",
    "                                     nir_saturated=False,\n",
    "                                     swir1_saturated=False,\n",
    "                                     swir2_saturated=False,\n",
    "                                     contiguous=True)\n",
    "    \n",
    "    sel_ds = sel_ds.where(good_quality)\n",
    "\n",
    "    # Now we can take the median of each set of low and high tide observations to produce a composite:\n",
    "#     geomedian_ds = sel_ds.median(dim = 'time', keep_attrs = True)\n",
    "    geomedian_ds = GeoMedian().compute(sel_ds)\n",
    "    fig, ax = DEAPlotting.three_band_image(ds=geomedian_ds, bands=['swir1', 'nir', 'green'], \n",
    "                                           title = 'Geomedian', reflect_stand=4000)\n",
    "    fig.savefig('output_{}.png'.format(from_date.strftime('%Y-%m-%d')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pq.where(obs_ds.tide_heights >= sel_min, drop = True)\n",
    "pq.sel(time=tide_ds.time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = geometry.point(lon, lat, CRS('WGS84')).to_crs(CRS('EPSG:3577')).points[0]\n",
    "query = {'x': (x - 5000, x + 5000),\n",
    "         'y': (y - 5000, y + 5000),         \n",
    "         'crs': 'EPSG:3577',\n",
    "         'time': ('1988-01-01', '1990-01-01')}\n",
    "\n",
    "# Load PQ data\n",
    "pq = dc.load(product = 'ls5_pq_albers',\n",
    "             group_by = 'solar_day',\n",
    "             fuse_func=ga_pq_fuser, \n",
    "             **query)\n",
    "\n",
    "masking.make_mask(pq['pixelquality'],\n",
    "                 cloud_acca='no_cloud',\n",
    "                 cloud_shadow_acca='no_cloud_shadow',\n",
    "                 cloud_shadow_fmask='no_cloud_shadow',\n",
    "                 cloud_fmask='no_cloud',\n",
    "                 blue_saturated=False,\n",
    "                 green_saturated=False,\n",
    "                 red_saturated=False,\n",
    "                 nir_saturated=False,\n",
    "                 swir1_saturated=False,\n",
    "                 swir2_saturated=False,\n",
    "                 contiguous=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pq "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
