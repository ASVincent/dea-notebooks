{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "from glob import glob\n",
    "import sys\n",
    "import pandas as pd\n",
    "import otps\n",
    "import os\n",
    "from datetime import datetime\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from datetime import timedelta\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import ndimage as nd\n",
    "import matplotlib.pyplot as plt\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "import datacube\n",
    "from datacube.utils import geometry\n",
    "from datacube.utils.geometry import CRS\n",
    "\n",
    "def lag_linregress_3D(x, y, lagx=0, lagy=0, first_dim='time'):\n",
    "    \n",
    "    \"\"\"\n",
    "    Takes two xr.Datarrays of any dimensions (input data could be a 1D time series, or for example, have \n",
    "    three dimensions e.g. time, lat, lon), and return covariance, correlation, regression slope and intercept, \n",
    "    p-value, and standard error on regression between the two datasets along their aligned first dimension.  \n",
    "    \n",
    "    Datasets can be provided in any order, but note that the regression slope and intercept will be calculated\n",
    "    for y with respect to x.\n",
    "    \n",
    "    Parameters\n",
    "    ----------    \n",
    "    x, y : xarray DataArray\n",
    "        Two xarray DataArrays with any number of dimensions, both sharing the same first dimension\n",
    "    lagx, lagy : int, optional\n",
    "        Optional integers giving lag values to assign to either of the data, with lagx shifting x, and lagy \n",
    "        shifting y with the specified lag amount. \n",
    "    first_dim : str, optional\n",
    "        An optional string giving the name of the first dimension on which to align datasets. The default is\n",
    "        'time'.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    cov, cor, slope, intercept, pval, stderr : xarray DataArray\n",
    "        Covariance, correlation, regression slope and intercept, p-value, and standard error on \n",
    "        regression between the two datasets along their aligned first dimension.  \n",
    "\n",
    "    \"\"\" \n",
    "    \n",
    "    #1. Ensure that the data are properly alinged to each other. \n",
    "    x, y = xr.align(x, y)\n",
    "    \n",
    "    #2. Add lag information if any, and shift the data accordingly\n",
    "    if lagx != 0:\n",
    "        \n",
    "        # If x lags y by 1, x must be shifted 1 step backwards. But as the 'zero-th' value is nonexistant, xr \n",
    "        # assigns it as invalid (nan). Hence it needs to be dropped:\n",
    "        x = x.shift(**{first_dim: -lagx}).dropna(dim=first_dim)\n",
    "        \n",
    "        # Next re-align the two datasets so that y adjusts to the changed coordinates of x:\n",
    "        x,y = xr.align(x, y)\n",
    "\n",
    "    if lagy!=0:\n",
    "        \n",
    "        y = y.shift(**{first_dim: -lagy}).dropna(dim=first_dim)\n",
    "        x, y = xr.align(x, y)\n",
    " \n",
    "    #3. Compute data length, mean and standard deviation along time axis for further use: \n",
    "    n = y.notnull().sum(dim=first_dim)\n",
    "    xmean = x.mean(axis=0)\n",
    "    ymean = y.mean(axis=0)\n",
    "    xstd = x.std(axis=0)\n",
    "    ystd = y.std(axis=0)\n",
    "    \n",
    "    #4. Compute covariance along first axis\n",
    "    cov = np.sum((x - xmean) * (y - ymean), axis=0) / (n)\n",
    "    \n",
    "    #5. Compute correlation along time axis\n",
    "    cor = cov / (xstd * ystd)\n",
    "    \n",
    "    #6. Compute regression slope and intercept:\n",
    "    slope = cov / (xstd**2)\n",
    "    intercept = ymean - xmean*slope  \n",
    "    \n",
    "    #7. Compute P-value and standard error\n",
    "    #Compute t-statistics\n",
    "    tstats = cor*np.sqrt(n - 2) / np.sqrt(1 - cor**2)\n",
    "    stderr = slope/tstats\n",
    "    \n",
    "    from scipy.stats import t\n",
    "    pval = t.sf(tstats, n - 2) * 2\n",
    "    pval = xr.DataArray(pval, dims=cor.dims, coords=cor.coords)\n",
    "\n",
    "    return cov, cor, slope, intercept, pval, stderr\n",
    "\n",
    "\n",
    "def date_range(start_date, end_date, increment, period):\n",
    "    \n",
    "    \"\"\"Generate dates seperated by given time increment/period\"\"\"\n",
    "    \n",
    "    result = []\n",
    "    nxt = start_date\n",
    "    delta = relativedelta(**{period:increment})\n",
    "    while nxt <= end_date:\n",
    "        result.append(nxt)\n",
    "        nxt += delta\n",
    "    return result\n",
    "\n",
    "\n",
    "# Extract vertex coordinates and heights from geopandas\n",
    "def contours_to_arrays(gdf, col):\n",
    "\n",
    "    coords_zvals = []\n",
    "\n",
    "    for i in range(1, len(gdf)):\n",
    "\n",
    "        val = gdf.iloc[i][col]\n",
    "\n",
    "        try:\n",
    "            coords = np.concatenate([np.vstack(x.coords.xy).T for x in gdf.iloc[i].geometry])\n",
    "\n",
    "        except:\n",
    "            coords = np.vstack(gdf.iloc[i].geometry.coords.xy).T\n",
    "\n",
    "        coords_zvals.append(np.column_stack((coords, np.full(np.shape(coords)[0], fill_value=val))))\n",
    "\n",
    "    return np.concatenate(coords_zvals)\n",
    "\n",
    "\n",
    "# Import external dea-notebooks functions using relative link to Scripts directory\n",
    "sys.path.append('../10_Scripts')\n",
    "import DEADataHandling, SpatialTools, DEAPlotting, BandIndices\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"width:100%;\"><div style=\"position:relative;width:100%;height:0;padding-bottom:60%;\"><iframe src=\"data:text/html;charset=utf-8;base64,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\" style=\"position:absolute;width:100%;height:100%;left:0;top:0;border:none !important;\" allowfullscreen webkitallowfullscreen mozallowfullscreen></iframe></div></div>"
      ],
      "text/plain": [
       "<folium.folium.Map at 0x7f85cb9c8cc0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set coords\n",
    "# lat, lon, study_area = -14.797361, 128.466634, \"cambridge\"  \n",
    "# lat, lon, study_area = -24.75, 152.353044, \"burnett\" \n",
    "# lat, lon, study_area = -15.19733, 141.59417, 'mitchell'\n",
    "lat, lon, study_area = -15.9448, 124.1996, 'doubtfulbay'\n",
    "time_range = ('1986-06-01', '2018-12-31')\n",
    "\n",
    "# Create datacube instance\n",
    "dc = datacube.Datacube(app='Tidal tagging')\n",
    "\n",
    "# Set up analysis data query\n",
    "x, y = geometry.point(lon, lat, CRS('WGS84')).to_crs(CRS('EPSG:3577')).points[0]\n",
    "query = {'x': (x - 6000, x + 5000),\n",
    "         'y': (y - 4000, y + 4500),         \n",
    "         'crs': 'EPSG:3577',\n",
    "         'resolution': (-25, 25),\n",
    "         'output_crs': 'EPSG:3577',\n",
    "         'time': time_range}\n",
    "\n",
    "DEAPlotting.display_map(x=query['x'], y=query['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in only clear Landsat observations with < 1% unclear values\n",
    "data = DEADataHandling.load_clearlandsat(dc=dc, query=query, \n",
    "                                       bands_of_interest=['red', 'green', 'blue', 'nir', 'swir1', 'swir2'], \n",
    "                                       masked_prop=0.0, mask_pixel_quality=True) \n",
    "\n",
    "# Compute water indices\n",
    "data['NDWI'] = BandIndices.calculate_indices(data, 'NDWI')\n",
    "# data['MNDWI'] = BandIndices.calculate_indices(data, 'MNDWI')\n",
    "# data['WI'] = BandIndices.calculate_indices(data, 'WI')\n",
    "# data['AWEI_noshadow'] = BandIndices.calculate_indices(data, 'AWEI_noshadow')\n",
    "# data['AWEI_shadow'] = BandIndices.calculate_indices(data, 'AWEI_shadow')\n",
    "\n",
    "data = data.astype(np.float32)\n",
    "data.NDWI.isel(time = 5).plot(size=10)\n",
    "                                                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute tides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate old tide heights\n",
    "observed_datetimes = data.time.data.astype('M8[s]').astype('O').tolist()\n",
    "observed_timepoints = [otps.TimePoint(lon, lat, dt) for dt in observed_datetimes]\n",
    "observed_predictedtides = otps.predict_tide(observed_timepoints)\n",
    "tideheights_old = [predictedtide.tide_m for predictedtide in observed_predictedtides]\n",
    "data['tideheights_old'] = xr.DataArray(tideheights_old, [('time', data.time)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rolling NIDEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_period = ('1987-01-01', '2018-01-01')\n",
    "epoch_years = 10\n",
    "\n",
    "# Create list of epochs between start and end of time_period in datetime format\n",
    "start = datetime.strptime(time_period[0], '%Y-%m-%d')\n",
    "end = datetime.strptime(time_period[1], '%Y-%m-%d')\n",
    "epochs = date_range(start, end, epoch_years, 'years')\n",
    "\n",
    "# Print list of epochs\n",
    "epochs_strings = [epoch.strftime('%Y-%m-%d') for epoch in epochs][:-1]\n",
    "print('Processing {} epochs: {}'.format(len(epochs_strings), ', '.join(epochs_strings)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If output data directory doesn't exist, create it\n",
    "if not os.path.isdir(f'output_data/rolling_outputs/{study_area}/'):\n",
    "    os.makedirs(f'output_data/rolling_outputs/{study_area}/')\n",
    "    \n",
    "# Create output dict\n",
    "dem_dict = {}\n",
    "intertidal_mask_dict = {}\n",
    "\n",
    "for epoch in epochs[:-1]:\n",
    "    \n",
    "    # Identify from and to date strings\n",
    "    from_date = epoch.strftime('%Y-%m-%d')\n",
    "    to_date = (epoch + relativedelta(years=epoch_years)).strftime('%Y-%m-%d')    \n",
    "    print(from_date, to_date)   \n",
    "    \n",
    "\n",
    "    ###################\n",
    "    # Convert to tide #\n",
    "    ###################    \n",
    "\n",
    "    data_tidal = data.sel(time=slice(from_date, to_date)).swap_dims({'time': 'tideheights_old'}).sortby('tideheights_old')\n",
    "#     data_tidal = data_tidal[['NDWI', 'MNDWI', 'WI', 'AWEI_noshadow', 'AWEI_shadow', 'red', 'green', 'blue']]\n",
    "    data_tidal = data_tidal[['NDWI', 'red', 'green', 'blue']]\n",
    "    data_tidal['tideheights_var'] = data_tidal.tideheights_old\n",
    "\n",
    "    data_tidal = data_tidal.rolling(tideheights_old = 50, center=True, min_periods=1).median(dim='tideheights_old')\n",
    "    data_tidal = data_tidal.astype(np.float32)\n",
    "\n",
    "    # Copy rolling median tide heights back into dimension\n",
    "    data_tidal['tideheights_old'] = data_tidal['tideheights_var']\n",
    "\n",
    "    \n",
    "    ####################\n",
    "    # Extract contours #\n",
    "    ####################\n",
    "\n",
    "    custom_attrs = {'elev_m': data_tidal['tideheights_var'].values.astype(np.float64)}\n",
    "    custom_attrs_dtypes = {'elev_m': 'float:9.2'}\n",
    "\n",
    "    nidem_rolling_df = SpatialTools.contour_extract(z_values=0.0,\n",
    "                                 ds_array=data_tidal.NDWI,\n",
    "                                 ds_crs=data.geobox.crs,\n",
    "                                 ds_affine=data.geobox.affine,\n",
    "                                 output_shp=f'output_data/rolling_outputs/{study_area}/contours_{study_area}_{from_date[0:4]}_{to_date[0:4]}.shp',\n",
    "                                 dim='tideheights_old',\n",
    "                                 attribute_data=custom_attrs,\n",
    "                                 attribute_dtypes=custom_attrs_dtypes,\n",
    "                                 verbose=False)\n",
    "\n",
    "    ###############\n",
    "    # Interpolate #\n",
    "    ###############\n",
    "\n",
    "    # Extract x, y and z points for interpolation\n",
    "    all_contours = contours_to_arrays(gdf=nidem_rolling_df, col='elev_m')\n",
    "    points_xy = all_contours[:, [1, 0]]\n",
    "    values_elev = all_contours[:, 2]\n",
    "\n",
    "    # # Create grid to interpolate into\n",
    "    x_size, _, upleft_x, _, y_size, upleft_y =  data.geobox.transform[0:6]\n",
    "    xcols = len(data.x)\n",
    "    yrows = len(data.y)\n",
    "    bottomright_x = upleft_x + (x_size * xcols)\n",
    "    bottomright_y = upleft_y + (y_size * yrows)\n",
    "    grid_y, grid_x = np.mgrid[upleft_y:bottomright_y:1j * yrows, upleft_x:bottomright_x:1j * xcols]\n",
    "\n",
    "    # # Interpolate x, y and z values using linear/TIN interpolation\n",
    "    out = scipy.interpolate.griddata(points_xy, values_elev, (grid_y, grid_x), method='linear')\n",
    "\n",
    "    # # Apply guassian blur to smooth transitions between z values (optional)\n",
    "    # from skimage import filters\n",
    "    # out = filters.gaussian(out, sigma=2)   \n",
    "\n",
    "    ##########\n",
    "    # Export #\n",
    "    ##########\n",
    "\n",
    "    # Remove water in lowest tide, and land in highest tide \n",
    "    # todo: replace with removing any pixel always water, always land\n",
    "    intertidal_mask = np.where(data_tidal.isel(tideheights_old = 0).NDWI > 0, 1, \n",
    "                               np.where(data_tidal.isel(tideheights_old = -1).NDWI < 0, 2, 0))\n",
    "    out_masked = np.where(intertidal_mask > 0, -9999, out)\n",
    "    \n",
    "    # Append to dict\n",
    "    dem_dict[f'{from_date[0:4]}_{to_date[0:4]}'] = out_masked\n",
    "    intertidal_mask_dict[f'{from_date[0:4]}_{to_date[0:4]}'] = intertidal_mask\n",
    "\n",
    "    import rasterio\n",
    "    kwargs = {'driver': 'GTiff',\n",
    "             'width': xcols,\n",
    "             'height': yrows,\n",
    "             'count': 1,\n",
    "             'dtype': rasterio.float64,\n",
    "             'crs': 'EPSG:3577',\n",
    "             'transform': data.geobox.transform,\n",
    "             'nodata': -9999}\n",
    "\n",
    "    with rasterio.open(f'output_data/rolling_outputs/{study_area}/dem_{study_area}_{from_date[0:4]}_{to_date[0:4]}.tif', 'w', **kwargs) as target:\n",
    "        target.write_band(1, out_masked)\n",
    "        \n",
    "        \n",
    "    #############\n",
    "    # Geomedian #\n",
    "    #############\n",
    "    \n",
    "    geomedian_layer = data_tidal[['red', 'green', 'blue']].sel(tideheights_old = data_tidal.tideheights_var < 0.0).median(dim='tideheights_old')\n",
    "\n",
    "    kwargs = {'driver': 'GTiff',\n",
    "         'width': xcols,\n",
    "         'height': yrows,\n",
    "         'count': 3,\n",
    "         'dtype': rasterio.float32,\n",
    "         'crs': 'EPSG:3577',\n",
    "         'transform': data.geobox.transform,\n",
    "         'nodata': -9999}\n",
    "\n",
    "    with rasterio.open(f'output_data/rolling_outputs/{study_area}/geomedian_{study_area}_{from_date[0:4]}_{to_date[0:4]}.tif', 'w', **kwargs) as target:\n",
    "        target.write(geomedian_layer.to_array().values)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import rasterio\n",
    "kwargs = {'driver': 'GTiff',\n",
    "         'width': xcols,\n",
    "         'height': yrows,\n",
    "         'count': 1,\n",
    "         'dtype': rasterio.int16,\n",
    "         'crs': 'EPSG:3577',\n",
    "         'transform': data.geobox.transform,\n",
    "         'nodata': -9999}\n",
    "\n",
    "with rasterio.open(f'output_data/rolling_outputs/{study_area}/intertidal_{study_area}_2007_2017.tif', 'w', **kwargs) as target:\n",
    "    target.write_band(1, intertidal_mask_dict['2007_2017'].astype(rasterio.int16))\n",
    "    \n",
    "with rasterio.open(f'output_data/rolling_outputs/{study_area}/intertidal_{study_area}_1997_2007.tif', 'w', **kwargs) as target:\n",
    "    target.write_band(1, intertidal_mask_dict['1997_2007'].astype(rasterio.int16))\n",
    "    \n",
    "with rasterio.open(f'output_data/rolling_outputs/{study_area}/intertidal_{study_area}_1987_1997.tif', 'w', **kwargs) as target:\n",
    "    target.write_band(1, intertidal_mask_dict['1987_1997'].astype(rasterio.int16))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_tide = np.nanmin([np.where(i != -9999, i, np.nan) for i in dem_dict.values()])\n",
    "max_tide = np.nanmax([np.where(i != -9999, i, np.nan) for i in dem_dict.values()])\n",
    "tide_fills = [np.array([0, min_tide, max_tide])[i] for i in intertidal_mask_dict.values()]\n",
    "time1, time2, time3 = [np.where(tide_fills[i] == 0, val, tide_fills[i]) for i, val in enumerate(dem_dict.values())]\n",
    "\n",
    "\n",
    "import rasterio\n",
    "kwargs = {'driver': 'GTiff',\n",
    "         'width': xcols,\n",
    "         'height': yrows,\n",
    "         'count': 1,\n",
    "         'dtype': rasterio.float64,\n",
    "         'crs': 'EPSG:3577',\n",
    "         'transform': data.geobox.transform,\n",
    "         'nodata': -9999}\n",
    "\n",
    "with rasterio.open(f'output_data/rolling_outputs/{study_area}/diff_cm_{study_area}_2minus1.tif', 'w', **kwargs) as target:\n",
    "    target.write_band(1, time2 - time1)\n",
    "    \n",
    "with rasterio.open(f'output_data/rolling_outputs/{study_area}/diff_cm_{study_area}_3minus2.tif', 'w', **kwargs) as target:\n",
    "    target.write_band(1, time3 - time2)\n",
    "    \n",
    "with rasterio.open(f'output_data/rolling_outputs/{study_area}/diff_cm_{study_area}_3minus1.tif', 'w', **kwargs) as target:\n",
    "    target.write_band(1, time3 - time1)  \n",
    "    \n",
    "    \n",
    "with rasterio.open(f'output_data/rolling_outputs/{study_area}/diff_m3_{study_area}_2minus1.tif', 'w', **kwargs) as target:\n",
    "    target.write_band(1, (time2 - time1) * (25.0 * 25.0))\n",
    "    \n",
    "with rasterio.open(f'output_data/rolling_outputs/{study_area}/diff_m3_{study_area}_3minus2.tif', 'w', **kwargs) as target:\n",
    "    target.write_band(1, (time3 - time2) * (25.0 * 25.0))\n",
    "\n",
    "with rasterio.open(f'output_data/rolling_outputs/{study_area}/diff_m3_{study_area}_3minus1.tif', 'w', **kwargs) as target:\n",
    "    target.write_band(1, (time3 - time1) * (25.0 * 25.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xr.DataArray(intertidal_mask_dict['2007_2017'].astype(float)).plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.where(data_tidal.isel(tideheights_old = 0).NDWI > 0, 1, \n",
    "                    np.where(data_tidal.isel(tideheights_old = -1).NDWI < 0, 2, 0)) )\n",
    "\n",
    "\n",
    "\n",
    "# | (data_tidal.isel(tideheights_old = -1).NDWI < 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datacube.helpers import write_geotiff\n",
    "# from skimage.filters import threshold_otsu\n",
    "\n",
    "# test = dict(NDWI=data_tidal.isel(tideheights_old=0)['NDWI'],\n",
    "#             MNDWI=data_tidal.isel(tideheights_old=0)['MNDWI'],\n",
    "#             AWEI_shadow=data_tidal.isel(tideheights_old=0)['AWEI_shadow'],\n",
    "#             AWEI_noshadow=data_tidal.isel(tideheights_old=0)['AWEI_noshadow'],\n",
    "#             WI=data_tidal.isel(tideheights_old=0)['WI'],\n",
    "#            )\n",
    "\n",
    "# # Plot RGB\n",
    "# rgb = data_tidal.isel(tideheights_old=0)[['red','green', 'blue']]\n",
    "# rgb.attrs['affine'] = data.affine\n",
    "# rgb.attrs['crs'] = data.crs\n",
    "# write_geotiff(f'output_data/indextest_rgb.tif', rgb)\n",
    "\n",
    "# # Export other indices\n",
    "# for key, to_write in test.items():\n",
    "#     otsu = threshold_otsu(to_write.values[np.isfinite((to_write > 0).values)])\n",
    "#     print(key, otsu)\n",
    "#     to_write = to_write.to_dataset()\n",
    "#     to_write.attrs['affine'] = data.affine\n",
    "#     to_write.attrs['crs'] = data.crs\n",
    "#     write_geotiff(f'output_data/indextest_{key}.tif', to_write, profile_override={'nodata': -9999})\n",
    "#     write_geotiff(f'output_data/indextest_{key}_classified.tif', (to_write > 0).astype(np.int32), profile_override={'nodata': -9999})\n",
    "#     write_geotiff(f'output_data/indextest_{key}_otsu.tif', (to_write > otsu).astype(np.int32), profile_override={'nodata': -9999})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.interpolate import Rbf\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "# 2-d tests - setup scattered data\n",
    "x = np.random.rand(100)*4.0-2.0\n",
    "y = np.random.rand(100)*4.0-2.0\n",
    "z = x*np.exp(-x**2-y**2)\n",
    "ti = np.linspace(-2.0, 2.0, 100)\n",
    "XI, YI = np.meshgrid(ti, ti)\n",
    "\n",
    "# use RBF\n",
    "rbf = Rbf(x, y, z, epsilon=2)\n",
    "ZI = rbf(XI, YI)\n",
    "\n",
    "# plot the result\n",
    "plt.subplot(1, 1, 1)\n",
    "plt.pcolor(XI, YI, ZI, cmap=cm.jet)\n",
    "plt.scatter(x, y, 100, z, cmap=cm.jet)\n",
    "plt.title('RBF interpolation - multiquadrics')\n",
    "plt.xlim(-2, 2)\n",
    "plt.ylim(-2, 2)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rbf(y=all_contours[:, 1], x=all_contours[:, 0], z=all_contours[:, 2], epsilon=2)\n",
    "\n",
    "\n",
    "\n",
    "# points_xy = all_contours[:, [1, 0]]\n",
    "# values_elev = all_contours[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tidal.mean(dim='tideheights_old').NDWI.plot(size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEAPlotting.rgb(data_tidal.isel(x=slice(300, 500), y=slice(0, 200), tideheights_old=0), aspect=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, x = -1572144.852608, -378147.238835\n",
    "data_tidal.sel(y=y, x=x, method='nearest').NDWI.plot()\n",
    "# .rolling(tideheights_old = 50, center=True, min_periods=1).median(dim='tideheights_old')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = data_tidal.sel(y=y, x=x, method='nearest').rolling(tideheights_old = 50, center=True, min_periods=1).median(dim='tideheights_old')\n",
    "test.sel(tideheights_old = (test.NDWI > -0.4) & (test.NDWI < 0.4))[['NDWI', 'tideheights_var']].to_dataframe().corr(method='spearman').iloc[0, 1]\n",
    "\n",
    "\n",
    "# cov,cor,slope,intercept,pval,stderr = lag_linregress_3D(x=data_tidal.tideheights_var, \n",
    "#                   y=data_tidal.NDWI, lagx=0, lagy=0, first_dim='tideheights_old')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.sel(tideheights_old = (test.NDWI > -0.4) & (test.NDWI < 0.4)).tideheights_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "scipy.stats.spearmanr(data_tidal.sel(x=slice(x- 500, x + 500), y=slice(y + 500, y - 500)).NDWI.values, \n",
    "                      ((data_tidal.sel(x=slice(x- 500, x + 500), y=slice(y + 500, y - 500)).NDWI.values / data_tidal.sel(x=slice(x- 500, x + 500), y=slice(y + 500, y - 500)).NDWI.values) * data_tidal.tideheights_var.values.reshape((490, 1, 1))), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tidal.NDWI.values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((data_tidal.NDWI.values / data_tidal.NDWI.values) * data_tidal.tideheights_var.values.reshape((490, 1, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.broadcast(data_tidal.NDWI.values, data_tidal.tideheights_var.values.reshape((490, 1, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.expand_dims(data_tidal.tideheights_var.values, 2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tidal.tideheights_var.values.reshape((490, 1, 1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEAPlotting.rgb(data_tidal.sel(x=slice(x- 500, x + 500), y=slice(y + 500, y - 500), tideheights_old=slice(0.6, 1.3)), aspect=1, col='tideheights_old', col_wrap=5)\n",
    "data_tidal.sel(x=slice(x- 500, x + 500), y=slice(y + 500, y - 500), tideheights_old=slice(0.6, 1.3)).NDWI.plot(col='tideheights_old', col_wrap=8)\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tidal.isel(x=slice(300, 500), y=slice(0, 200), tideheights_old=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tidal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tidal.isel(x=slice(300, 500), y=slice(0, 200), tideheights_old=slice(305, 307))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tidal['tideheights_old'] = data_tidal['tideheights_var']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load WA sea level into xarray format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Open all data\n",
    "# all_nodes = xr.open_mfdataset(paths=files, concat_dim='file').squeeze('coast_id')\n",
    "\n",
    "# # Interpolate hourly results to get sea level estimates for exact acquisition time\n",
    "# all_nodes = all_nodes.interp(time=data.time.data.astype('M8[s]').astype('O').tolist())\n",
    "\n",
    "# # Select only required timesteps\n",
    "# all_nodes  = all_nodes[['time', 'id', 'node', 'latitude', 'longitude', 'sealevel']]\n",
    "\n",
    "# # Re-index to give latitude, longitude and time dimensions\n",
    "# test = all_nodes.set_index({'file': ['latitude', 'longitude']}).unstack()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(0,len(test.time) - 1):\n",
    "    \n",
    "#     # Import as a dataframe with lon, lat, time, sea level columns\n",
    "#     test_df_subset = test.isel(time=i).sealevel.to_dataframe().dropna(axis=0).reset_index() \n",
    "\n",
    "#     from pyproj import Proj, transform\n",
    "#     xx, yy = transform(Proj(init='epsg:4326'), Proj(init='epsg:3577'), test_df_subset.longitude.values, test_df_subset.latitude.values)\n",
    "#     test_df_subset['Coordinates'] = list(zip(*transform(Proj(init='epsg:4326'), Proj(init='epsg:3577'), test_df_subset.longitude.values, test_df_subset.latitude.values)))\n",
    "\n",
    "#     # # Create geopandas dataframe by creating geometry column\n",
    "#     # test_df_subset['Coordinates'] = list(zip(test_df_subset.longitude, test_df_subset.latitude))\n",
    "#     test_df_subset['Coordinates'] = test_df_subset['Coordinates'].apply(Point)\n",
    "#     test_df_subset = gpd.GeoDataFrame(test_df_subset, geometry='Coordinates', crs='EPSG:3577')\n",
    "#     # # # test_gdf.to_file('test.shp')\n",
    "\n",
    "#     # Interpolate\n",
    "#     resolution = 500\n",
    "#     out = interp_tides(x_coords=test_df_subset['longitude'].values, \n",
    "#                        y_coords=test_df_subset['latitude'].values, \n",
    "#                        z_coords=test_df_subset[['sealevel']].values, \n",
    "#                        sigma=15, resolution=resolution)\n",
    "\n",
    "\n",
    "#     # Create land sea mask from shapefile\n",
    "#     test_affine = rasterio.transform.from_bounds(west=xx.min(), \n",
    "#                                                  south=yy.min(), \n",
    "#                                                  east=xx.max(), \n",
    "#                                                  north=yy.max(), width=out.shape[1], height=out.shape[0])\n",
    "\n",
    "#     coastline_mask = rasterio.features.geometry_mask(coastline_gpd.geometry,\n",
    "#                                                      out_shape=(resolution, resolution),\n",
    "#                                                      transform=test_affine,\n",
    "#                                                      all_touched=False, invert=True)\n",
    "    \n",
    "#     print(np.min(out), np.max(out))\n",
    "\n",
    "#     # Remove land\n",
    "#     out[coastline_mask] = np.nan\n",
    "# #     out[out > 2500] = np.nan\n",
    "# #     out[out < 1500] = np.nan\n",
    "\n",
    "#     # Plot\n",
    "#     fig, ax = plt.subplots(1, 2, figsize=(20, 16))\n",
    "#     test_df_subset.plot(column='sealevel', ax=ax[0])\n",
    "#     im = ax[1].imshow(out, extent=[upleft_x, bottomright_x, bottomright_y, upleft_y], vmin=-2000, vmax=3000)\n",
    "#     # fig.colorbar(im)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
