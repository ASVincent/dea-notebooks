{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Datacube Stats\n",
    "| Author(s):  | [Arapaut Sivaprasad](mailto:Sivaprasad.Arapaut@ga.gov.au)|\n",
    "|----------|----------------|\n",
    "| Created: | May 17, 2018 |\n",
    "| Last edited: | May 23, 2018 |\n",
    "| Acknowledgements: | Imam Alam|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About this document\n",
    "\n",
    "**Background**\n",
    "\n",
    "Data Cube Statistics is a an application to calculate large scale temporal statistics on data stored using an Open Data Cube (ODC) installation. It provides a command line application which uses a YAML configuration file to specify the data range and statistics to calculate.\n",
    "\n",
    "**What does this document do?**\n",
    "\n",
    "This document is a startup guide to using the 'document-stats' program to analyse continental scale statistics over a 30+ year time range. It aims to get you started using the program, with basic examples and statistical analyses. Please see **Advanced Users Guide for Datacube-stats** for more complex analyses and options.\n",
    "\n",
    "The program is intended to be run from commandline, or as a queue job, on Raijin. It takes several minutes to several hours to complete the job. However, in order to understand the usage and to see the output in a visual manner, this document will show an interactive session to analyse just one scene or \"tile\", which takes only a few seconds. Below it will be given the detailed instructions to run the program on Raijin interactively and through queue jobs.\n",
    "\n",
    "A basic understanding of the PSB job scheduler on Raijin is advantageous, but not essential, to follow the instructions in this document. You must have, as expected to be, an account on Raijin with memebership in at least one project. Please verify with your supervisor about which project to use and how much resources are allocated to your research. The costs will vary depending on the type of PBS queue used as well as the complexity and size of the job.\n",
    "\n",
    "**How to use this document**\n",
    "\n",
    "There are five sections that describe the software and its usage. You can skip any section by clicking its heading line.\n",
    "\n",
    "1. Backgound information about the software and its inputs and outputs.\n",
    "2. Run interactively in a Unix shell and and monitor the progress.\n",
    "3. Submit to a PBS job queue and monitor the progress.\n",
    "4. Run interactively through Jupyter notebook.\n",
    "5. Visualise the outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECTION 1: Background on 'Datacube-stats' Application\n",
    "\n",
    "\n",
    "### Data Cube Statistics Tools\n",
    "Data Cube Statistics is an application to calculate large scale temporal statistics on data stored using an Open Data Cube (ODC) installation. It provides a command line application which uses a YAML configuration file to specify the data range and statistics to calculate.\n",
    "\n",
    "#### Main Features\n",
    "\n",
    "- Calculate continental scale statistics over a 30+ year time range.\n",
    "- Simple yet powerful Configuration format.\n",
    "- Scripts supplied for running in a HPC PBS based environment.\n",
    "- Flexible tiling and chunking for efficient execution of 100+Tb jobs.\n",
    "- Track and store full provenance record of operations.\n",
    "- Round-trip workflow from ODC back into ODC.\n",
    "- Supports saving to NetCDF, GeoTIFF or other GDAL supported format.\n",
    "- Optional per-pixel metadata tracking.\n",
    "- Out of the box support for most common statistics - see Available statistics.\n",
    "- Able to create user defined Custom statistics.\n",
    "- Able to handle any CRS and resolution combination (through the power of the ODC).\n",
    "\n",
    "#### Usage\n",
    "This is a commandline-driven program and all configurations are specified in a YAML file. The simplest way to execute it is as below.\n",
    "\n",
    "> $ datacube-stats example-configuration.yaml\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Available Statistics\n",
    "The following statistical methods are implemented.\n",
    "\n",
    "- simple\n",
    "- percentile\n",
    "- percentile_no_prov\n",
    "- medoid\n",
    "- medoid_no_prov\n",
    "- medoid_simple\n",
    "- simple_normalised_difference\n",
    "- none\n",
    "- wofs_summary\n",
    "- tcwbg_summary\n",
    "- masked_multi_count\n",
    "- external\n",
    "- wofs-summary\n",
    "- geomedian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECTION 2: Interactive execution on Raijin or VDI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This program is meant to be run as a batch job, since it may take several minutes to hours to complete. However, when the date range is small, you may want to execute it interactively and see its progress. A commandline execution is possible in this situation.\n",
    "\n",
    "#### Run in serial mode\n",
    "In serial mode, the execution happens on just one core (CPU) and uses as much RAM as required. There is no need, or facility, to specify the number of cores and RAM in serial mode.\n",
    "\n",
    "To run in serial mode, the following command is issued at the shell prompt.\n",
    "\n",
    "> ***datacube-stats config_file***, where 'config_file' is a properly formatted YAML input file (see below)\n",
    "\n",
    "e.g. *datacube-stats working_example_1.yaml*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### working_example_1.yaml\n",
    "A simple example to load and analyse all scenes for a date range is given in the example YAML file below. It can be run by the following commandline switches:\n",
    "\n",
    "- **datacube-stats working_example_1.yaml:** To create an output file for each of the tiles. The date range below results in **89** tiles.\n",
    "\n",
    "- **datacube-stats --tile-index 15 -40 working_example_1.yaml:** Create just one output file for the specified tile."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sources:\n",
    "  - product: ls8_nbar_albers\n",
    "    measurements: [blue, green, red, nir, swir1, swir2]\n",
    "    group_by: solar_day\n",
    "\n",
    "date_ranges:\n",
    "  start_date: 2017-01-01\n",
    "  end_date: 2017-01-02\n",
    "\n",
    "location: /tmp\n",
    "\n",
    "storage:\n",
    "  driver: NetCDF CF\n",
    "  crs: EPSG:3577\n",
    "  tile_size:\n",
    "      x: 100000.0\n",
    "      y: 100000.0\n",
    "  resolution:\n",
    "      x: 25\n",
    "      y: -25\n",
    "  chunking:\n",
    "      x: 200\n",
    "      y: 200\n",
    "      time: 1\n",
    "  dimension_order: [time, y, x]\n",
    "\n",
    "output_products:\n",
    " - name: landsat_seasonal_mean\n",
    "   product_type: seasonal_stats\n",
    "   statistic: simple\n",
    "   statistic_args:\n",
    "     reduction_function: mean\n",
    "   output_params:\n",
    "     zlib: True\n",
    "     fletcher32: True\n",
    "   file_path_template: 'SR_N_MEAN/SR_N_MEAN_3577_{x:02d}_{y:02d}_{epoch_start:%Y%m%d}.nc'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Running interactively on the login node is inefficient and error-prone for the following reasons.** Avoid it if possible.\n",
    "- Competes with other processes on the node. \n",
    "    - It means that other users on the system could be affected by your process.\n",
    "    - Your process may get terminated by the system for using excessive resources.\n",
    "    - On raijin, a process that runs more than 30 minutes on the login node is automatically terminated.\n",
    "- Your process may not get enough RAM to run and will exit.\n",
    "    - This results in frequent \"Memory Error\"\n",
    "- It will take extremely long time to finish, if at all runs without errors, as the resources are small.\n",
    "- Use it only if you are analysing a single scene as below.\n",
    "    - **datacube-stats --tile-index 15 -40 working_example_1.yaml:** Create just one output file for the specified tile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run interactively in a PBS queue**\n",
    "\n",
    "If you require to run interactively, for any reason such as development or analysis, there is a facility to create an interactive session through a queue job. It will allocate one or more nodes for your exclusive use, and you can execute the program interactively as you would on any login session,l but without competing for resources.\n",
    "\n",
    "To start an interactive queue sesssion, issue one of the following commands at the shell prompt.\n",
    "\n",
    "- **qsub -I -qexpress -lwalltime=01:00:00,ncpus=16,mem=32GB,jobfs=300GB,wd** : To use 1 node and 16 CPUs and 32 GB RAM\n",
    "- **qsub -I -qexpress -lwalltime=01:00:00,ncpus=32,mem=64GB,jobfs=300GB,wd** : To use 2 nodes with 32 CPUs and 64 GB RAM\n",
    "- etc.\n",
    "\n",
    "You must remember to request a longer session time, **-lwalltime=10:00:00** if the job is expected to run for longer than 1 hour. Otherwise, the job may get killed when the requested session time exceeds. If the job finishes in less time, but you do not kill the queue job, it will keep racking up the wall time and get charged for the full 10 hours.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run in parallel mode\n",
    "\n",
    "Running the program in parallel mode will speed up the execution almost proportionately as the number of cores (CPUs) used. To do it, all that is to be done is add **--parallel N**, where N is the number of cores to be used. When you have started an interactive queue session as described in the previous section, you can specify N as a number equal to or less than the CPUs requested.\n",
    "\n",
    "e.g. **datacube-stats --parallel 16 working_example_1.yaml:** To parallelise the execution on 16 cores.\n",
    "\n",
    "Arbitrarily increasing the cores in your request will not result in linearly increased performance. After certain number of cores the performance actually goes down. This is due to the fact that communication and coordination between cores consume resources and time. It is a variable that you can only arrive at by trial and error. The speedup is based on **[Amdahl's Law](https://en.wikipedia.org/wiki/Amdahl%27s_law)**, and is depended on the level and type of parallelisation in the code. With the example YAML file given above, it has been observed that the peak performance is at **16** cores. If requesting more than 16 cores, in the **--parallel N** option, the execution time actually goes up considerably. It is, presumably, due to the fact that a node has only 16 cores and, therefore, requesting more than 16 results in communication overhead between nodes (*please comment!*).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Errors and warnings\n",
    "\n",
    "1. **Memory Exceeded:** This is commonly seen when the date range is anything more than a few days. Running on a PBS queue with 16 cores and 32 GB could only safely run 2 days' date range. When the date range was increased to 1 month, the following errors started to happen.\n",
    "\n",
    "**Running the job on an interactive queue invoked by the '*qsub -I -qexpress -lwalltime=01:00:00,ncpus=16,mem=128GB,jobfs=300GB,wd*' gave the errors below, irrespective of the 'ncpus' and 'mem' used.**\n",
    "\n",
    "- PBS: job killed: mem job total 134217728 kb exceeded limit 134217728 kb\n",
    "\n",
    "> Job 7122684.r-man2 has exceeded memory allocation on node r3567\n",
    "\n",
    "> HDF5-DIAG: Error detected in HDF5 (1.10.1) thread 140385198302976:\n",
    "\n",
    "> 2018-05-23 09:14:06,258 15574 datacube.storage.storage ERROR Error opening source dataset: NetCDF:/g/data/rs0/datacube/002/LS8_OLI_NBAR/17_-32/LS8_OLI_NBAR_3577_17_-32_2013_v1496416918.nc:swir1\n",
    "2018-05-23 09:14:08,392 15574 datacube.utils WARNING Ignoring Exception: Read or write failed\n",
    "2018-05-23 09:14:13,086 15538 datacube_stats.main ERROR A child process terminated abruptly, the process pool is not usable anymore\n",
    "\n",
    "> #008: H5FDsec2.c line 715 in H5FD_sec2_read(): file read failed: time = Wed May 23 09:14:01 2018\n",
    ", filename = '/g/data/rs0/datacube/002/LS8_OLI_NBAR/17_-32/LS8_OLI_NBAR_3577_17_-32_2013_v1496416918.nc', file descriptor = 38, errno = 12, error message = 'Cannot allocate memory', buf = 0x92f506f, total read size = 12693, bytes this sub-read = 12693, bytes actually read = 18446744073709551615, offset = 2349871104\n",
    "\n",
    "**Running a batch job with the same YAML file did not cause any error.** \n",
    "\n",
    "- Memory Requested:   64.0GB;   Memory Used: 27.99GB; NCPUs Used: 16\n",
    "- Number of tiles: 1,415\n",
    "\n",
    "It means that somehow the interactive job, even though using dedicated resources, is exceeding the RAM uasge. \n",
    "\n",
    "This disparity between interactive and batch jobs does not make any obvious sense, and requires a deep investigation into the code. \n",
    "\n",
    "Unless extreme care is taken in managing the memory, it is possible to fill up the RAM. I have encountered it when writing a pattern matching algorithm where several billions of words were to be analysed. The solutions Python and Perl provided for that problem were very elgant. We might look into it to see if it is applicable in the current situation.\n",
    "\n",
    "Even though there is a RAM issue as above, there may not be any practical impact of it. The sample YAML was trying to use ALL tiles taken during a specified range within the  Spatial Reference EPSG:3577 (i.e. whole of Australia). I do not think it will ever be an intention to take all of them (*please comment!*). Instead, if we specify the particular tiles we are interested in (e.g. tiles 15,-40 plus 15,-41 to cover all of Canberra) there may not be any issue. This option, however, is not specifiable in the YAML but is a commandline option. Getting one tile in a 2-day window took less than a few seconds. Perhaps for a longer date range it will take longer, but still not hours, and no RAM exceeded error may occur. We can list as many tiles as we need in a plain text file and use it alongwith the '--tile-index-file' param.\n",
    "\n",
    "**Error Mitigation Strategies:**\n",
    "\n",
    "**1.** By specifying small chunk size it should be possible to reduce the RAM usage. The following in the YAML file might do it.\n",
    "\n",
    ">computation:\n",
    ">  chunking:\n",
    ">    x: 200\n",
    ">    y: 200\n",
    "\n",
    "The above reduced the RAM usage intially, using ~4GB for the first 10min, but it slowly creeped up. The speed of execution dropped significantly. Where previously it analysed 89 tiles in 1.6 min, it was now taking 2 min per tile. At that rate it will not be practical to run it for any significant number of tiles. Increasing the chunk size and the amount of RAM request may find a balance where execution time is acceptable. \n",
    "\n",
    "Given that the batch job is runing without the above chunking, it is only an academic issue to find out why interactive job is crashing.\n",
    "\n",
    "**2.** Another way to circumvent the crash is by using the switch, '--tile-index' to specify the tiles you need to analyse. For example, the whole of Canberra is enclosed in just 2 tiles. These two tiles can be listed in a text file as below.\n",
    "\n",
    ">15 -40<br>\n",
    ">15 -41\n",
    "\n",
    "Then, running the program without the chunking size limitation runs faster. It appears that parallelistaion has been implemented per task which in turn refers to a tile. Hence, if only 2 tiles are given in the 'tile-index' file, then only 2 CPUs are used even if you ask for more.\n",
    "\n",
    "\n",
    "2. **Existing Output File:** If there are existing files with the same name in the output dir, the program issues a warning but does not overwrite the files (*needs verification*).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECTION 3: PBS queue execution on Raijin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECTION 4: Interactive API execution on Jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    A StatsApp can produce a set of time based statistical products.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import datacube\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import exposure\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "\n",
    "import yaml\n",
    "from datacube import Datacube\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import sys\n",
    "import os.path\n",
    "from datacube_stats import StatsApp\n",
    "\n",
    "print(StatsApp.__doc__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.9b1'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datacube_stats\n",
    "datacube_stats.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import a local module \n",
    "While the modules loaded above are available to everyone, we need two functions from a module that was in-house developed. In order to use it, you must do the following:\n",
    "- Git clone the **dea-notebooks** repository to your local workspace.\n",
    "    - e.g. as /g/data/u46/users/**sa9525**/dea-notebooks, where **sa9525** is to be replaced with your userID.\n",
    "- Create a soft link from your home dir (e.g. /home/547/sa9525) to the dea-notebooks/Scripts as below.\n",
    "    - *ln -s /g/data/u46/users/sa9525/dea-notebooks dea-notebooks*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.expanduser('~/dea-notebooks/Scripts'))\n",
    "from DEAPlotting import three_band_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration File\n",
    "The entire config for this application resides in a YAML file as given below. More details about its components later..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    config_yaml = \"\"\"\n",
    "    sources:\n",
    "      - product: ls8_nbar_albers\n",
    "        measurements: [red, green, blue]\n",
    "        group_by: solar_day\n",
    "\n",
    "    date_ranges:\n",
    "        start_date: 2014-01-01\n",
    "        end_date: 2014-01-01\n",
    "\n",
    "    storage:\n",
    "        # this driver enables in-memory computation\n",
    "        driver: xarray\n",
    "\n",
    "        crs: EPSG:3577\n",
    "        tile_size:\n",
    "            x: 40000.0\n",
    "            y: 40000.0\n",
    "        resolution:\n",
    "            x: 25\n",
    "            y: -25\n",
    "        chunking:\n",
    "            x: 200\n",
    "            y: 200\n",
    "            time: 1\n",
    "        dimension_order: [time, y, x]\n",
    "\n",
    "    computation:\n",
    "        chunking:\n",
    "            x: 800\n",
    "            y: 800\n",
    "\n",
    "#    date_ranges:\n",
    "#      start_date: 2011-01-01\n",
    "#      end_date: 2011-01-15\n",
    "\n",
    "    input_region:\n",
    "          longitude: [149.05, 149.17]\n",
    "          latitude: [-35.25, -35.35]\n",
    "\n",
    "    output_products:\n",
    "        - name: nbar_mean\n",
    "          statistic: simple\n",
    "          statistic_args:\n",
    "               reduction_function: mean\n",
    "    \"\"\"\n",
    "    return config_yaml\n",
    "#    print(config_yaml)\n",
    "    # or manually creating a config dictionary works too"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and load the config\n",
    "Instead of a YAML format as above, the whole data can be specified as a dictionary object (see below). The advantage of YAML is that it is more human readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "    config_yaml = main()\n",
    "    config = yaml.load(config_yaml)\n",
    "#    print(yaml.dump(config, indent=4))\n",
    "#    print(config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the datacube stats application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = Datacube()\n",
    "app = StatsApp(config, dc.index);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the tasks\n",
    "Do not know yet what the tasks are. Will update it as I learn more!\n",
    "\n",
    "This example is taking a simple mean of the data spread over the date range. There are other methods that are more complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('generating tasks')\n",
    "tasks = app.generate_tasks()\n",
    "#print (dir(tasks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {\n",
    "        'lat': (-35.25, -35.35),\n",
    "        'lon': (149.05, 149.17),\n",
    "        'time':('2017-01-01', '2017-03-15')\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_boundingbox(ds):\n",
    "    Extent = namedtuple('Extent', ['boundingbox'])\n",
    "    BoundingBox = namedtuple('BoundingBox', ['left', 'bottom', 'right', 'top'])\n",
    "    left = ds.x.min().item()\n",
    "    right = ds.x.max().item()\n",
    "    top = ds.y.min().item()\n",
    "    bottom=ds.y.max().item()\n",
    "    return Extent(boundingbox=BoundingBox(left=left, bottom=bottom, right=right, top=top))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running tasks. May take some time. Be patient!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-32f3f486aece>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'nbar_mean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#    print(dir(ds))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'crs'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'EPSG:3577'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'extent'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_boundingbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ds' is not defined"
     ]
    }
   ],
   "source": [
    "    print('Running tasks. May take some time. Be patient!')\n",
    "    for task in tasks:\n",
    "#        print(task)\n",
    "        # this method is only available for the xarray output driver\n",
    "        output = app.execute_task(task)\n",
    "        print(\"task: \", task)\n",
    "        ds = output.result['nbar_mean']\n",
    "#    print(dir(ds))\n",
    "    ds\n",
    "    ds.attrs['crs'] = 'EPSG:3577'\n",
    "    ds.attrs['extent'] = calculate_boundingbox(ds)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.extent.boundingbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot it as a grid first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "gs = gridspec.GridSpec(2,2) # set up a 2 x 2 grid of 4 images for better presentation\n",
    "\n",
    "ax1=plt.subplot(gs[0,0])\n",
    "ds.red.isel(time=0).plot(cmap='Reds')\n",
    "\n",
    "ax2=plt.subplot(gs[1,0])\n",
    "ds.green.isel(time=0).plot(cmap='Greens')\n",
    "\n",
    "ax3=plt.subplot(gs[0,1])\n",
    "ds.blue.isel(time=0).plot(cmap='Blues')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot it as three bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "three_band_image(ds, bands = ['red', 'green', 'blue'], time = 0, contrast_enhance=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
