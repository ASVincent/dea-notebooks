{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Datacube Stats\n",
    "| Author(s):  | [Arapaut Sivaprasad](mailto:Sivaprasad.Arapaut@ga.gov.au)|\n",
    "|----------|----------------|\n",
    "| Created: | May 17, 2018 |\n",
    "| Last edited: | May 31, 2018 |\n",
    "| Acknowledgements: | Imam Alam|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About this document\n",
    "\n",
    "**Background**\n",
    "\n",
    "Data Cube Statistics is a an application to calculate large scale temporal statistics on data stored using an Open Data Cube (ODC) installation. It provides a command line application which uses a YAML configuration file to specify the data range and statistics to calculate.\n",
    "\n",
    "**What does this document do?**\n",
    "\n",
    "This document is a startup guide to using the 'document-stats' program to analyse continental scale statistics over a 30+ year time range. It aims to get you started using the program, with basic examples and statistical analyses.\n",
    "\n",
    "The program is intended to be run from commandline, or as a queue job, on Raijin. It takes several minutes to several hours to complete the job. However, in order to understand the usage and to see the output in a visual manner, this document will show an interactive session to analyse just one scene or \"tile\", which takes only a few seconds. Below it will be given the detailed instructions to run the program on Raijin interactively and through queue jobs.\n",
    "\n",
    "A basic understanding of the PSB job scheduler on Raijin is advantageous, but not essential. You must have, as expected to be, an account on Raijin with memebership in at least one project. Please verify with your supervisor about which project to use and how much resources are allocated to your research. The costs will vary depending on the type of PBS queue used as well as the complexity and size of the job.\n",
    "\n",
    "**How to use this document**\n",
    "\n",
    "There are five sections that describe the software and its usage. You can skip any section by clicking its heading line.\n",
    "\n",
    "1. Backgound information about the software and its inputs and outputs.\n",
    "2. Run interactively in a Unix shell and and monitor the progress.\n",
    "3. Submit to a PBS job queue and monitor the progress.\n",
    "4. Run interactively through Jupyter notebook.\n",
    "5. Visualise the outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECTION 1: Background on 'Datacube-stats' Application\n",
    "\n",
    "\n",
    "### Data Cube Statistics Tools\n",
    "Data Cube Statistics is an application to calculate large scale temporal statistics on data stored using an Open Data Cube (ODC) installation. It provides a command line application which uses a YAML configuration file to specify the data range and statistics to calculate.\n",
    "\n",
    "#### Main Features\n",
    "\n",
    "- Calculate continental scale statistics over a 30+ year time range.\n",
    "- Simple yet powerful Configuration format.\n",
    "- Scripts supplied for running in a HPC PBS based environment.\n",
    "- Flexible tiling and chunking for efficient execution of 100+Tb jobs.\n",
    "- Track and store full provenance record of operations.\n",
    "- Round-trip workflow from ODC back into ODC.\n",
    "- Supports saving to NetCDF, GeoTIFF or other GDAL supported format.\n",
    "- Optional per-pixel metadata tracking.\n",
    "- Out of the box support for most common statistics - see Available statistics.\n",
    "- Able to create user defined Custom statistics.\n",
    "- Able to handle any CRS and resolution combination (through the power of the ODC).\n",
    "\n",
    "#### Usage\n",
    "This is a commandline-driven program and all configurations are specified in a YAML file. The simplest way to execute it is as below.\n",
    "\n",
    "> **$ datacube-stats example-configuration.yaml**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Available Statistics\n",
    "Type in the following commandline to get a list of all statistical methods that are implemented.\n",
    "\n",
    "> **$ ds --list-statistics**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to run the program\n",
    "\n",
    "There are three ways to run it, depending upon how large or complex the tasks are, how soon you want the result and how much cost can be incurred.\n",
    "\n",
    "- **Serial mode:** A single thread is used to go through all tasks in a sequential manner.\n",
    "\n",
    "- **Parallel mode:** Multiple threads are created, each handling one task. If there are more tasks than threads, a thread that finishes a task will take on another.\n",
    "\n",
    "- **Batch mode:** Submitting to a PBS queue on Raijin. A batch job  always runs in parallel, as multiple CPUs and RAM can be allocated to a job to run it faster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECTION 2: Interactive execution on Raijin or VDI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This program is meant to be run as a batch job, since it may take several minutes to hours to complete. However, when the date range is small, you may want to execute it interactively and get results quickly. A commandline execution is possible in this situation.\n",
    "\n",
    "**INTERACTIVE, PARALLEL EXECUTION IS ERROR-PRONE DUE TO SOME UNIDENTIFIED MEMORY ISSUE(S) IN THE PROGRAM. IT IS THEREFORE ADVISED TO USE ONLY THE BATCH MODE, WHICH RUNS IN PARALLEL, WHEN THE DATA IS LARGE.** \n",
    "\n",
    "See **this document** if you wish to learn more about interactive, serial and parallel executions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECTION 3: Configuration file\n",
    "Apart form some specific commandline options, all parameters to the program can be supplied through a configuration file, which is in YAML format.  An example config file is given below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple example to load and analyse all scenes for a date range is given in the example YAML file below. It can be run by the following commandline switches:\n",
    "\n",
    "- **datacube-stats working_example_1.yaml:** To create an output file for each of the tiles. The date range below results in **89** tiles.\n",
    "\n",
    "- **datacube-stats --tile-index 15 -40 working_example_1.yaml:** Create just one output file for the specified tile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### working_example_1.yaml\n",
    "    sources:\n",
    "      - product: ls8_nbar_albers\n",
    "        measurements: [blue, green, red, nir, swir1, swir2]\n",
    "        group_by: solar_day\n",
    "\n",
    "    date_ranges:\n",
    "      start_date: 2017-01-01\n",
    "      end_date: 2017-01-02\n",
    "\n",
    "    location: /tmp\n",
    "\n",
    "    storage:\n",
    "      driver: NetCDF CF\n",
    "      crs: EPSG:3577\n",
    "      tile_size:\n",
    "          x: 100000.0\n",
    "          y: 100000.0\n",
    "      resolution:\n",
    "          x: 25\n",
    "          y: -25\n",
    "      chunking:\n",
    "          x: 200\n",
    "          y: 200\n",
    "          time: 1\n",
    "      dimension_order: [time, y, x]\n",
    "\n",
    "    output_products:\n",
    "     - name: landsat_seasonal_mean\n",
    "       product_type: seasonal_stats\n",
    "       statistic: simple\n",
    "       statistic_args:\n",
    "         reduction_function: mean\n",
    "       output_params:\n",
    "         zlib: True\n",
    "         fletcher32: True\n",
    "       file_path_template: 'SR_N_MEAN/SR_N_MEAN_3577_{x:02d}_{y:02d}_{epoch_start:%Y%m%d}.nc'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of parameters\n",
    "Short descriptions of the params are given here. See [here](https://github.com/GeoscienceAustralia/datacube-stats/#configuration-format) for details and examples.\n",
    "\n",
    "- **sources:** The products, e.g. ls8_nbar_albers\n",
    "\n",
    "    - measurements: The bands. e.g. [blue, green, red]\n",
    "    \n",
    "    - group_by: solar_day\n",
    "    \n",
    "    - source_filter: \n",
    "          \n",
    "         - product: ls5_level1_scene\n",
    "          \n",
    "         - gqa_iterative_mean_xy: [0, 1]\n",
    "              \n",
    "              - Do not consider the datasets that were derived from some `ls5_level1_scene` that had a particularly bad GQA (Geometric Quality Assessment) \n",
    "    \n",
    "    - masks: Apply the Pixel Quality masks. See [this document](http://geoscienceaustralia.github.io/digitalearthau/notebooks/DEA_datasets/Landsat5-7-8-PQ.html) for details.\n",
    "    \n",
    "- **date_ranges:** Date ranges for the data. Each tile may have several \"time slices\" based on the date range.\n",
    "\n",
    "- **input_region:** Latitude/Longitude co-ordinates, specified either as numbers  (as below) or as in a [shape file](http://geojson.io/#map=2/20.0/0.0).\n",
    "\n",
    "   - longitude: [149.05, 149.17]\n",
    "   \n",
    "   - latitude: [-35.25, -35.35]\n",
    "\n",
    "- **computation:** Each tile has 4000 x 4000 pixels. Split them into smaller chunks for calculation. Too small value (say, 200) will take very long time to finish, whereas larger than 1000 will consume too much RAM.\n",
    "      \n",
    "     - chunking:\n",
    "        \n",
    "       - x: 1000\n",
    "        \n",
    "       - y: 1000\n",
    "       \n",
    "- **location:** Directory for output files, e.g. /g/data/u46/users/sa9525/avs. Make sure that it is on a drive with sufficient quota for your user ID. Trying it on /tmp will not work, as it has only a quota of 100MB. Also, avoid the /short directory if you intend to keep the output files for long term storage (> 365 days).\n",
    "\n",
    "- **storage:** The output file format\n",
    "  \n",
    "  - driver: NetCDF CF: Create files in NetCDF format (\\*.nc). These can be viewed with Panoply and ncview.\n",
    "\n",
    "  - crs: EPSG:3577: Geographical Spatial References. EPSG:3577 = Australia-wide\n",
    "  \n",
    "  - tile_size: Dimensions of tiles in meters.\n",
    "\n",
    "      - x: 100000.0\n",
    "      \n",
    "      - y: 100000.0\n",
    "      \n",
    "  - resolution: Pixel dimensions in meters.\n",
    "      - x: 25\n",
    "      \n",
    "      - y: -25\n",
    "      \n",
    "  - chunking: Chunk sizes for writing the output\n",
    "      \n",
    "      - x: 200\n",
    "      \n",
    "      - y: 200\n",
    "      \n",
    "      - time: 1\n",
    "\n",
    "- **dimension_order:** [time, y, x]: Each tile is processed layer by layer based on time slices.\n",
    "\n",
    "- **output_products:** Specifications of the statistical method to be used, and the output file path.\n",
    "     \n",
    "     - name: landsat_seasonal_mean\n",
    "       \n",
    "     - product_type: seasonal_stats\n",
    "       \n",
    "     - statistic: simple\n",
    "   \n",
    "     - statistic_args:\n",
    "     \n",
    "         - reduction_function: mean\n",
    "   \n",
    "   - output_params:\n",
    "     \n",
    "     - zlib: True\n",
    "     \n",
    "     - fletcher32: True\n",
    "     \n",
    "   - file_path_template: 'SR_N_MEAN/SR_N_MEAN_3577_{x:02d}_{y:02d}_{epoch_start:%Y%m%d}.nc'\n",
    "       \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECTION 4: Commandline Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The commandline switches, or parameters, act as further ways to modify the operation of the program. A list of these parameters and their short descriptions can be seen by the command, **datacube-stats --help**. Some additional details are given below.\n",
    "\n",
    "| Parameter | Description | Additional Information |\n",
    "|-|-|\n",
    "| **--save-tasks PATH:** | Save the tasks in a file. | The program reads the datacube data and creates a set of tasks, which can be saved in a file to be used again. Useful only when using the same date range and input region.|\n",
    "| **--load-tasks PATH:** | The saved tasks can be loaded with this command.| You can load parts of the tasks in any order |\n",
    "| **--tile-index INTEGER...:** | Specify the IDs of tiles to be analysed. | e.g. tile \"15 -40\" covers a part of ACT.|\n",
    "| **--tile-index-file PATH:** | Specify more than one tile, if any, in a file as one per line. | e.g. \"15 -40 and 15 -41\" to cover the whole of ACT. They need not be contiguous. |\n",
    "| **--output-location TEXT:** | Override output location specified in configuration file.|\n",
    "| **--year INTEGER:** | Override time period specified in configuration file.|\n",
    "| **--task-slice SLICE:** | The subset of tasks to perform, using Python's slice syntax.| e.g. you can specify a sub-section of the tasks as [3:9] or [0:100:10], etc. |\n",
    "| **--batch INTEGER:** | The number of batch jobs to launch using PBS and the serial executor.| The batch jobs can be automatically sent with the \"---qsub\" option or manually by the \"--batch\" option. In the latter case, it will not use the celery executor to farm out the tasks and, therefore, you have better control. |\n",
    "| **--list-statistics:** | Display the different statistical method that have been implemented.|\n",
    "| **--version:** | Display the program version.|\n",
    "| **-v, --verbose:** | Show debugging lines. | Specify more then one 'v' to increase the levels of such lines. e.g. -vvv |\n",
    "| **--log-file TEXT:** | Specify a log file.|\n",
    "| **-C, --config, --config_file TEXT:**| This refers to the file that specifies which datacube is to be used | It is defined by the environment variable, DATACUBE_CONFIG_PATH, which is defined as */g/data/u46/users/sa9525/datacube.conf* in your '.bashrc'. You can specify a different file here.|\n",
    "| **-E, --env TEXT:**| This specifies which section of the config file to use. | e.g. *--env dea-staging* will use the [dea-staging] in the config file. |\n",
    "| **--log-queries:** | Print database queries.| Useful for debugging and development purposes. Not needed by the end user. |\n",
    "| **--qsub OPTS:** | Launch via qsub, supply comma or new-line separated list of parameters. Try --qsub=help.| This option, without the '--batch' option will farm out, by using the celery executor, the tasks to multiple cores. |\n",
    "| **--workers-per-node INTEGER:** | Specify the number of tasks allocated to each node. | Applicable only in the case of interactive parallel mode.|\n",
    "| **--queue-size INTEGER:** | Overwrite defaults for queue size.|\n",
    "| **--celery HOST:PORT:** | Use celery backend for parallel computation. | Supply redis server address, or \"pbs-launch\" to launch redis server and workers when running under pbs.|\n",
    "| **--dask HOST:PORT:** | Use celery backend for parallel computation. | Supply redis server address, or \"pbs-launch\"to launch redis server and workers when running under pbs.|\n",
    "| **--parallel INTEGER:** | Run locally in parallel mode using the specified number of cores.|\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECTION 5: Batch job on PBS queue\n",
    "\n",
    "A batch job is one where a program is run on a queue that is detached from the interactive session. Unlike an interactive session, a batch job will write all output to a file instead of the terminal. Such file is not written until after the job finishes and, hence, it is not possible to follow the progress of a job (except by looking at the results files).\n",
    "\n",
    "Use the following command to submit a **datacube-stats** job to the batch queue.\n",
    "\n",
    "> **datacube-stats --qsub 'ncpus=16,walltime=8h,project=v10,queue=express,noask,mem=4G,name=Datacube-stats' batch.yaml**\n",
    "\n",
    "> **datacube-stats --qsub 'nodes=1,walltime=8h,project=v10,queue=express,noask,mem=4G,name=Datacube-stats' batch.yaml**\n",
    "\n",
    "where, \n",
    "\n",
    "- **--qsub:** Commandline parameter to run the program in batch mode.\n",
    "\n",
    "- **que=express:** Use the express queue. \n",
    "    \n",
    "- **walltime=8h:** Requesting 8 hours of continuous operation. \n",
    "    \n",
    "    - The walltime must be enough to complete the job. Otherwise, it will get killed.\n",
    "    \n",
    "    - You can request more walltime than required. Only the amount actually used will be charged.\n",
    "\n",
    "- **ncpus=16:** The number of CPUs (or cores) to be used. \n",
    "    \n",
    "    - It will use as many CPUs as required.\n",
    "\n",
    "    - Requesting more than 16 CPUs is OK for batch queue.\n",
    "    \n",
    "- **nodes=1** The number of nodes or physical machines to use. Each node has 16 cores and, hence, specifying that nodes=1 or ncpus=16 both mean the same, and only one needs to be there. \n",
    "   \n",
    "- **mem:4G** Amount of RAM requested per core. Thus, in the above command, we are requesting 64GB.\n",
    "    \n",
    "    - You can request up to 8 GB per CPU, but should choose 4 GB or less to avoid getting rejected.   \n",
    "    \n",
    "- **project=v10:** The project ID number. \n",
    "\n",
    "- **noask:** The queue starts without further confirmation from you.\n",
    "    \n",
    "    - It is best to use this switch, as there may be a wait of several minutes before teh queue is allocated.\n",
    "    \n",
    "- **name=Datacube-stats:** An arbitrary name to be given to the queue.\n",
    "\n",
    "    - This will be used in creating the outputs and queue statistics files.\n",
    "    \n",
    "    - It will also be used to report any error with the que.\n",
    "    \n",
    "- **batch.yaml:** The YAML file to run the datacube-stats program.    \n",
    "\n",
    "\n",
    "Two files, *viz.,* **Job-ID.e\\*** and **Job-ID.o\\***, will be created upon the completion of the queue job. They will include the errors and/or program output messages (those which normally go to the terminal in an interactive session) and the job statistics, respectively.  \n",
    "\n",
    "The results will be written out in a sub-directory of that specified as **location** and **file_path_template** specified in the config file (see above).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECTION 4: Interactive API execution on Jupyter notebook\n",
    "\n",
    "This section is a quick demonstration to understand the configuration file and to visualise the data interactively. In order to get quick results, only a small area and/or time range must be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    A StatsApp can produce a set of time based statistical products.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import datacube\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import exposure\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "\n",
    "import yaml\n",
    "from datacube import Datacube\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import sys\n",
    "import os.path\n",
    "from datacube_stats import StatsApp\n",
    "\n",
    "print(StatsApp.__doc__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.9b1'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datacube_stats\n",
    "datacube_stats.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import a local module \n",
    "While the modules loaded above are available to everyone, we need two functions from a module that was in-house developed. In order to use it, you must do the following:\n",
    "- Git clone the **dea-notebooks** repository to your local workspace.\n",
    "    - e.g. as /g/data/u46/users/**sa9525**/dea-notebooks, where **sa9525** is to be replaced with your userID.\n",
    "- Create a soft link from your home dir (e.g. /home/547/sa9525) to the dea-notebooks/Scripts as below.\n",
    "    - *ln -s /g/data/u46/users/sa9525/dea-notebooks dea-notebooks*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.expanduser('~/dea-notebooks/Scripts'))\n",
    "from DEAPlotting import three_band_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_boundingbox(ds):\n",
    "    Extent = namedtuple('Extent', ['boundingbox'])\n",
    "    BoundingBox = namedtuple('BoundingBox', ['left', 'bottom', 'right', 'top'])\n",
    "    left = ds.x.min().item()\n",
    "    right = ds.x.max().item()\n",
    "    top = ds.y.min().item()\n",
    "    bottom=ds.y.max().item()\n",
    "    return Extent(boundingbox=BoundingBox(left=left, bottom=bottom, right=right, top=top))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration File\n",
    "The entire config for this application resides in a YAML file as given below. More details about its components later..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    config_yaml = \"\"\"\n",
    "    sources:\n",
    "      - product: ls8_nbar_albers\n",
    "        measurements: [red, green, blue]\n",
    "        group_by: solar_day\n",
    "\n",
    "    date_ranges:\n",
    "        start_date: 2014-01-01\n",
    "        end_date: 2014-02-01\n",
    "\n",
    "    storage:\n",
    "        # this driver enables in-memory computation\n",
    "        driver: xarray\n",
    "\n",
    "        crs: EPSG:3577\n",
    "        tile_size:\n",
    "            x: 100000.0\n",
    "            y: 100000.0\n",
    "        resolution:\n",
    "            x: 25\n",
    "            y: -25\n",
    "        chunking:\n",
    "            x: 200\n",
    "            y: 200\n",
    "            time: 1\n",
    "        dimension_order: [time, y, x]\n",
    "\n",
    "    computation:\n",
    "        chunking:\n",
    "            x: 800\n",
    "            y: 800\n",
    "\n",
    "# Uncomment this section to see Canberra region\n",
    "    input_region:\n",
    "          longitude: [149.05, 149.17]\n",
    "          latitude: [-35.25, -35.35]\n",
    "\n",
    "# Uncomment this section (and comment out above) to see two tiles that encode all of ACT\n",
    "#    input_region:\n",
    "#      tiles:\n",
    "#        - [15, -40]\n",
    "#        - [15, -41]\n",
    "    \n",
    "    output_products:\n",
    "        - name: nbar_mean\n",
    "          statistic: simple\n",
    "          statistic_args:\n",
    "               reduction_function: mean\n",
    "    \"\"\"\n",
    "    return config_yaml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and load the config\n",
    "Instead of a YAML format as above, the whole data can be specified as a dictionary object (see below). The advantage of YAML is that it is more human readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sources': [{'product': 'ls8_nbar_albers',\n",
       "   'measurements': ['red', 'green', 'blue'],\n",
       "   'group_by': 'solar_day'}],\n",
       " 'date_ranges': {'start_date': datetime.date(2014, 1, 1),\n",
       "  'end_date': datetime.date(2014, 2, 1)},\n",
       " 'storage': {'driver': 'xarray',\n",
       "  'crs': 'EPSG:3577',\n",
       "  'tile_size': {'x': 100000.0, 'y': 100000.0},\n",
       "  'resolution': {'x': 25, 'y': -25},\n",
       "  'chunking': {'x': 200, 'y': 200, 'time': 1},\n",
       "  'dimension_order': ['time', 'y', 'x']},\n",
       " 'computation': {'chunking': {'x': 800, 'y': 800}},\n",
       " 'input_region': {'longitude': [149.05, 149.17], 'latitude': [-35.25, -35.35]},\n",
       " 'output_products': [{'name': 'nbar_mean',\n",
       "   'statistic': 'simple',\n",
       "   'statistic_args': {'reduction_function': 'mean'}}]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_yaml = main()\n",
    "config = yaml.load(config_yaml)\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the datacube stats application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = Datacube()\n",
    "app = StatsApp(config, dc.index);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the tasks and get the results\n",
    "A **task** is to process one **tile**, which in turn may be composed of several **time slices**.\n",
    "\n",
    "This example is taking a simple mean of the data spread over the date range. There are other methods that are more complex.\n",
    "\n",
    "The 'results' variable below is a list that holds the 'source' of all time slices and the 'result' that has the mean values. We are interested in the latter. Depending on the date range there will be 0 to many time slices, but always only 1 array for the mean values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running tasks. May take some time. Be patient!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'nbar_mean': <xarray.Dataset>\n",
       " Dimensions:  (time: 1, x: 492, y: 500)\n",
       " Coordinates:\n",
       "   * time     (time) datetime64[ns] 2014-01-01\n",
       "   * y        (y) float64 -3.953e+06 -3.953e+06 -3.953e+06 -3.953e+06 ...\n",
       "   * x        (x) float64 1.542e+06 1.542e+06 1.542e+06 1.542e+06 1.542e+06 ...\n",
       " Data variables:\n",
       "     red      (time, y, x) int16 2354 2368 2585 2688 2749 2684 2584 2472 2488 ...\n",
       "     green    (time, y, x) int16 2194 2236 2505 2614 2682 2659 2557 2400 2393 ...\n",
       "     blue     (time, y, x) int16 2024 2035 2296 2336 2377 2385 2286 2164 2149 ...}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Running tasks. May take some time. Be patient!')\n",
    "tasks = app.generate_tasks()\n",
    "results = [app.execute_task(task) for task in tasks]\n",
    "[x] = results # List Convert\n",
    "x.result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect the data for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__contains__', '__delattr__', '__delitem__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', 'clear', 'copy', 'fromkeys', 'get', 'items', 'keys', 'pop', 'popitem', 'setdefault', 'update', 'values']\n"
     ]
    }
   ],
   "source": [
    "ds = x.result\n",
    "#t = ds.time\n",
    "print(dir(ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'attrs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-1a7e6b3d6599>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#ds = x.result['nbar_mean']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'crs'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'EPSG:3577'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'extent'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_boundingbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'attrs'"
     ]
    }
   ],
   "source": [
    "#ds = x.result['nbar_mean']\n",
    "ds = x.result\n",
    "ds.attrs['crs'] = 'EPSG:3577'\n",
    "ds.attrs['extent'] = calculate_boundingbox(ds)    \n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot it as three bands combined image\n",
    "If there are several time slices for the tile, then the plot will show the average values for each pixel. Since in this interactive demo we are using just one time slice, the image below will be no different to one scene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'red'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-375c91cba2fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mthree_band_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbands\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'red'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'green'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'blue'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrast_enhance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/dea-notebooks/Scripts/DEAPlotting.py\u001b[0m in \u001b[0;36mthree_band_image\u001b[0;34m(ds, bands, time, figsize, title, projection, contrast_enhance, reflect_stand)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;31m# Create new numpy array matching shape of xarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbands\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mrawimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'red'"
     ]
    }
   ],
   "source": [
    "three_band_image(ds, bands = ['red', 'green', 'blue'], time = 0, contrast_enhance=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
