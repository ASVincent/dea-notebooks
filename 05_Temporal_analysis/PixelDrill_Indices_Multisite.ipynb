{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pixel Drill for Indices outputs to CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Authors:  | Bex Dunn|\n",
    "|----------|----------------|\n",
    "| Created: | March 6, 2019 |\n",
    "| Last edited: | March 6, 2019 |\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "You need to run the following commands from the command line prior to launching jupyter notebooks from the same terminal so that the required libraries and paths are set:\n",
    "\n",
    "`module use /g/data/v10/public/modules/modulefiles` \n",
    "\n",
    "`module load dea`\n",
    "\n",
    "If you find an error or bug in this notebook, please either create an 'Issue' in the Github repository, or fix it yourself and create a 'Pull' request to contribute the updated notebook back into the repository (See the repository [README](https://github.com/GeoscienceAustralia/dea-notebooks/blob/master/README.rst) for instructions on creating a Pull request).\n",
    "\n",
    "__Background:__ Data from the [Landsat](https://landsat.usgs.gov/about-landsat) 5,7 and 8 satellite missions are accessible through [Digital Earth Australia](http://www.ga.gov.au/about/projects/geographic/digital-earth-australia) (DEA).\n",
    "\n",
    "__What does this notebook do?:__ This notebook takes a supplied CSV of site points. It runs a pixel drill through surface reflectance, calculates NDVI, Taselled cap wetness and greenness, and outputs a csv of values for each site and plots of each index for each site."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tags**: :index:`Landsat`,:index:`Landsat5`,:index:`Landsat7`,:index:`Landsat8`, :index:`pixeldrill`, :index:`DEAPlotting`, :index:`datacube.utils.geometry`, :index:`query`,:index:`Scripts`,:index:`tasseled_cap`, :index:`NDVI`,                                                                                                           :index:`DEADataHandling`, :index:`DEAPlotting`, :index:`load_clearlandsat`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import some modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "running for index 0\n",
      "GWY_MUNG1\n",
      "/g/data/r78/rjd547/LTIM_veg_monitoring/GWY_MUNG1.csv\n",
      "Loading ls5\n",
      "    Loading 338 filtered ls5 timesteps\n",
      "Loading ls7\n",
      "    Ignoring SLC-off observations for ls7\n",
      "    Loading 73 filtered ls7 timesteps\n",
      "Loading ls8\n",
      "    Loading 131 filtered ls8 timesteps\n",
      "Combining and sorting ls5, ls7, ls8 data\n",
      "    Replacing invalid -999 values with NaN (data will be coerced to float64)\n",
      "The formula we are using is (nir - red)/(nir + red)\n",
      "/g/data/r78/rjd547/LTIM_veg_monitoring/GWY_MUNG1.csv\n",
      "/g/data/r78/rjd547/LTIM_veg_monitoring/LTIM vegetation monitoring sites.csv\n",
      "/g/data/r78/rjd547/LTIM_veg_monitoring/MDBA_SCA_Tool_uniall_scores.csv\n",
      "running for index 1\n",
      "GWY_ODB1\n",
      "/g/data/r78/rjd547/LTIM_veg_monitoring/GWY_ODB1.csv\n",
      "Loading ls5\n"
     ]
    }
   ],
   "source": [
    "import datacube\n",
    "import datetime\n",
    "import fiona\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio.mask\n",
    "import rasterio.features\n",
    "import shapely\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import xarray as xr\n",
    "\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datacube.storage import masking\n",
    "from datacube.utils import geometry\n",
    "\n",
    "sys.path.append('../10_Scripts')\n",
    "import DEADataHandling, DEAPlotting, TasseledCapTools, BandIndices\n",
    "\n",
    "dc = datacube.Datacube(app='pixel drill')\n",
    "\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "#set up file to open \n",
    "\n",
    "inpath = '/g/data/r78/rjd547/LTIM_veg_monitoring/LTIM vegetation monitoring sites.csv'\n",
    "\n",
    "### Convert csv latitude and longitude values into a geopandas geodatafrome\n",
    "\n",
    "veg_sites = pd.read_csv(inpath, delimiter=\",\")\n",
    "#turn csv geometry into geopandas dataframe geometry using lambda functions and shapely\n",
    "veg_sites['geometry']=veg_sites.apply(lambda z: shapely.geometry.Point(z.LATITUDE, z.LONGITUDE), axis=1)\n",
    "veg_sites = gpd.GeoDataFrame(veg_sites)\n",
    "\n",
    "#define an output location\n",
    "output_loc = '/g/data/r78/rjd547/LTIM_veg_monitoring/'\n",
    "\n",
    "### run this for multiple sites\n",
    "\n",
    "for site in range(0,len(veg_sites)):\n",
    "    print(f'running for index {site}')\n",
    "\n",
    "    query = {'lat':(veg_sites['LATITUDE'][site]), \n",
    "             'lon':(veg_sites['LONGITUDE'][site])}               \n",
    "\n",
    "    Veg_Site = '_'.join(veg_sites['NAME'][site].split(' '))\n",
    "    print(Veg_Site)\n",
    "\n",
    "    outfilename=output_loc+Veg_Site+'.csv'\n",
    "    print (outfilename)\n",
    "\n",
    "    ls578 = DEADataHandling.load_clearlandsat(dc=dc, query=query, product='nbart')\n",
    "\n",
    "    ### Calculate NDVI \n",
    "\n",
    "    ### Calculate the tasseled cap indices\n",
    "\n",
    "    #transform the nbart into tci - we want unthresholded, which is why we set drop_tci_bands to False\n",
    "    tci = TasseledCapTools.thresholded_tasseled_cap(ls578,wetness_threshold=-350, drop=True , drop_tc_bands=False)\n",
    "\n",
    "    tcw = tci.wetness\n",
    "    tcg = tci.greenness\n",
    "\n",
    "    ndvi = BandIndices.calculate_indices(ls578, index='NDVI')\n",
    "\n",
    "    ### create and fill pandas dataframe to write to csv\n",
    "\n",
    "    #drop dimensions of length 1\n",
    "    ndvi = ndvi.squeeze('x')\n",
    "    ndvi = ndvi.squeeze('y')\n",
    "\n",
    "    #make a new dataframe using the data from the xarray of ndvi\n",
    "    NDVI = pd.DataFrame(data=ndvi.data, index=ndvi.time,columns=['ndvi'])\n",
    "\n",
    "    #rename the dataframe so that we can add the other indexes\n",
    "    INDICIES=NDVI\n",
    "\n",
    "    #again drop dims\n",
    "    TCW = tcw.squeeze().data\n",
    "    TCG = tcg.squeeze().data\n",
    "\n",
    "    # set up final dataframe to write to file 'INDICES'\n",
    "\n",
    "    INDICIES['tcw']=TCW\n",
    "    INDICIES['tcg']=TCG\n",
    "    #we have some values where the pixel is cloudy. Drop these values\n",
    "    INDICES = INDICIES.dropna(axis='index', thresh =2)\n",
    "\n",
    "    INDICES.to_csv(outfilename)\n",
    "\n",
    "    !ls /g/data/r78/rjd547/LTIM_veg_monitoring/*.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
