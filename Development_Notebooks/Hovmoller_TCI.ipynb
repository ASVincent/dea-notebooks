{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hovmoller Sites TCI\n",
    "\n",
    "This notebook opens a shape file of transects, allows you to select a transect by number (or plot all sites), and plot a hovmoller diagram of the site tasselled cap indices based on datacube landsat surface reflectance data and BoM rainfall data.\n",
    "\n",
    "Dependencies:\n",
    "\n",
    "  * shapefile of the site transects in EPSG 4326\n",
    "  \n",
    "The following sensors are available for the following time frames:\n",
    "* Landsat 5 - 1986 to April 1999  followed by a gap until May 2003 - November 2011 (data from 2009 onwards becomes less reliable in southern Australia)\n",
    "* Landsat 7 - April 1999 to present, however after May 2003 the scan line corrector (SLC) failed, \n",
    "so data are referred to as SLC-off, meaning they've got a venetian blinds appearance with wedges of missing data\n",
    "  * This data is not well suited for inclusion in composites, but is fine to use in time series analysis\n",
    "* Landsat 8 - April 2013 onwards\n",
    "  \n",
    "Bex Dunn June 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T04:27:46.179951Z",
     "start_time": "2018-06-08T04:27:46.176572Z"
    }
   },
   "source": [
    "### Set up modules, functions and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-15T01:14:55.807362Z",
     "start_time": "2018-06-15T01:14:48.796952Z"
    }
   },
   "outputs": [],
   "source": [
    "#in this notebook we want to plot non-interactively\n",
    "%matplotlib inline\n",
    "\n",
    "#suppress warnings thrown when rainfall data is imported\n",
    "# import logging\n",
    "# logging.getLogger('rasterio._gdal').setLevel(logging.ERROR)\n",
    "# import warnings\n",
    "\n",
    "#get standard libraries\n",
    "import datetime as dt\n",
    "import fiona\n",
    "import geopandas as gpd\n",
    "import shapely.geometry\n",
    "from shapely.geometry import shape\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib as mpl\n",
    "from matplotlib import colors\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.dates\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import rasterio\n",
    "import rasterio.features\n",
    "import sys\n",
    "import xarray as xr\n",
    "\n",
    "#modules for datacube\n",
    "import datacube\n",
    "from datacube.utils import geometry\n",
    "from datacube.storage.storage import write_dataset_to_netcdf\n",
    "from datacube.helpers import write_geotiff\n",
    "\n",
    "# Import external functions from dea-notebooks\n",
    "sys.path.append(os.path.expanduser('~/dea-notebooks/Scripts/'))\n",
    "import DEAPlotting, DEADataHandling\n",
    "from BandIndices import tasseled_cap\n",
    "from FileDialogs import *\n",
    "\n",
    "#ignore datacube warnings (needs to be last import statement)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', module='datacube')\n",
    "\n",
    "#alias for datacube\n",
    "dc = datacube.Datacube(app='dc-BoMrainfallandNbart')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up functions for this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-15T01:14:55.821324Z",
     "start_time": "2018-06-15T01:14:55.810327Z"
    }
   },
   "outputs": [],
   "source": [
    "#This defines the function that converts a linear vector file into a string of x,y coordinates\n",
    "def geom_query(geom, geom_crs='EPSG:4326'):\n",
    "    \"\"\"\n",
    "    Create datacube query snippet for geometry\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'x': (geom.bounds[0], geom.bounds[2]),\n",
    "        'y': (geom.bounds[1], geom.bounds[3]),\n",
    "        'crs': geom_crs\n",
    "    }\n",
    "\n",
    "def warp_geometry(geom, crs_crs, dst_crs):\n",
    "    \"\"\"\n",
    "    warp geometry from crs_crs to dst_crs\n",
    "    \"\"\"\n",
    "    return shapely.geometry.shape(rasterio.warp.transform_geom(crs_crs, dst_crs, shapely.geometry.mapping(geom)))\n",
    "\n",
    "\n",
    "def transect(data, geom, resolution, method='nearest', tolerance=None):\n",
    "    \"\"\"\n",
    "    gets the transect\n",
    "    \"\"\"\n",
    "    #Changed for py3 compatibility 17.03.17\n",
    "    dist = [i for i in range(0, int(geom.length), resolution)]\n",
    "    #points = zip(*[geom.interpolate(d).coords[0] for d in dist]) py2\n",
    "    points = list(zip(*[geom.interpolate(d).coords[0] for d in dist])) #py3\n",
    "    indexers = {\n",
    "        data.crs.dimensions[0]: list(points[1]),\n",
    "        data.crs.dimensions[1]: list(points[0])        \n",
    "    }\n",
    "    return data.sel_points(xr.DataArray(dist, name='distance', dims=['distance']),\n",
    "                           method=method,\n",
    "                           tolerance=tolerance,\n",
    "                           **indexers)\n",
    "def load_rainfall(query):\n",
    "    \n",
    "    dc_rf =datacube.Datacube(config='/g/data/r78/bom_grids/rainfall.conf')\n",
    "    \n",
    "    rf_data = dc_rf.load(product = 'rainfall_grids_1901_2017',**query)\n",
    "\n",
    "    return rf_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add the path to the input shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-15T01:14:55.851311Z",
     "start_time": "2018-06-15T01:14:55.824369Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#transects file will be the file containing our transects\n",
    "transects_file = '/g/data/r78/rjd547/groundwater_activities/Burdekin/Burdekin_shapefiles/transects/merged2_hovmoller_transects2.shp'\n",
    "transects_df = gpd.read_file(transects_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-15T01:14:55.858369Z",
     "start_time": "2018-06-15T01:14:55.853604Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temporal range is 2015-10-01 to 2017-10-01\n"
     ]
    }
   ],
   "source": [
    "#Define temporal range\n",
    "start_of_epoch = '2015-10-01'#'1987-10-01' \n",
    "end_of_epoch =  '2017-10-01'\n",
    "print ('temporal range is '+start_of_epoch+' to '+end_of_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-15T01:14:59.908224Z",
     "start_time": "2018-06-15T01:14:55.860824Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "choose the index of the transect you would like to plot:0\n"
     ]
    }
   ],
   "source": [
    "#use fiona module to open the shape file\n",
    "transects = fiona.open(transects_file)\n",
    "\n",
    "#this statement asks you to choose a shapefile. put in a number and press enter.\n",
    "i = int(input(\"choose the index of the transect you would like to plot:\"))\n",
    "#for i in range(len(transects)):\n",
    "\n",
    "geom1 = shape(transects[i]['geometry'])\n",
    "geom_query1 = geom_query(geom1)\n",
    "query = {\n",
    "    'time': (start_of_epoch, end_of_epoch),  \n",
    "}\n",
    "query.update(geom_query(geom1, geom_crs=transects.crs_wkt)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use the transect dataframe to get transect metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-15T01:15:03.195784Z",
     "start_time": "2018-06-15T01:15:03.188165Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BasaltR_AmeliaDowns\n",
      "BasaltR_AmeliaDowns_ix0\n",
      "OS error: [Errno 17] File exists: '/g/data/r78/rjd547/groundwater_activities/Burdekin/Burdekin_Results/HovResults/BasaltR_AmeliaDowns_ix0_/'\n"
     ]
    }
   ],
   "source": [
    "#use the transect dataframe to get transect metadata\n",
    "transect_index = transects_df.index[i]\n",
    "transect_name = transects_df['Name'][i] \n",
    "try:\n",
    "    print(transect_name)\n",
    "except TypeError as err:\n",
    "    print(\"TypeError: {0}\".format(err))\n",
    "    print(transect_name)\n",
    "\n",
    "#create a filename for the transect\n",
    "shape_name = transect_name.split()\n",
    "shape_name ='_'.join(shape_name)\n",
    "shape_name=shape_name+'_ix'+str(transect_index)\n",
    "print(shape_name)\n",
    "\n",
    "#setup a save directory for our data\n",
    "savepath ='/g/data/r78/rjd547/groundwater_activities/Burdekin/Burdekin_Results/HovResults/'+shape_name+'_/'\n",
    "try:\n",
    "        os.mkdir(savepath)\n",
    "except OSError as err:\n",
    "        print(\"OS error: {0}\".format(err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if this notebook has already been run and the data saved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-15T01:15:05.406650Z",
     "start_time": "2018-06-15T01:15:05.401911Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/g/data/r78/rjd547/groundwater_activities/Burdekin/Burdekin_Results/HovResults/BasaltR_AmeliaDowns_ix0_/hov_data_BasaltR_AmeliaDowns_ix0_2015-10-01_2017-10-01.pkl\n"
     ]
    }
   ],
   "source": [
    "print(savepath+'hov_data_{}_{}_{}'.format(shape_name,start_of_epoch,end_of_epoch)+'.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If not, pull in rainfall and nbar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-15T01:15:34.382775Z",
     "start_time": "2018-06-15T01:15:08.549807Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "did not load data from previous run\n",
      "Loading ls5 PQ\n",
      "    Skipping ls5\n",
      "Loading ls7 PQ\n",
      "    Loading 14 filtered ls7 timesteps\n",
      "Loading ls8 PQ\n",
      "    Loading 33 filtered ls8 timesteps\n",
      "Combining and sorting ls5, ls7 and ls8 data\n",
      "loaded SR data from DEA, getting rainfall grids\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile(savepath+'hov_data_{}_{}_{}'.format(shape_name,start_of_epoch,end_of_epoch)+'.pkl'):\n",
    "    f = open(savepath+'hov_data_{}_{}_{}'.format(shape_name,start_of_epoch,end_of_epoch)+'.pkl', 'rb')\n",
    "    hov_data = pickle.load(f) \n",
    "    f.close()\n",
    "    if hov_data is not None: \n",
    "        print('loaded data from file')\n",
    "        ds = hov_data['ds']\n",
    "        Studysite_rain=hov_data['Studysite_rain']\n",
    "    \n",
    "else:\n",
    "    print('did not load data from previous run')\n",
    "    #use our DEADataHandling function to load landsat data for all sensors in epoch and mask out cloudy scenes\n",
    "    ds = DEADataHandling.load_clearlandsat(dc, query,product='nbart',masked_prop=0.80)\n",
    "    print('loaded SR data from DEA, getting rainfall grids')\n",
    "    #Grab bom_rainfall_grids from the datacube\n",
    "#     Studysite_rain = load_rainfall(query)\n",
    "#     print('loaded bom rainfall grids from DEA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-15T01:15:34.406879Z",
     "start_time": "2018-06-15T01:15:34.385185Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:    (time: 47, x: 8, y: 6)\n",
       "Coordinates:\n",
       "  * y          (y) float64 -2.181e+06 -2.181e+06 -2.181e+06 -2.181e+06 ...\n",
       "  * x          (x) float64 1.393e+06 1.394e+06 1.394e+06 1.394e+06 1.394e+06 ...\n",
       "  * time       (time) datetime64[ns] 2015-10-21T00:17:15 2015-10-29T00:17:37 ...\n",
       "Data variables:\n",
       "    blue       (time, y, x) float64 684.0 547.0 590.0 649.0 627.0 634.0 ...\n",
       "    green      (time, y, x) float64 964.0 740.0 780.0 843.0 838.0 854.0 ...\n",
       "    red        (time, y, x) float64 1.194e+03 871.0 946.0 1.015e+03 981.0 ...\n",
       "    nir        (time, y, x) float64 2.138e+03 1.914e+03 2.018e+03 2.054e+03 ...\n",
       "    swir1      (time, y, x) float64 2.119e+03 1.715e+03 1.689e+03 1.738e+03 ...\n",
       "    swir2      (time, y, x) float64 1.518e+03 1.065e+03 1.068e+03 1.144e+03 ...\n",
       "    data_perc  (time) float64 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 ...\n",
       "Attributes:\n",
       "    crs:      EPSG:3577"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-15T01:15:34.738782Z",
     "start_time": "2018-06-15T01:15:34.408905Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-f065d1679c20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtci\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtasseled_cap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/dea-notebooks/Scripts/BandIndices.py\u001b[0m in \u001b[0;36mtasseled_cap\u001b[0;34m(sensor_data, tc_bands, drop)\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtc_band\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtc_bands\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;31m# Create xarray of coefficient values used to multiply each band of input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0mcoeff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manalysis_coefficient\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtc_band\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0msensor_coeff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msensor_data\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcoeff\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'xr' is not defined"
     ]
    }
   ],
   "source": [
    "tci = tasseled_cap(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-15T01:15:34.740185Z",
     "start_time": "2018-06-15T01:15:17.012Z"
    }
   },
   "outputs": [],
   "source": [
    "tci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-15T01:15:34.741564Z",
     "start_time": "2018-06-15T01:15:19.648Z"
    }
   },
   "outputs": [],
   "source": [
    "tci.wetness.isel(time=0).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-15T01:15:34.743099Z",
     "start_time": "2018-06-15T01:15:19.992Z"
    }
   },
   "outputs": [],
   "source": [
    "tci.isel(time=0).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### resample gridded rainfall data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T01:03:42.678615Z",
     "start_time": "2018-06-14T01:03:42.581132Z"
    }
   },
   "outputs": [],
   "source": [
    "#resample xarray Dataset Studysite_rain by Annual'AS' to get yearly avg with year starting in october\n",
    "#Note that the resampling we did means that each year is labelled according to its first day  \n",
    "rain_sp = Studysite_rain.mean(dim = ('latitude','longitude'))\n",
    "month_sp = rain_sp.resample('MS', dim = 'time', how = 'mean')\n",
    "year_avg = Studysite_rain.resample('AS-OCT', dim='time', how='mean', keep_attrs=True)\n",
    "# Create a spatial average\n",
    "year_avg_sp = year_avg.mean(dim = ('latitude', 'longitude'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T05:51:43.602480Z",
     "start_time": "2018-06-08T05:51:43.599448Z"
    }
   },
   "source": [
    "### Set up some colour maps for the Hovmoller plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T01:03:45.281690Z",
     "start_time": "2018-06-14T01:03:45.277185Z"
    }
   },
   "outputs": [],
   "source": [
    "#This controls the colour map used for plotting NDVI\n",
    "ndvi_cmap = mpl.colors.ListedColormap(['blue', '#ffcc66','#ffffcc' , '#ccff66' , '#2eb82e', '#009933' , '#006600'])\n",
    "ndvi_bounds = [-1, 0, 0.1, 0.25, 0.35, 0.5, 0.8, 1]\n",
    "#Be aware that clip = True might be causing issues\n",
    "ndvi_norm = mpl.colors.BoundaryNorm(ndvi_bounds, ndvi_cmap.N, clip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up data for Hovmoller plots and calculate NDVI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T00:52:05.612181Z",
     "start_time": "2018-06-13T00:52:05.608275Z"
    }
   },
   "source": [
    "#### Build 2D transect dataset for hovmoller plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T04:01:36.508820Z",
     "start_time": "2018-06-14T04:01:36.492557Z"
    }
   },
   "outputs": [],
   "source": [
    "## Compress \n",
    "crs = ds.crs\n",
    "crs_wkt = ds.crs.wkt\n",
    "geom_w = warp_geometry(geom1, query['crs'], crs_wkt)\n",
    "hov_ds= transect(ds, geom_w, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T10:38:02.758529Z",
     "start_time": "2018-06-13T10:38:02.749255Z"
    }
   },
   "outputs": [],
   "source": [
    "#calculate NDVI here\n",
    "hov_multi_ndvi = ((hov_ds.nir-hov_ds.red)/(hov_ds.nir+hov_ds.red))\n",
    "hov_multi =hov_ds\n",
    "hov_multi_ndvi_drop = hov_multi_ndvi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Hovmoller plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T10:38:04.547655Z",
     "start_time": "2018-06-13T10:38:02.761335Z"
    }
   },
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    #try plotting the first clean time slice to see where we are\n",
    "    print('transect image: ')\n",
    "    DEAPlotting.three_band_image(ds, bands=['swir1','nir','green'],time =0, contrast_enhance=True)\n",
    "    plt.scatter(x=hov_ds.coords['x'], y=hov_ds.coords['y'], c='r',\n",
    "               linewidths='0.01', alpha = 0.5)\n",
    "    plt.savefig('{}{}_HovImg_{}_{}.png'.format(savepath,shape_name,start_of_epoch,end_of_epoch),\n",
    "            bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "#Make a hovmoller plot and save it to file.\n",
    "#Use firstyear and last year to zoom into periods of interest\n",
    "firstyearhov = start_of_epoch#'1987-06-01'\n",
    "lastyearhov = end_of_epoch #'2016-12-31'\n",
    "fig = plt.figure(figsize=(11.27,11.69))\n",
    "gs = gridspec.GridSpec(1, 2, width_ratios=[1,3])\n",
    "ax1 = plt.subplot(gs[0, 0])\n",
    "ax1.plot(month_sp.rainfall, month_sp.time, color='b', alpha = 1)\n",
    "ax1.axes.set_xlabel('ave daily rainfall (mm): monthly (blue) yearly (grey)')\n",
    "\n",
    "#set up fill time as a datetime 64 object for matplotlib input\n",
    "#set up variables to plot the fill behind the yearly rain data\n",
    "filltime = (year_avg_sp.time.astype('datetime64'))\n",
    "zeros = np.zeros(shape=(len(filltime)))\n",
    "plt.fill_betweenx(filltime.time.values, zeros, year_avg_sp.rainfall, color ='k', alpha = 0.8)\n",
    "#set up variables to plot the fill behind the months data\n",
    "mzeros = np.zeros(shape=(len(month_sp.rainfall.time.values)))\n",
    "plt.fill_betweenx(month_sp.rainfall.time.values, mzeros, month_sp.rainfall, color ='c', alpha = 1)\n",
    "plt.axis([0, month_sp.rainfall.max(),lastyearhov , firstyearhov])\n",
    "\n",
    "ax2 = plt.subplot(gs[0, 1])\n",
    "hov_multi_ndvi_drop.plot(x='distance', y='time', yincrease = False, cmap = ndvi_cmap, norm = ndvi_norm, vmin=-1, vmax =1)\n",
    "#fig.delaxes(fig.axes[]) #remove current colour bar\n",
    "plt.axis([0, hov_multi_ndvi_drop.distance.max(), lastyearhov , firstyearhov])\n",
    "ax2.set_anchor(\"SE\")\n",
    "#make a title replacing underscores with spaces\n",
    "plt.suptitle(shape_name.replace('_',' '), fontsize ='24')\n",
    "\n",
    "#save plot to file\n",
    "plt.savefig('{}{}_HovPlot_{}_{}.png'.format(savepath,shape_name,start_of_epoch,end_of_epoch),\n",
    "            bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the data to an output file for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T10:38:04.590905Z",
     "start_time": "2018-06-13T10:38:04.550994Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    #make a dictionary of the data we want to save\n",
    "    hov_data = {'ds':ds, 'Studysite_rain':Studysite_rain,'rain_sp':rain_sp,'month_sp':month_sp,\n",
    "                'year_avg':year_avg,'year_avg_sp':year_avg_sp,'hov_ds':hov_ds, 'start_of_epoch': start_of_epoch, \n",
    "                'end_of_epoch':end_of_epoch,'ndvi_cmap':ndvi_cmap, 'ndvi_norm':ndvi_norm, 'ndvi_bounds':ndvi_bounds}\n",
    "\n",
    "    f = open(savepath+'hov_data_{}_{}_{}'.format(shape_name,start_of_epoch,end_of_epoch)+'.pkl', 'wb')\n",
    "    pickle.dump(hov_data,f) \n",
    "    print('saved data to file')\n",
    "    #pickle.dump(pickle_vars,f,protocol = 2, fix_imports = True) #maintain compatibility with python 2\n",
    "    f.close()\n",
    "except:\n",
    "    print('did not save to file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
