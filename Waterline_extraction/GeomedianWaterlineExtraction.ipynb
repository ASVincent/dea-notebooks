{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract annual geomedian waterlines across time\n",
    "**What does this notebook do?** \n",
    "\n",
    "This notebooks demonstrates how to extract waterline contours from the geomedian composite layers for each year. \n",
    "\n",
    "**Requirements:** \n",
    "\n",
    "You need to run the following commands from the command line prior to launching jupyter notebooks from the same terminal so that the required libraries and paths are set:\n",
    "\n",
    "`module use /g/data/v10/public/modules/modulefiles` \n",
    "\n",
    "`module load dea/20180515`  *(currently using an older version of `dea` due to a bug in `xr.concat`; will be reverted to `module load dea` in future)*\n",
    "\n",
    "If you find an error or bug in this notebook, please either create an 'Issue' in the Github repository, or fix it yourself and create a 'Pull' request to contribute the updated notebook back into the repository (See the repository [README](https://github.com/GeoscienceAustralia/dea-notebooks/blob/master/README.rst) for instructions on creating a Pull request).\n",
    "\n",
    "**Date:** September 2018\n",
    "\n",
    "**Author:** Robbi Bishop-Taylor"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "**Tags**: :index:`tidal_model`, :index:`OTPS`, :index:`tidal_tagging`, :index:`predict_tide`, :index:`composites`, :index:`dask`, :index:`write_geotiff`,  :index:`filmstrip_plot`, :index:`geopandas`, :index:`point_in_polygon`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import datacube\n",
    "import itertools\n",
    "import warnings\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from datacube.utils import geometry\n",
    "from datacube.utils.geometry import CRS\n",
    "from datacube.helpers import write_geotiff\n",
    "from shapely.geometry import Point\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "sys.path.append('../10_Scripts')\n",
    "import SpatialTools\n",
    "\n",
    "# For nicer notebook plotting, hide warnings (comment out for real analysis)\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "# Create datacube instance\n",
    "dc = datacube.Datacube(app='Tidal geomedian filmstrips')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geomedian filmstrip parameters\n",
    "Set the area, time period  and sensors of interest, and tide limits and epoch length used to produce each geomedian composite. This is the only cell that needs to be edited to run the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up centre of study area and buffer size in metres for data extraction\n",
    "study_area = 'hume'  # name used as prefix for output files\n",
    "lat, lon = -36.100, 147.210  # centre of study area\n",
    "buffer = 23000  # metre units to extend region of interest on each side of centre point\n",
    "\n",
    "# Set up query\n",
    "x, y = geometry.point(lon, lat, CRS('WGS84')).to_crs(CRS('EPSG:3577')).points[0]\n",
    "query = {'x': (x - buffer, x + buffer),\n",
    "         'y': (y - buffer, y + buffer),\n",
    "         'crs': 'EPSG:3577'}\n",
    "\n",
    "query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine multiple sensors, load data and generate geomedians\n",
    "For each epoch, combine all sensors into one dataset, load the data for the first time using `dask`'s `.compute()`, then composite all timesteps into a single array using a geometric median computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:  (time: 5, x: 1841, y: 1841)\n",
       "Coordinates:\n",
       "  * time     (time) datetime64[ns] 2013-01-01 2014-01-01 2015-01-01 ...\n",
       "  * y        (y) float64 -4.003e+06 -4.003e+06 -4.003e+06 -4.003e+06 ...\n",
       "  * x        (x) float64 1.344e+06 1.344e+06 1.344e+06 1.344e+06 1.344e+06 ...\n",
       "Data variables:\n",
       "    blue     (time, y, x) int16 452 438 447 455 440 434 422 427 429 433 416 ...\n",
       "    green    (time, y, x) int16 792 783 804 835 837 823 799 780 769 761 769 ...\n",
       "    red      (time, y, x) int16 698 682 708 728 681 660 627 658 673 684 626 ...\n",
       "    nir      (time, y, x) int16 4180 4165 4186 4280 4479 4638 4546 4043 3970 ...\n",
       "    swir1    (time, y, x) int16 2297 2275 2321 2374 2304 2261 2179 2166 2165 ...\n",
       "    swir2    (time, y, x) int16 1209 1185 1214 1247 1172 1125 1087 1103 1123 ...\n",
       "Attributes:\n",
       "    crs:      EPSG:3577"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Dict to hold output geomedian composits\n",
    "# all_data = []\n",
    "\n",
    "# time_dict = {'ls5': ('1987-01-01', '2018-01-01'),\n",
    "#             'ls7': ('1987-01-01', '2003-05-30'),\n",
    "#             'ls8': ('1987-01-01', '2018-01-01')}\n",
    "\n",
    "# for sensor in ['ls5', 'ls7', 'ls8']:\n",
    "\n",
    "#     # Return observations matching query without actually loading them using dask\n",
    "#     sensor_all = dc.load(product = '{}_nbart_geomedian_annual'.format(sensor), \n",
    "#                      group_by = 'solar_day', \n",
    "#                      time=time_dict[sensor],\n",
    "# #                      dask_chunks={'time': 1},\n",
    "#                      **query)\n",
    "    \n",
    "#     all_data.append(sensor_all)\n",
    "\n",
    "# # Import data\n",
    "# all_data \n",
    "\n",
    "# xr.concat()  #.mean(dim=['concat_dims'])  #[['red', 'green', 'blue']].to_array()  #.plot.imshow(robust=True)\n",
    "\n",
    "\n",
    "# for from_date in sensor_epoch_dict.keys():\n",
    "    \n",
    "\n",
    "    \n",
    "#     # Compute NDWI\n",
    "#     # sensor_combined[\"ndwi\"] = (sensor_combined.green - sensor_combined.nir) / (sensor_combined.green + sensor_combined.nir)\n",
    "#     sensor_combined[\"ndwi\"] = (sensor_combined.green - sensor_combined.swir1) / (sensor_combined.green + sensor_combined.swir1)\n",
    "\n",
    "#     # Compute NDWI composite using all timesteps\n",
    "#     print('    Computing NDWI median')\n",
    "#     ndwi_median = sensor_combined[[\"ndwi\"]].median(dim='time', keep_attrs=True)\n",
    "    \n",
    "#     # Export to file\n",
    "#     filename = 'output_data/{0}/{0}_{1}.tif'.format(study_area, from_date)\n",
    "#     print('    Exporting to {}'.format(filename))\n",
    "#     write_geotiff(filename=filename, dataset=ndwi_median)\n",
    "    \n",
    "#     # Assign to dict\n",
    "#     ndwi_dict[from_date] = ndwi_median\n",
    "\n",
    "import datacube\n",
    "dc = datacube.Datacube(app='Annual geomedians')\n",
    "    \n",
    "# Return observations matching query without actually loading them using dask\n",
    "sensor_all = dc.load(product = 'ls8_nbart_geomedian_annual', \n",
    "                     time=('1987-01-01', '2018-01-01'),\n",
    "                     x=(1343834.550438912, 1389834.550438912),\n",
    "                     y=(-4048783.3089217427, -4002783.3089217427),\n",
    "                     crs='EPSG:3577')\n",
    "\n",
    "sensor_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract waterlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for date, ndwi in ndwi_dict.items():\n",
    "    \n",
    "    print(date)\n",
    "    \n",
    "    # Prepare attributes as input to contour extract\n",
    "    attribute_data = {'date': [date[0:4]]}\n",
    "    attribute_dtypes = {'date': 'int'}\n",
    "    \n",
    "    # Extract contours with custom attribute fields:\n",
    "    contour_dict = SpatialTools.contour_extract(z_values=[0],\n",
    "                                   ds_array=ndwi.ndwi,\n",
    "                                   ds_crs='epsg:3577',\n",
    "                                   ds_affine=ndwi.geobox.transform,\n",
    "                                   output_shp=f'output_data/{study_area}/{study_area}_{date}.shp',\n",
    "                                   attribute_data=attribute_data,\n",
    "                                   attribute_dtypes=attribute_dtypes)\n",
    "    \n",
    "# Combine all shapefiles into one file\n",
    "import glob\n",
    "shapefiles = glob.glob(f'output_data/{study_area}/{study_area}_*01-01.shp')\n",
    "gdf = pd.concat([gpd.read_file(shp) for shp in shapefiles], sort=False).pipe(gpd.GeoDataFrame)\n",
    "\n",
    "# Save as combined shapefile\n",
    "gdf = gdf.reset_index()[['date', 'geometry']].sort_values('date')\n",
    "gdf.crs = 'epsg:3577'\n",
    "gdf.to_file(f'output_data/{study_area}/{study_area}_combined.shp')\n",
    "\n",
    "gdf.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create tidal modelled vs observed plot\n",
    "Create a plot comparing selected Landsat observations to all Landsat observations and the entire tidal history of the study area:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each hour between start and end of time series, predict tide and add to list\n",
    "all_times = date_range(start, end, 1, 'hours')\n",
    "tp_model = [TimePoint(tidepost_lon, tidepost_lat, dt) for dt in all_times]\n",
    "tides_model = [tide.tide_m for tide in predict_tide(tp_model)]\n",
    "\n",
    "# Covert to dataframe of modelled dates and tidal heights\n",
    "modelled_df = pd.DataFrame({'tide_heights': tides_model}, index=pd.DatetimeIndex(all_times))\n",
    "\n",
    "# Return dataframe of previously-generated observed dates and tidal heights\n",
    "observed_df = xr.concat([i.tide_heights for i in sensor_dict.values()], dim='time').to_dataframe()   \n",
    "\n",
    "# Set up plot\n",
    "fig = plt.figure(figsize=(10, 4))\n",
    "plt.margins(0)\n",
    "fig.axes[0].spines['right'].set_visible(False)\n",
    "fig.axes[0].spines['top'].set_visible(False)\n",
    "fig.axes[0].yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "plt.ylabel('Tide height (m)')\n",
    "\n",
    "plt.annotate('Tidepost: {} E, {} S ({})'.format(tidepost_lon, tidepost_lat, study_area), \n",
    "             xy=(0, 1.2), xycoords='axes fraction', \n",
    "             xytext=(5, -3), textcoords='offset points',\n",
    "             fontsize=10, verticalalignment='top', fontweight='bold') \n",
    "\n",
    "# Plot modelled values as grey background\n",
    "plt.plot(modelled_df.index, modelled_df.tide_heights,\n",
    "         color='gainsboro', linewidth=0.6, zorder=1, label='OTPS model')\n",
    "\n",
    "# Plot all observations as black points\n",
    "plt.scatter(observed_df.index, observed_df.tide_heights,\n",
    "            s=4, color='darkgrey', marker='o', zorder=2, label='All observations')\n",
    "\n",
    "# Plot selected observations in red by clipping observed_df to min and max tide selection\n",
    "selected_df = observed_df[observed_df.tide_heights.between(sel_min-0.001, sel_max+0.001)]\n",
    "plt.scatter(selected_df.index, selected_df.tide_heights,\n",
    "            s=10, color='black', marker='o', zorder=2, label='Selected observations')\n",
    "\n",
    "# Plot horizontal lines defining border of selected tidal range\n",
    "plt.axhline(y=sel_min, color='red', alpha=0.2) \n",
    "plt.axhline(y=sel_max, color='red', alpha=0.2) \n",
    "\n",
    "# Add vertical lines and annotation defining each epoch\n",
    "for epoch in epochs[:-1]:\n",
    "    \n",
    "    # Compute from and to date strings\n",
    "    from_date = epoch.strftime('%Y-%m-%d')\n",
    "    to_date = (epoch + relativedelta(years=epoch_years)).strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Add vertical line and epoch titles\n",
    "    plt.axvline(x=epoch, color='red', alpha=0.2)\n",
    "    plt.annotate('{} to {}'.format(from_date, to_date), \n",
    "                 xy=(epoch, modelled_df.tide_heights.max()), \n",
    "                 xytext=(5, 10), textcoords='offset points', fontsize=7) \n",
    "    \n",
    "# Add legend\n",
    "plt.legend(bbox_to_anchor=(0.975, 1.2), loc=1, borderaxespad=0, ncol=3, \n",
    "           handletextpad=0.1, frameon=False, columnspacing=0.3, fontsize=7)\n",
    "\n",
    "# Export plot\n",
    "filename = 'figures/{0}/{0}_tideobs.png'.format(study_area)\n",
    "print('    Exporting to {}'.format(filename))\n",
    "# fig.savefig(filename, dpi=300, bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = datacube.Datacube(config='/home/561/rt1527/unpublished_products.conf')\n",
    "dc.list_products()\n",
    "\n",
    "# Set up centre of area to analyse, and a buffer in metres around this centrepoint\n",
    "lat, lon, buffer_m, name = -17.6080348351, 139.80119891, 5000, 'mangrove_test'\n",
    "time_range = ('1986-01-01', '2019-09-01')\n",
    "resolution = (-25, 25)\n",
    "\n",
    "x, y = geometry.point(lon, lat, CRS('WGS84')).to_crs(CRS('EPSG:3577')).points[0]\n",
    "query = {'x': (x - buffer_m - 3000, x + buffer_m + 3000),\n",
    "         'y': (y - buffer_m, y + buffer_m),    \n",
    "         'time': time_range,\n",
    "         'crs': 'EPSG:3577',\n",
    "         'output_crs': 'EPSG:3577',\n",
    "         'resolution': resolution} \n",
    "\n",
    "\n",
    "test = dc.load(product=\"mangrove_extent_cover_albers\", **query)\n",
    "\n",
    "import DEAPlotting\n",
    "DEAPlotting.animated_timeseries(ds=test.isel(time=[1, 5, 10]),\n",
    "                                output_path=f'animated_timeseries_{name}.gif',\n",
    "                                bands=['canopy_cover_class'],\n",
    "                                interval=500,\n",
    "                                width_pixels=1000,\n",
    "                                percentile_stretch=[0.0, 1.0],\n",
    "                                show_date=False,\n",
    "                                onebandplot_kwargs={'cmap':'jet'},\n",
    "#                                 onebandplot_cbar = False,\n",
    "                                title=test.time.dt.year.values.tolist())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
