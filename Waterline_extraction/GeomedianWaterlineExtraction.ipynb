{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract annual geomedian waterlines across time\n",
    "**What does this notebook do?** \n",
    "\n",
    "This notebooks demonstrates how to extract waterline contours from the geomedian composite layers for each year. \n",
    "\n",
    "**Requirements:** \n",
    "\n",
    "You need to run the following commands from the command line prior to launching jupyter notebooks from the same terminal so that the required libraries and paths are set:\n",
    "\n",
    "`module use /g/data/v10/public/modules/modulefiles` \n",
    "\n",
    "`module load dea/20180515`  *(currently using an older version of `dea` due to a bug in `xr.concat`; will be reverted to `module load dea` in future)*\n",
    "\n",
    "If you find an error or bug in this notebook, please either create an 'Issue' in the Github repository, or fix it yourself and create a 'Pull' request to contribute the updated notebook back into the repository (See the repository [README](https://github.com/GeoscienceAustralia/dea-notebooks/blob/master/README.rst) for instructions on creating a Pull request).\n",
    "\n",
    "**Date:** September 2018\n",
    "\n",
    "**Author:** Robbi Bishop-Taylor"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "**Tags**: :index:`tidal_model`, :index:`OTPS`, :index:`tidal_tagging`, :index:`predict_tide`, :index:`composites`, :index:`dask`, :index:`write_geotiff`,  :index:`filmstrip_plot`, :index:`geopandas`, :index:`point_in_polygon`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import datacube\n",
    "import itertools\n",
    "import warnings\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from datacube.utils import geometry\n",
    "from datacube.utils.geometry import CRS\n",
    "from datacube.helpers import write_geotiff\n",
    "from shapely.geometry import Point\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "sys.path.append('../10_Scripts')\n",
    "import SpatialTools\n",
    "\n",
    "# For nicer notebook plotting, hide warnings (comment out for real analysis)\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "# Create datacube instance\n",
    "dc = datacube.Datacube(app='Tidal geomedian filmstrips')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geomedian filmstrip parameters\n",
    "Set the area, time period  and sensors of interest, and tide limits and epoch length used to produce each geomedian composite. This is the only cell that needs to be edited to run the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up centre of study area and buffer size in metres for data extraction\n",
    "study_area = 'hume'  # name used as prefix for output files\n",
    "lat, lon = -36.100, 147.210  # centre of study area\n",
    "buffer = 23000  # metre units to extend region of interest on each side of centre point\n",
    "\n",
    "# Set up query\n",
    "x, y = geometry.point(lon, lat, CRS('WGS84')).to_crs(CRS('EPSG:3577')).points[0]\n",
    "query = {'x': (x - buffer, x + buffer),\n",
    "         'y': (y - buffer, y + buffer),\n",
    "         'crs': 'EPSG:3577'}\n",
    "\n",
    "query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine multiple sensors, load data and generate geomedians\n",
    "For each epoch, combine all sensors into one dataset, load the data for the first time using `dask`'s `.compute()`, then composite all timesteps into a single array using a geometric median computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error opening source dataset: NetCDF:/g/data2/fk4/datacube/002/stats/geomedian/annual/ls5_gm/13_-41/LS5_GM_NBART_13_-41_19870101.nc:blue\n"
     ]
    },
    {
     "ename": "RasterioIOError",
     "evalue": "NetCDF:/g/data2/fk4/datacube/002/stats/geomedian/annual/ls5_gm/13_-41/LS5_GM_NBART_13_-41_19870101.nc:blue: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCPLE_OpenFailedError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32mrasterio/_base.pyx\u001b[0m in \u001b[0;36mrasterio._base.DatasetBase.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mrasterio/_shim.pyx\u001b[0m in \u001b[0;36mrasterio._shim.open_dataset\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mrasterio/_err.pyx\u001b[0m in \u001b[0;36mrasterio._err.exc_wrap_pointer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mCPLE_OpenFailedError\u001b[0m: NetCDF:/g/data2/fk4/datacube/002/stats/geomedian/annual/ls5_gm/13_-41/LS5_GM_NBART_13_-41_19870101.nc:blue: No such file or directory",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRasterioIOError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-a79de4be7859>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m                      \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1343834.550438912\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1389834.550438912\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                      \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4048783.3089217427\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m4002783.3089217427\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m                      crs='EPSG:3577')\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0msensor_all\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/g/data/v10/public/modules/dea/20180815/lib/python3.6/site-packages/datacube/api/core.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, product, measurements, output_crs, resolution, resampling, dask_chunks, like, fuse_func, align, datasets, use_threads, **query)\u001b[0m\n\u001b[1;32m    309\u001b[0m                                 \u001b[0mfuse_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfuse_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                                 \u001b[0mdask_chunks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdask_chunks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m                                 use_threads=use_threads)\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mapply_aliases\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatacube_product\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeasurements\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/g/data/v10/public/modules/dea/20180815/lib/python3.6/site-packages/datacube/api/core.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(sources, geobox, measurements, fuse_func, dask_chunks, skip_broken_datasets, use_threads)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         return Datacube.create_storage(OrderedDict((dim, sources.coords[dim]) for dim in sources.dims),\n\u001b[0;32m--> 525\u001b[0;31m                                        geobox, measurements, data_func, use_threads)\n\u001b[0m\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/g/data/v10/public/modules/dea/20180815/lib/python3.6/site-packages/datacube/api/core.py\u001b[0m in \u001b[0;36mcreate_storage\u001b[0;34m(coords, geobox, measurements, data_func, use_threads)\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwork_measurements\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeasurements\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmeasurements\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmeasurement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmeasurements\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/g/data/v10/public/modules/dea/20180815/lib/python3.6/site-packages/datacube/api/core.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwork_measurements\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeasurements\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmeasurements\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmeasurement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmeasurements\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/g/data/v10/public/modules/dea/20180815/lib/python3.6/site-packages/datacube/api/core.py\u001b[0m in \u001b[0;36mdata_func\u001b[0;34m(measurement)\u001b[0m\n\u001b[1;32m    499\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndenumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msources\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m                         _fuse_measurement(data[index], datasets, geobox, measurement, fuse_func=fuse_func,\n\u001b[0;32m--> 501\u001b[0;31m                                           skip_broken_datasets=skip_broken_datasets)\n\u001b[0m\u001b[1;32m    502\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m                     \u001b[0;32mdef\u001b[0m \u001b[0mwork_load_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/g/data/v10/public/modules/dea/20180815/lib/python3.6/site-packages/datacube/api/core.py\u001b[0m in \u001b[0;36m_fuse_measurement\u001b[0;34m(dest, datasets, geobox, measurement, skip_broken_datasets, fuse_func)\u001b[0m\n\u001b[1;32m    639\u001b[0m                        \u001b[0mresampling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmeasurement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'resampling_method'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'nearest'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m                        \u001b[0mfuse_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfuse_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 641\u001b[0;31m                        skip_broken_datasets=skip_broken_datasets)\n\u001b[0m\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/g/data/v10/public/modules/dea/20180815/lib/python3.6/site-packages/datacube/storage/storage.py\u001b[0m in \u001b[0;36mreproject_and_fuse\u001b[0;34m(datasources, destination, dst_transform, dst_projection, dst_nodata, resampling, fuse_func, skip_broken_datasets)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasources\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mignore_exceptions_if\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskip_broken_datasets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m             \u001b[0mread_from_source\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasources\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestination\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst_transform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst_nodata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst_projection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresampling\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdestination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/g/data/v10/public/modules/dea/20180815/lib/python3.6/site-packages/datacube/storage/storage.py\u001b[0m in \u001b[0;36mread_from_source\u001b[0;34m(source, dest, dst_transform, dst_nodata, dst_projection, resampling)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mData\u001b[0m \u001b[0mdestination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \"\"\"\n\u001b[0;32m--> 146\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m         \u001b[0marray_transform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m~\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdst_transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;31m# if the CRS is the same use decimated reads if possible (NN or 1:1 scaling)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/g/data/v10/public/modules/dea-env/20180728/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/g/data/v10/public/modules/dea/20180815/lib/python3.6/site-packages/datacube/storage/storage.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m             \u001b[0m_LOG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error opening source dataset: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/g/data/v10/public/modules/dea/20180815/lib/python3.6/site-packages/datacube/storage/storage.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0m_LOG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"opening %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mrasterio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m                 \u001b[0moverride\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/g/data/v10/public/modules/dea-env/20180728/lib/python3.6/site-packages/rasterio/env.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    401\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/g/data/v10/public/modules/dea-env/20180728/lib/python3.6/site-packages/rasterio/__init__.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, driver, width, height, count, crs, transform, dtype, nodata, sharing, **kwargs)\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;31m# None.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDatasetReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_writer_for_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mrasterio/_base.pyx\u001b[0m in \u001b[0;36mrasterio._base.DatasetBase.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mRasterioIOError\u001b[0m: NetCDF:/g/data2/fk4/datacube/002/stats/geomedian/annual/ls5_gm/13_-41/LS5_GM_NBART_13_-41_19870101.nc:blue: No such file or directory"
     ]
    }
   ],
   "source": [
    "# # Dict to hold output geomedian composits\n",
    "# all_data = []\n",
    "\n",
    "# time_dict = {'ls5': ('1987-01-01', '2018-01-01'),\n",
    "#             'ls7': ('1987-01-01', '2003-05-30'),\n",
    "#             'ls8': ('1987-01-01', '2018-01-01')}\n",
    "\n",
    "# for sensor in ['ls5', 'ls7', 'ls8']:\n",
    "\n",
    "#     # Return observations matching query without actually loading them using dask\n",
    "#     sensor_all = dc.load(product = '{}_nbart_geomedian_annual'.format(sensor), \n",
    "#                      group_by = 'solar_day', \n",
    "#                      time=time_dict[sensor],\n",
    "# #                      dask_chunks={'time': 1},\n",
    "#                      **query)\n",
    "    \n",
    "#     all_data.append(sensor_all)\n",
    "\n",
    "# # Import data\n",
    "# all_data \n",
    "\n",
    "# xr.concat()  #.mean(dim=['concat_dims'])  #[['red', 'green', 'blue']].to_array()  #.plot.imshow(robust=True)\n",
    "\n",
    "\n",
    "# for from_date in sensor_epoch_dict.keys():\n",
    "    \n",
    "\n",
    "    \n",
    "#     # Compute NDWI\n",
    "#     # sensor_combined[\"ndwi\"] = (sensor_combined.green - sensor_combined.nir) / (sensor_combined.green + sensor_combined.nir)\n",
    "#     sensor_combined[\"ndwi\"] = (sensor_combined.green - sensor_combined.swir1) / (sensor_combined.green + sensor_combined.swir1)\n",
    "\n",
    "#     # Compute NDWI composite using all timesteps\n",
    "#     print('    Computing NDWI median')\n",
    "#     ndwi_median = sensor_combined[[\"ndwi\"]].median(dim='time', keep_attrs=True)\n",
    "    \n",
    "#     # Export to file\n",
    "#     filename = 'output_data/{0}/{0}_{1}.tif'.format(study_area, from_date)\n",
    "#     print('    Exporting to {}'.format(filename))\n",
    "#     write_geotiff(filename=filename, dataset=ndwi_median)\n",
    "    \n",
    "#     # Assign to dict\n",
    "#     ndwi_dict[from_date] = ndwi_median\n",
    "\n",
    "import datacube\n",
    "dc = datacube.Datacube(app='Annual geomedians')\n",
    "    \n",
    "# Return observations matching query without actually loading them using dask\n",
    "sensor_all = dc.load(product = 'ls5_nbart_geomedian_annual', \n",
    "                     time=('1987-01-01', '2018-01-01'),\n",
    "                     x=(1343834.550438912, 1389834.550438912),\n",
    "                     y=(-4048783.3089217427, -4002783.3089217427),\n",
    "                     crs='EPSG:3577')\n",
    "\n",
    "sensor_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_dict['ls7']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract waterlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for date, ndwi in ndwi_dict.items():\n",
    "    \n",
    "    print(date)\n",
    "    \n",
    "    # Prepare attributes as input to contour extract\n",
    "    attribute_data = {'date': [date[0:4]]}\n",
    "    attribute_dtypes = {'date': 'int'}\n",
    "    \n",
    "    # Extract contours with custom attribute fields:\n",
    "    contour_dict = SpatialTools.contour_extract(z_values=[0],\n",
    "                                   ds_array=ndwi.ndwi,\n",
    "                                   ds_crs='epsg:3577',\n",
    "                                   ds_affine=ndwi.geobox.transform,\n",
    "                                   output_shp=f'output_data/{study_area}/{study_area}_{date}.shp',\n",
    "                                   attribute_data=attribute_data,\n",
    "                                   attribute_dtypes=attribute_dtypes)\n",
    "    \n",
    "# Combine all shapefiles into one file\n",
    "import glob\n",
    "shapefiles = glob.glob(f'output_data/{study_area}/{study_area}_*01-01.shp')\n",
    "gdf = pd.concat([gpd.read_file(shp) for shp in shapefiles], sort=False).pipe(gpd.GeoDataFrame)\n",
    "\n",
    "# Save as combined shapefile\n",
    "gdf = gdf.reset_index()[['date', 'geometry']].sort_values('date')\n",
    "gdf.crs = 'epsg:3577'\n",
    "gdf.to_file(f'output_data/{study_area}/{study_area}_combined.shp')\n",
    "\n",
    "gdf.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create tidal modelled vs observed plot\n",
    "Create a plot comparing selected Landsat observations to all Landsat observations and the entire tidal history of the study area:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each hour between start and end of time series, predict tide and add to list\n",
    "all_times = date_range(start, end, 1, 'hours')\n",
    "tp_model = [TimePoint(tidepost_lon, tidepost_lat, dt) for dt in all_times]\n",
    "tides_model = [tide.tide_m for tide in predict_tide(tp_model)]\n",
    "\n",
    "# Covert to dataframe of modelled dates and tidal heights\n",
    "modelled_df = pd.DataFrame({'tide_heights': tides_model}, index=pd.DatetimeIndex(all_times))\n",
    "\n",
    "# Return dataframe of previously-generated observed dates and tidal heights\n",
    "observed_df = xr.concat([i.tide_heights for i in sensor_dict.values()], dim='time').to_dataframe()   \n",
    "\n",
    "# Set up plot\n",
    "fig = plt.figure(figsize=(10, 4))\n",
    "plt.margins(0)\n",
    "fig.axes[0].spines['right'].set_visible(False)\n",
    "fig.axes[0].spines['top'].set_visible(False)\n",
    "fig.axes[0].yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "plt.ylabel('Tide height (m)')\n",
    "\n",
    "plt.annotate('Tidepost: {} E, {} S ({})'.format(tidepost_lon, tidepost_lat, study_area), \n",
    "             xy=(0, 1.2), xycoords='axes fraction', \n",
    "             xytext=(5, -3), textcoords='offset points',\n",
    "             fontsize=10, verticalalignment='top', fontweight='bold') \n",
    "\n",
    "# Plot modelled values as grey background\n",
    "plt.plot(modelled_df.index, modelled_df.tide_heights,\n",
    "         color='gainsboro', linewidth=0.6, zorder=1, label='OTPS model')\n",
    "\n",
    "# Plot all observations as black points\n",
    "plt.scatter(observed_df.index, observed_df.tide_heights,\n",
    "            s=4, color='darkgrey', marker='o', zorder=2, label='All observations')\n",
    "\n",
    "# Plot selected observations in red by clipping observed_df to min and max tide selection\n",
    "selected_df = observed_df[observed_df.tide_heights.between(sel_min-0.001, sel_max+0.001)]\n",
    "plt.scatter(selected_df.index, selected_df.tide_heights,\n",
    "            s=10, color='black', marker='o', zorder=2, label='Selected observations')\n",
    "\n",
    "# Plot horizontal lines defining border of selected tidal range\n",
    "plt.axhline(y=sel_min, color='red', alpha=0.2) \n",
    "plt.axhline(y=sel_max, color='red', alpha=0.2) \n",
    "\n",
    "# Add vertical lines and annotation defining each epoch\n",
    "for epoch in epochs[:-1]:\n",
    "    \n",
    "    # Compute from and to date strings\n",
    "    from_date = epoch.strftime('%Y-%m-%d')\n",
    "    to_date = (epoch + relativedelta(years=epoch_years)).strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Add vertical line and epoch titles\n",
    "    plt.axvline(x=epoch, color='red', alpha=0.2)\n",
    "    plt.annotate('{} to {}'.format(from_date, to_date), \n",
    "                 xy=(epoch, modelled_df.tide_heights.max()), \n",
    "                 xytext=(5, 10), textcoords='offset points', fontsize=7) \n",
    "    \n",
    "# Add legend\n",
    "plt.legend(bbox_to_anchor=(0.975, 1.2), loc=1, borderaxespad=0, ncol=3, \n",
    "           handletextpad=0.1, frameon=False, columnspacing=0.3, fontsize=7)\n",
    "\n",
    "# Export plot\n",
    "filename = 'figures/{0}/{0}_tideobs.png'.format(study_area)\n",
    "print('    Exporting to {}'.format(filename))\n",
    "# fig.savefig(filename, dpi=300, bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
