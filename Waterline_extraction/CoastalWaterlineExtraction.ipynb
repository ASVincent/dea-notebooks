{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract coastal waterlines across time\n",
    "**What does this notebook do?** \n",
    "\n",
    "This notebooks demonstrates how to tidally tag remotely sensed imagery using the [OSU Tidal Prediction Software or OTPS](http://volkov.oce.orst.edu/tides/otps.html) model, create geomedian composites for a given set of epochs and tidal range, and extract waterline contours from the composite layers for each epoch. \n",
    "\n",
    "**Requirements:** \n",
    "\n",
    "You need to run the following commands from the command line prior to launching jupyter notebooks from the same terminal so that the required libraries and paths are set:\n",
    "\n",
    "`module use /g/data/v10/public/modules/modulefiles` \n",
    "\n",
    "`module load dea/20180515`  *(currently using an older version of `dea` due to a bug in `xr.concat`; will be reverted to `module load dea` in future)*\n",
    "\n",
    "`module load otps`\n",
    "\n",
    "If you find an error or bug in this notebook, please either create an 'Issue' in the Github repository, or fix it yourself and create a 'Pull' request to contribute the updated notebook back into the repository (See the repository [README](https://github.com/GeoscienceAustralia/dea-notebooks/blob/master/README.rst) for instructions on creating a Pull request).\n",
    "\n",
    "**Date:** September 2018\n",
    "\n",
    "**Author:** Robbi Bishop-Taylor"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "**Tags**: :index:`tidal_model`, :index:`OTPS`, :index:`tidal_tagging`, :index:`predict_tide`, :index:`composites`, :index:`dask`, :index:`write_geotiff`,  :index:`filmstrip_plot`, :index:`geopandas`, :index:`point_in_polygon`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import datacube\n",
    "import itertools\n",
    "import warnings\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from otps import TimePoint\n",
    "from otps import predict_tide\n",
    "from datetime import datetime, timedelta\n",
    "from datacube.utils import geometry\n",
    "from datacube.utils.geometry import CRS\n",
    "from datacube_stats.statistics import GeoMedian\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from datacube.helpers import ga_pq_fuser\n",
    "from datacube.helpers import write_geotiff\n",
    "from datacube.storage import masking\n",
    "from shapely.geometry import Point\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "sys.path.append('../10_Scripts')\n",
    "import SpatialTools\n",
    "\n",
    "# For nicer notebook plotting, hide warnings (comment out for real analysis)\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "# Create datacube instance\n",
    "dc = datacube.Datacube(app='Tidal geomedian filmstrips')\n",
    "\n",
    "def date_range(start_date, end_date, increment, period):\n",
    "    \n",
    "    \"\"\"Generate dates seperated by given time increment/period\"\"\"\n",
    "    \n",
    "    result = []\n",
    "    nxt = start_date\n",
    "    delta = relativedelta(**{period:increment})\n",
    "    while nxt <= end_date:\n",
    "        result.append(nxt)\n",
    "        nxt += delta\n",
    "    return result\n",
    "\n",
    "def ds_to_rgb(ds, bands=['red', 'green', 'blue'], reflect_stand=4000):\n",
    "    \n",
    "    \"\"\"Converts an xarray dataset to a three band array\"\"\"\n",
    "    \n",
    "    rawimg = np.transpose(ds[bands].to_array().data, [1, 2, 0])\n",
    "    img_toshow = (rawimg / reflect_stand).clip(0, 1)\n",
    "    return img_toshow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "('Unknown arguments: ', {'time'})",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-30b02dfe3353>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m          \u001b[0;34m'resolution'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m            }\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mARDremotedc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproduct\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ls8_ard'\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;34m**\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/g/data/v10/public/modules/dea/20180926/lib/python3.6/site-packages/datacube/api/core.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, product, measurements, output_crs, resolution, resampling, dask_chunks, like, fuse_func, align, datasets, use_threads, **query)\u001b[0m\n\u001b[1;32m    289\u001b[0m                                      \"please apply `xarray.Dataset.to_array()` to the result instead\")\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m         \u001b[0mobservations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproduct\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproduct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlike\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlike\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mobservations\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mxarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/g/data/v10/public/modules/dea/20180926/lib/python3.6/site-packages/datacube/api/core.py\u001b[0m in \u001b[0;36mfind_datasets\u001b[0;34m(self, **search_terms)\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mseealso\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mmeth\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mgroup_datasets\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mmeth\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mmeth\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mfind_datasets_lazy\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \"\"\"\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_datasets_lazy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0msearch_terms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfind_datasets_lazy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/g/data/v10/public/modules/dea/20180926/lib/python3.6/site-packages/datacube/api/core.py\u001b[0m in \u001b[0;36mfind_datasets_lazy\u001b[0;34m(self, limit, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mseealso\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mmeth\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mgroup_datasets\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mmeth\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mmeth\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mfind_datasets\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \"\"\"\n\u001b[0;32m--> 342\u001b[0;31m         \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQuery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproduct\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"must specify a product\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/g/data/v10/public/modules/dea/20180926/lib/python3.6/site-packages/datacube/api/query.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, index, product, geopolygon, like, **search_terms)\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;31m# TODO: What about keys source filters, and what if the keys don't match up with this product...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0munknown_keys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unknown arguments: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munknown_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: ('Unknown arguments: ', {'time'})"
     ]
    }
   ],
   "source": [
    "\n",
    "from datacube import Datacube\n",
    "from datacube.model import Range\n",
    "from datetime import datetime\n",
    "ARDremotedc = Datacube(config='/home/561/rt1527/.ard-interoperability.conf')\n",
    " \n",
    "date_1= datetime(2016, 8, 1, 0, 0)\n",
    "date_2= datetime(2016, 8, 17, 0, 0)\n",
    "x1=140.22\n",
    "x2=145.23\n",
    "y1=-34.58\n",
    "y2=-34.59\n",
    "query = {\n",
    "         'time': (date_1,date_2),\n",
    "         'x' : (x1, x2),\n",
    "         'y' : (y1, y2),\n",
    "         'output_crs' : 'EPSG:4326',\n",
    "         'resolution': (-0.0001,0.0001)\n",
    "           }\n",
    "data = ARDremotedc.load(product='ls8_ard',  **query)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      values       date\n",
      "0   1.490608 2011-01-02\n",
      "1   0.716810 2011-01-09\n",
      "2   0.124855 2011-01-16\n",
      "3  -0.402116 2011-01-23\n",
      "4   0.054888 2011-01-30\n",
      "5  -0.778704 2011-02-06\n",
      "6   0.563152 2011-02-13\n",
      "7  -1.343159 2011-02-20\n",
      "8   0.244016 2011-02-27\n",
      "9  -0.803574 2011-03-06\n",
      "10 -1.215627 2011-03-13\n",
      "11 -2.288238 2011-03-20\n",
      "12 -0.449190 2011-03-27\n",
      "13 -0.280904 2011-04-03\n",
      "14  0.205058 2011-04-10\n",
      "15  0.157664 2011-04-17\n",
      "16  0.889344 2011-04-24\n",
      "17  1.138114 2011-05-01\n",
      "18 -0.249259 2011-05-08\n",
      "19  0.120709 2011-05-15\n",
      "20  1.371686 2011-05-22\n",
      "21  0.040607 2011-05-29\n",
      "22  1.177085 2011-06-05\n",
      "23 -0.386156 2011-06-12\n",
      "24 -0.649618 2011-06-19\n",
      "25  1.167408 2011-03-13\n",
      "26  0.338236 2011-03-20\n",
      "27  0.887707 2011-03-27\n",
      "28  1.513982 2011-04-03\n",
      "29  1.492196 2011-04-10\n",
      "30  0.030728 2011-04-17\n",
      "31 -1.296572 2011-04-24\n",
      "32 -0.781270 2011-05-01\n",
      "33  0.471042 2011-05-08\n",
      "34  0.447963 2011-05-15\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>values</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.649618</td>\n",
       "      <td>2011-06-19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      values       date\n",
       "24 -0.649618 2011-06-19"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Example data\n",
    "dates = pd.date_range('2011-01-01', periods=25, freq='W')\n",
    "dates = dates.append(dates[10:20])  # Create duplicates\n",
    "df = pd.DataFrame({'values': np.random.randn(len(dates)),\n",
    "                   'date': dates}, index=range(len(dates)))\n",
    "print(df)\n",
    "\n",
    "# Example date\n",
    "DateToExtract = dt.datetime(2012, 1, 1)\n",
    "\n",
    "# Row that has the smallest absolute difference (i.e. either before\n",
    "# or after the DateToExtract is the closest; get index\n",
    "closest_index = abs(df.date - DateToExtract).idxmin()\n",
    "\n",
    "# Subset\n",
    "df.iloc[[closest_index]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geomedian filmstrip parameters\n",
    "Set the area, time period  and sensors of interest, and tide limits and epoch length used to produce each geomedian composite. This is the only cell that needs to be edited to run the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set up centre of study area and buffer size in metres for data extraction\n",
    "# study_area = 'pointstuart_wide'  # name used as prefix for output files\n",
    "# lat, lon = -12.238, 131.849  # centre of study area\n",
    "# buffer = 15000  # metre units to extend region of interest on each side of centre point\n",
    "\n",
    "# # Set up centre of study area and buffer size in metres for data extraction\n",
    "# study_area = 'mitchell'  # name used as prefix for output files\n",
    "# lat, lon = -15.14995, 141.63650  # centre of study area\n",
    "# buffer = 10000  # metre units to extend region of interest on each side of centre point\n",
    "\n",
    "# # Set up centre of study area and buffer size in metres for data extraction\n",
    "# study_area = 'inskip'  # name used as prefix for output files\n",
    "# lat, lon = -25.7909598625, 153.073231901  # centre of study area\n",
    "# buffer = 8000  # metre units to extend region of interest on each side of centre point\n",
    "\n",
    "# # Set up centre of study area and buffer size in metres for data extraction\n",
    "# study_area = 'narrabeen'  # name used as prefix for output files\n",
    "# lat, lon = -33.71773611412014, 151.30049228668213  # centre of study area\n",
    "# buffer = 5000  # metre units to extend region of interest on each side of centre point\n",
    "\n",
    "# # Set up centre of study area and buffer size in metres for data extraction\n",
    "# study_area = 'victoriariver_2yr'  # name used as prefix for output files\n",
    "# lat, lon = -15.4440342949, 129.811912341  # centre of study area\n",
    "# buffer = 8000  # metre units to extend region of interest on each side of centre point\n",
    "\n",
    "# # Set up centre of study area and buffer size in metres for data extraction\n",
    "# study_area = 'victoriariver2_2yr'  # name used as prefix for output files\n",
    "# lat, lon = -15.4475061785, 130.128534164  # centre of study area\n",
    "# buffer = 8000  # metre units to extend region of interest on each side of centre point\n",
    "\n",
    "# Set up centre of study area and buffer size in metres for data extraction\n",
    "study_area = 'ard_test'  # name used as prefix for output files\n",
    "lat, lon = -19.4947481277, 147.404945988  # centre of study area\n",
    "buffer = 20000  # metre units to extend region of interest on each side of centre point\n",
    "\n",
    "# Set up compositing parameters\n",
    "time_period = ('2015-01-01', '2019-01-01')  # Total date range to import data from\n",
    "sensors = ['ls5', 'ls7', 'ls8']  # Landsat sensors from which to import data from\n",
    "ls7_slc_off = False  # Whether to include striped > May 31 2003 Landsat 7 SLC-off data\n",
    "lower_tideheight = 0.666 # Minimum proportion of the observed tidal range to include \n",
    "upper_tideheight = 1.0  # Maximum proportion of the observed tidal range to include \n",
    "epoch_years = 1  # Length of each epoch used to compute geomedians (i.e. 5 years = 1988 to 1993)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sensor_all = dc.load(product = 'ls8_nbart_albers', \n",
    "#                  group_by = 'solar_day', \n",
    "#                  measurements = ['green', 'nir', 'swir1', 'swir2'],\n",
    "#                  **query)\n",
    "\n",
    "\n",
    "dc.list_measurements()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Return observations for each sensor for the entire time series\n",
    "Use `dask` to lazily load Landsat data and PQ for each sensor for the entire time series. No data is actually loaded here: this is saved until after the layers have been filtered by tidal stage and date to save processing time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If output data and figure directories doesn't exist, create them\n",
    "if not os.path.isdir('output_data/{}/'.format(study_area)):\n",
    "    os.makedirs('output_data/{}/'.format(study_area))\n",
    "    \n",
    "if not os.path.isdir('figures/{}/'.format(study_area)):\n",
    "    os.makedirs('figures/{}/'.format(study_area))\n",
    "\n",
    "# Create list of epochs between start and end of time_period in datetime format\n",
    "start = datetime.strptime(time_period[0], '%Y-%m-%d')\n",
    "end = datetime.strptime(time_period[1], '%Y-%m-%d')\n",
    "epochs = date_range(start, end, epoch_years, 'years')\n",
    "\n",
    "# Print list of epochs\n",
    "epochs_strings = [epoch.strftime('%Y-%m-%d') for epoch in epochs][:-1]\n",
    "print('Processing {} epochs: {}'.format(len(epochs_strings), ', '.join(epochs_strings)))\n",
    "\n",
    "# Identify tide point by importing ITEM polygons\n",
    "polygons_gpd = gpd.read_file('/g/data/r78/intertidal/GA_native_tidal_model.shp')\n",
    "polygon_gpd = polygons_gpd[polygons_gpd.intersects(Point(lon, lat))]\n",
    "tidepost_lon, tidepost_lat = polygon_gpd[['lon', 'lat']].values[0]\n",
    "# tidepost_lon, tidepost_lat = 129.52, -14.89\n",
    "print('Using tidepost coordinates: {} E, {} S\\n'.format(tidepost_lon, tidepost_lat))\n",
    "\n",
    "# Set up query\n",
    "x, y = geometry.point(lon, lat, CRS('WGS84')).to_crs(CRS('EPSG:3577')).points[0]\n",
    "query = {'x': (x - buffer, x + buffer),\n",
    "         'y': (y - buffer, y + buffer),\n",
    "         'time': time_period,\n",
    "         'crs': 'EPSG:3577'}\n",
    "\n",
    "# Output dicts to hold entire time-series for each sensor\n",
    "sensor_dict = {}\n",
    "pq_dict = {}\n",
    "\n",
    "# For each sensor, dask load data and compute tide heights for each sensor\n",
    "for sensor in sensors:\n",
    "    \n",
    "    # The following code will fail if there is no data for the sensor in time_period;\n",
    "    # this try statement catches this and skips the affected sensor    \n",
    "    try:\n",
    "    \n",
    "        # Return observations matching query without actually loading them using dask\n",
    "        sensor_all = dc.load(product = '{}_nbart_albers'.format(sensor), \n",
    "                         group_by = 'solar_day', \n",
    "                         measurements = ['green', 'nir', 'swir1', 'swir2'],\n",
    "                         dask_chunks={'time': 1},\n",
    "                         **query)\n",
    "\n",
    "        # Load PQ data matching query without actually loading them using dask\n",
    "        pq_all = dc.load(product = '{}_pq_albers'.format(sensor),\n",
    "                        group_by = 'solar_day',\n",
    "                        fuse_func=ga_pq_fuser, \n",
    "                        dask_chunks={'time': 1},\n",
    "                        **query)\n",
    "        \n",
    "        # Remove Landsat 7 SLC-off from PQ layer if ls7_slc_off = False\n",
    "        if not ls7_slc_off and sensor == 'ls7':\n",
    "            sensor_all = sensor_all.where(sensor_all.time < np.datetime64('2003-05-30'), drop=True) \n",
    "\n",
    "        # Use the tidal mode to compute tide heights for each observation:\n",
    "        print('Computing tidal heights for {} {} observations'.format(len(sensor_all.time), sensor))\n",
    "        obs_datetimes = sensor_all.time.data.astype('M8[s]').astype('O').tolist()\n",
    "        obs_timepoints = [TimePoint(tidepost_lon, tidepost_lat, dt) for dt in obs_datetimes]\n",
    "        obs_predictedtides = predict_tide(obs_timepoints)\n",
    "        obs_tideheights = [predictedtide.tide_m for predictedtide in obs_predictedtides]\n",
    "\n",
    "        # Assign tide heights to the dataset as a new variable\n",
    "        sensor_all['tide_heights'] = xr.DataArray(obs_tideheights, [('time', sensor_all.time)]) \n",
    "\n",
    "        # Append results for each sensor to a dictionary with sensor name as the key\n",
    "        sensor_dict[sensor] = sensor_all\n",
    "        pq_dict[sensor] = pq_all \n",
    "    \n",
    "    except AttributeError:\n",
    "        \n",
    "        print('{} selection failed due to likely lack of data in time_period; skipping'.format(sensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset data by tidal height and epoch time range\n",
    "Use image time-stamps to compute tidal heights at the time of each observation, and take a subset of the entire Landsat and PQ time series based on tide height and epoch date range for each sensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If any sensor has 0 observations, remove it from the dictionary before proceeding\n",
    "sensor_dict = {key:value for key, value in sensor_dict.items() if len(value.time) > 0}\n",
    "\n",
    "# Calculate max and min tide heights for the entire time series and all sensors\n",
    "obs_min = np.min([sensor_ds.tide_heights.min() for sensor_ds in sensor_dict.values()])\n",
    "obs_max = np.max([sensor_ds.tide_heights.max() for sensor_ds in sensor_dict.values()])\n",
    "obs_range = obs_max - obs_min\n",
    "\n",
    "# Calculate tidal limits used for subsequent data selection\n",
    "sel_min = obs_min + (obs_range * lower_tideheight)\n",
    "sel_max = obs_min + (obs_range * upper_tideheight)\n",
    "print('Keeping tidal heights of {0:.2f} m to {1:.2f} m out of an observed local tidal ' \n",
    "      'range of {2:.2f} m to {3:.2f} m'.format(sel_min, sel_max, obs_min, obs_max))\n",
    "\n",
    "# Create dictionaries to hold filtered sensor data for each epoch\n",
    "sensor_epoch_dict = defaultdict(list)\n",
    "pq_epoch_dict = defaultdict(list)\n",
    "   \n",
    "for epoch, sensor in itertools.product(epochs[:-1], sensor_dict.keys()):\n",
    "                                           \n",
    "    # Select dataset\n",
    "    sensor_all = sensor_dict[sensor]  \n",
    "    pq_all = pq_dict[sensor] \n",
    "    \n",
    "    # Filter to keep only observations that have matching PQ data \n",
    "    time = (sensor_all.time - pq_all.time).time\n",
    "    sensor_subset = sensor_all.sel(time=time)\n",
    "    pq_subset = pq_all.sel(time=time)\n",
    "                                           \n",
    "    # Filter by tidal stage\n",
    "    sensor_subset = sensor_subset.where((sensor_subset.tide_heights >= sel_min) &\n",
    "                                        (sensor_subset.tide_heights <= sel_max), drop = True)\n",
    "    \n",
    "    # Filter pq to same timesteps (this avoids conversion to float by `.where`)\n",
    "    pq_subset = pq_subset.sel(time = sensor_subset.time)\n",
    "\n",
    "    # Identify from and to date strings\n",
    "    from_date = epoch.strftime('%Y-%m-%d')\n",
    "    to_date = (epoch + relativedelta(years=epoch_years)).strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Subset sensor to from and to date for epoch\n",
    "    print('    Filtering from {} to {} for {}'.format(from_date, to_date, sensor))\n",
    "    sensor_subset = sensor_subset.sel(time=slice(from_date, to_date))\n",
    "    pq_subset = pq_subset.sel(time=slice(from_date, to_date))\n",
    "\n",
    "    # Add subsetted data to dicts (one key matching a list of sensor data for each epoch)\n",
    "    sensor_epoch_dict[from_date].append(sensor_subset)\n",
    "    pq_epoch_dict[from_date].append(pq_subset)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine multiple sensors, load data and generate geomedians\n",
    "For each epoch, combine all sensors into one dataset, load the data for the first time using `dask`'s `.compute()`, then composite all timesteps into a single array using a geometric median computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dict to hold output geomedian composits\n",
    "ndwi_dict = {}\n",
    "mndwi_dict = {}\n",
    "awei_dict = {}\n",
    "tideheights_dict = {}\n",
    "\n",
    "for from_date in sensor_epoch_dict.keys():\n",
    "    \n",
    "    # For the first time, actually load data for all sensors and then combine into one dataset  \n",
    "    print('Loading and combining {} data for {}'.format(\", \".join(sensor_dict.keys()), from_date))\n",
    "    sensor_combined = xr.concat([i.compute() for i in sensor_epoch_dict[from_date]], dim='time')\n",
    "    pq_combined = xr.concat([i.compute() for i in pq_epoch_dict[from_date]], dim='time')\n",
    "    print('    {} observations combined'.format(len(sensor_combined.time)))\n",
    "    \n",
    "    # Manually add flag definition back in (it is lost during the concatenation)\n",
    "    pq_combined.pixelquality.attrs['flags_definition'] = pq_all.pixelquality.flags_definition\n",
    "    \n",
    "    # Sort output datasets by date\n",
    "    sensor_combined = sensor_combined.sortby('time')\n",
    "    pq_combined = pq_combined.sortby('time')\n",
    "    \n",
    "    # Identify pixels with no clouds/shadows in either ACCA for Fmask\n",
    "    good_quality = masking.make_mask(pq_combined.pixelquality,\n",
    "                                     cloud_acca='no_cloud',\n",
    "                                     cloud_shadow_acca='no_cloud_shadow',\n",
    "                                     cloud_shadow_fmask='no_cloud_shadow',\n",
    "                                     cloud_fmask='no_cloud',\n",
    "                                     blue_saturated=False,\n",
    "                                     green_saturated=False,\n",
    "                                     red_saturated=False,\n",
    "                                     nir_saturated=False,\n",
    "                                     swir1_saturated=False,\n",
    "                                     swir2_saturated=False,\n",
    "                                     contiguous=True)\n",
    "    \n",
    "    # Apply mask to set all PQ-affected pixels to NaN and set nodata to NaN\n",
    "    print('    Applying PQ mask and setting nodata to NaN')\n",
    "    sensor_combined = sensor_combined.where(good_quality)\n",
    "    sensor_combined = masking.mask_invalid_data(sensor_combined)\n",
    "    \n",
    "    # Compute NDWI\n",
    "    sensor_combined[\"ndwi\"] = (sensor_combined.green - sensor_combined.nir) / (sensor_combined.green + sensor_combined.nir)\n",
    "    sensor_combined[\"mndwi\"] = (sensor_combined.green - sensor_combined.swir1) / (sensor_combined.green + sensor_combined.swir1)\n",
    "    sensor_combined[\"awei\"] = (4 * (sensor_combined.green * 0.0001 - sensor_combined.swir1 * 0.0001) -\n",
    "                              (0.25 * sensor_combined.nir * 0.0001 + 2.75 * sensor_combined.swir2 * 0.0001))\n",
    "\n",
    "    # Compute NDWI composite using all timesteps\n",
    "    print('    Computing NDWI, MNDWI and AWEI median')\n",
    "    ndwi_median = sensor_combined[[\"ndwi\"]].median(dim='time', keep_attrs=True)\n",
    "    mndwi_median = sensor_combined[[\"mndwi\"]].median(dim='time', keep_attrs=True)\n",
    "    awei_median = sensor_combined[[\"awei\"]].median(dim='time', keep_attrs=True)\n",
    "    \n",
    "    # Export to file\n",
    "    print('    Exporting to file')\n",
    "    filename_ndwi = 'output_data/{0}/{0}_{1}_ndwi.tif'.format(study_area, from_date)\n",
    "    filename_mndwi = 'output_data/{0}/{0}_{1}_mndwi.tif'.format(study_area, from_date)\n",
    "    filename_awei = 'output_data/{0}/{0}_{1}_awei.tif'.format(study_area, from_date)\n",
    "    write_geotiff(filename=filename_ndwi, dataset=ndwi_median)\n",
    "    write_geotiff(filename=filename_mndwi, dataset=mndwi_median)\n",
    "    write_geotiff(filename=filename_awei, dataset=awei_median)\n",
    "    \n",
    "    # Assign to dict\n",
    "    ndwi_dict[from_date] = ndwi_median\n",
    "    mndwi_dict[from_date] = mndwi_median\n",
    "    awei_dict[from_date] = awei_median\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract waterlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0\n",
    "metric = 'ndwi'\n",
    "\n",
    "for date, data in ndwi_dict.items():\n",
    "    \n",
    "    print(date)\n",
    "    \n",
    "    # Prepare attributes as input to contour extract\n",
    "    attribute_data = {'date': [date[0:4]]}\n",
    "    attribute_dtypes = {'date': 'int'}\n",
    "    \n",
    "    # Extract contours with custom attribute fields:\n",
    "    contour_dict = SpatialTools.contour_extract(z_values=[threshold],\n",
    "                                   ds_array=data[metric],\n",
    "                                   ds_crs='epsg:3577',\n",
    "                                   ds_affine=data.geobox.transform,\n",
    "                                   output_shp=f'output_data/{study_area}/{study_area}_{metric}_{date}.shp',\n",
    "                                   attribute_data=attribute_data,\n",
    "                                   attribute_dtypes=attribute_dtypes)\n",
    "    \n",
    "# Combine all shapefiles into one file\n",
    "import glob\n",
    "shapefiles = glob.glob(f'output_data/{study_area}/{study_area}_{metric}_*01-01.shp')\n",
    "gdf = pd.concat([gpd.read_file(shp) for shp in shapefiles], sort=False).pipe(gpd.GeoDataFrame)\n",
    "\n",
    "# Save as combined shapefile\n",
    "gdf = gdf.reset_index()[['date', 'geometry']].sort_values('date')\n",
    "gdf['date'] = gdf['date'].astype(np.float32)\n",
    "gdf.crs = 'epsg:3577'\n",
    "gdf.to_file(f'output_data/{study_area}/{study_area}_{metric}_combined.shp')\n",
    "\n",
    "gdf['date'].dtype\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create tidal modelled vs observed plot\n",
    "Create a plot comparing selected Landsat observations to all Landsat observations and the entire tidal history of the study area:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each hour between start and end of time series, predict tide and add to list\n",
    "all_times = date_range(start, end, 1, 'hours')\n",
    "tp_model = [TimePoint(tidepost_lon, tidepost_lat, dt) for dt in all_times]\n",
    "tides_model = [tide.tide_m for tide in predict_tide(tp_model)]\n",
    "\n",
    "# Covert to dataframe of modelled dates and tidal heights\n",
    "modelled_df = pd.DataFrame({'tide_heights': tides_model}, index=pd.DatetimeIndex(all_times))\n",
    "\n",
    "# Return dataframe of previously-generated observed dates and tidal heights\n",
    "observed_df = xr.concat([i.tide_heights for i in sensor_dict.values()], dim='time').to_dataframe()   \n",
    "\n",
    "# Set up plot\n",
    "fig = plt.figure(figsize=(10, 4))\n",
    "plt.margins(0)\n",
    "fig.axes[0].spines['right'].set_visible(False)\n",
    "fig.axes[0].spines['top'].set_visible(False)\n",
    "fig.axes[0].yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "plt.ylabel('Tide height (m)')\n",
    "\n",
    "plt.annotate('Tidepost: {} E, {} S ({})'.format(tidepost_lon, tidepost_lat, study_area), \n",
    "             xy=(0, 1.2), xycoords='axes fraction', \n",
    "             xytext=(5, -3), textcoords='offset points',\n",
    "             fontsize=10, verticalalignment='top', fontweight='bold') \n",
    "\n",
    "# Plot modelled values as grey background\n",
    "plt.plot(modelled_df.index, modelled_df.tide_heights,\n",
    "         color='gainsboro', linewidth=0.6, zorder=1, label='OTPS model')\n",
    "\n",
    "# Plot all observations as black points\n",
    "plt.scatter(observed_df.index, observed_df.tide_heights,\n",
    "            s=4, color='darkgrey', marker='o', zorder=2, label='All observations')\n",
    "\n",
    "# Plot selected observations in red by clipping observed_df to min and max tide selection\n",
    "selected_df = observed_df[observed_df.tide_heights.between(sel_min-0.001, sel_max+0.001)]\n",
    "plt.scatter(selected_df.index, selected_df.tide_heights,\n",
    "            s=10, color='black', marker='o', zorder=2, label='Selected observations')\n",
    "\n",
    "# Plot horizontal lines defining border of selected tidal range\n",
    "plt.axhline(y=sel_min, color='red', alpha=0.2) \n",
    "plt.axhline(y=sel_max, color='red', alpha=0.2) \n",
    "\n",
    "# Add vertical lines and annotation defining each epoch\n",
    "for epoch in epochs[:-1]:\n",
    "    \n",
    "    # Compute from and to date strings\n",
    "    from_date = epoch.strftime('%Y-%m-%d')\n",
    "    to_date = (epoch + relativedelta(years=epoch_years)).strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Add vertical line and epoch titles\n",
    "    plt.axvline(x=epoch, color='red', alpha=0.2)\n",
    "    plt.annotate('{} to {}'.format(from_date, to_date), \n",
    "                 xy=(epoch, modelled_df.tide_heights.max()), \n",
    "                 xytext=(5, 10), textcoords='offset points', fontsize=7) \n",
    "    \n",
    "# Add legend\n",
    "plt.legend(bbox_to_anchor=(0.975, 1.2), loc=1, borderaxespad=0, ncol=3, \n",
    "           handletextpad=0.1, frameon=False, columnspacing=0.3, fontsize=7)\n",
    "\n",
    "# Export plot\n",
    "filename = 'figures/{0}/{0}_tideobs.png'.format(study_area)\n",
    "print('    Exporting to {}'.format(filename))\n",
    "# fig.savefig(filename, dpi=300, bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
