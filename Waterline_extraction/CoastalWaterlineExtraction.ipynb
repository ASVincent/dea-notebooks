{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract coastal waterlines across time\n",
    "**What does this notebook do?** \n",
    "\n",
    "This notebooks demonstrates how to tidally tag remotely sensed imagery using the [OSU Tidal Prediction Software or OTPS](http://volkov.oce.orst.edu/tides/otps.html) model, create geomedian composites for a given set of epochs and tidal range, and extract waterline contours from the composite layers for each epoch. \n",
    "\n",
    "**Requirements:** \n",
    "\n",
    "You need to run the following commands from the command line prior to launching jupyter notebooks from the same terminal so that the required libraries and paths are set:\n",
    "\n",
    "`module use /g/data/v10/public/modules/modulefiles` \n",
    "\n",
    "`module load dea/20180515`  *(currently using an older version of `dea` due to a bug in `xr.concat`; will be reverted to `module load dea` in future)*\n",
    "\n",
    "`module load otps`\n",
    "\n",
    "If you find an error or bug in this notebook, please either create an 'Issue' in the Github repository, or fix it yourself and create a 'Pull' request to contribute the updated notebook back into the repository (See the repository [README](https://github.com/GeoscienceAustralia/dea-notebooks/blob/master/README.rst) for instructions on creating a Pull request).\n",
    "\n",
    "**Date:** September 2018\n",
    "\n",
    "**Author:** Robbi Bishop-Taylor"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "**Tags**: :index:`tidal_model`, :index:`OTPS`, :index:`tidal_tagging`, :index:`predict_tide`, :index:`composites`, :index:`dask`, :index:`write_geotiff`,  :index:`filmstrip_plot`, :index:`geopandas`, :index:`point_in_polygon`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import datacube\n",
    "import itertools\n",
    "import warnings\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from otps import TimePoint\n",
    "from otps import predict_tide\n",
    "from datetime import datetime, timedelta\n",
    "from datacube.utils import geometry\n",
    "from datacube.utils.geometry import CRS\n",
    "from datacube_stats.statistics import GeoMedian\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from datacube.helpers import ga_pq_fuser\n",
    "from datacube.helpers import write_geotiff\n",
    "from datacube.storage import masking\n",
    "from shapely.geometry import Point\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "sys.path.append('../10_Scripts')\n",
    "import SpatialTools\n",
    "\n",
    "# For nicer notebook plotting, hide warnings (comment out for real analysis)\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "# Create datacube instance\n",
    "dc = datacube.Datacube(app='Tidal geomedian filmstrips')\n",
    "\n",
    "def date_range(start_date, end_date, increment, period):\n",
    "    \n",
    "    \"\"\"Generate dates seperated by given time increment/period\"\"\"\n",
    "    \n",
    "    result = []\n",
    "    nxt = start_date\n",
    "    delta = relativedelta(**{period:increment})\n",
    "    while nxt <= end_date:\n",
    "        result.append(nxt)\n",
    "        nxt += delta\n",
    "    return result\n",
    "\n",
    "def ds_to_rgb(ds, bands=['red', 'green', 'blue'], reflect_stand=4000):\n",
    "    \n",
    "    \"\"\"Converts an xarray dataset to a three band array\"\"\"\n",
    "    \n",
    "    rawimg = np.transpose(ds[bands].to_array().data, [1, 2, 0])\n",
    "    img_toshow = (rawimg / reflect_stand).clip(0, 1)\n",
    "    return img_toshow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geomedian filmstrip parameters\n",
    "Set the area, time period  and sensors of interest, and tide limits and epoch length used to produce each geomedian composite. This is the only cell that needs to be edited to run the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set up centre of study area and buffer size in metres for data extraction\n",
    "# study_area = 'pointstuart_wide'  # name used as prefix for output files\n",
    "# lat, lon = -12.238, 131.849  # centre of study area\n",
    "# buffer = 15000  # metre units to extend region of interest on each side of centre point\n",
    "\n",
    "# # Set up centre of study area and buffer size in metres for data extraction\n",
    "# study_area = 'mitchell'  # name used as prefix for output files\n",
    "# lat, lon = -15.14995, 141.63650  # centre of study area\n",
    "# buffer = 10000  # metre units to extend region of interest on each side of centre point\n",
    "\n",
    "# # Set up centre of study area and buffer size in metres for data extraction\n",
    "# study_area = 'inskip'  # name used as prefix for output files\n",
    "# lat, lon = -25.7909598625, 153.073231901  # centre of study area\n",
    "# buffer = 8000  # metre units to extend region of interest on each side of centre point\n",
    "\n",
    "# # Set up centre of study area and buffer size in metres for data extraction\n",
    "# study_area = 'narrabeen'  # name used as prefix for output files\n",
    "# lat, lon = -33.71773611412014, 151.30049228668213  # centre of study area\n",
    "# buffer = 5000  # metre units to extend region of interest on each side of centre point\n",
    "\n",
    "# # Set up centre of study area and buffer size in metres for data extraction\n",
    "# study_area = 'victoriariver_2yr'  # name used as prefix for output files\n",
    "# lat, lon = -15.4440342949, 129.811912341  # centre of study area\n",
    "# buffer = 8000  # metre units to extend region of interest on each side of centre point\n",
    "\n",
    "# # Set up centre of study area and buffer size in metres for data extraction\n",
    "# study_area = 'victoriariver2_2yr'  # name used as prefix for output files\n",
    "# lat, lon = -15.4475061785, 130.128534164  # centre of study area\n",
    "# buffer = 8000  # metre units to extend region of interest on each side of centre point\n",
    "\n",
    "# Set up centre of study area and buffer size in metres for data extraction\n",
    "study_area = 'ard_test_comp'  # name used as prefix for output files\n",
    "lat, lon = -19.4889944734, 147.507306961  # centre of study area\n",
    "buffer = 10800  # metre units to extend region of interest on each side of centre point\n",
    "\n",
    "# Set up compositing parameters\n",
    "time_period = ('1987-01-01', '2019-01-01')  # Total date range to import data from\n",
    "sensors = ['ls5', 'ls7', 'ls8']  # Landsat sensors from which to import data from\n",
    "ls7_slc_off = False  # Whether to include striped > May 31 2003 Landsat 7 SLC-off data\n",
    "lower_tideheight = 0.666 # Minimum proportion of the observed tidal range to include \n",
    "upper_tideheight = 1.0  # Maximum proportion of the observed tidal range to include \n",
    "epoch_years = 3  # Length of each epoch used to compute geomedians (i.e. 5 years = 1988 to 1993)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Return observations for each sensor for the entire time series\n",
    "Use `dask` to lazily load Landsat data and PQ for each sensor for the entire time series. No data is actually loaded here: this is saved until after the layers have been filtered by tidal stage and date to save processing time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 10 epochs: 1987-01-01, 1990-01-01, 1993-01-01, 1996-01-01, 1999-01-01, 2002-01-01, 2005-01-01, 2008-01-01, 2011-01-01, 2014-01-01\n",
      "Using tidepost coordinates: 147.56 E, -19.34 S\n",
      "\n",
      "Computing tidal heights for 372 ls5 observations\n",
      "Computing tidal heights for 67 ls7 observations\n",
      "Computing tidal heights for 121 ls8 observations\n"
     ]
    }
   ],
   "source": [
    "# If output data and figure directories doesn't exist, create them\n",
    "if not os.path.isdir('output_data/{}/'.format(study_area)):\n",
    "    os.makedirs('output_data/{}/'.format(study_area))\n",
    "    \n",
    "if not os.path.isdir('figures/{}/'.format(study_area)):\n",
    "    os.makedirs('figures/{}/'.format(study_area))\n",
    "\n",
    "# Create list of epochs between start and end of time_period in datetime format\n",
    "start = datetime.strptime(time_period[0], '%Y-%m-%d')\n",
    "end = datetime.strptime(time_period[1], '%Y-%m-%d')\n",
    "epochs = date_range(start, end, epoch_years, 'years')\n",
    "\n",
    "# Print list of epochs\n",
    "epochs_strings = [epoch.strftime('%Y-%m-%d') for epoch in epochs][:-1]\n",
    "print('Processing {} epochs: {}'.format(len(epochs_strings), ', '.join(epochs_strings)))\n",
    "\n",
    "# Identify tide point by importing ITEM polygons\n",
    "polygons_gpd = gpd.read_file('/g/data/r78/intertidal/GA_native_tidal_model.shp')\n",
    "polygon_gpd = polygons_gpd[polygons_gpd.intersects(Point(lon, lat))]\n",
    "tidepost_lon, tidepost_lat = polygon_gpd[['lon', 'lat']].values[0]\n",
    "# tidepost_lon, tidepost_lat = 129.52, -14.89\n",
    "print('Using tidepost coordinates: {} E, {} S\\n'.format(tidepost_lon, tidepost_lat))\n",
    "\n",
    "# Set up query\n",
    "x, y = geometry.point(lon, lat, CRS('WGS84')).to_crs(CRS('EPSG:3577')).points[0]\n",
    "query = {'x': (x - buffer, x + buffer),\n",
    "         'y': (y - buffer, y + buffer),\n",
    "         'time': time_period,\n",
    "         'crs': 'EPSG:3577'}\n",
    "\n",
    "# Output dicts to hold entire time-series for each sensor\n",
    "sensor_dict = {}\n",
    "pq_dict = {}\n",
    "\n",
    "# For each sensor, dask load data and compute tide heights for each sensor\n",
    "for sensor in sensors:\n",
    "    \n",
    "    # The following code will fail if there is no data for the sensor in time_period;\n",
    "    # this try statement catches this and skips the affected sensor    \n",
    "    try:\n",
    "    \n",
    "        # Return observations matching query without actually loading them using dask\n",
    "        sensor_all = dc.load(product = '{}_nbart_albers'.format(sensor), \n",
    "                         group_by = 'solar_day', \n",
    "                         measurements = ['green', 'nir', 'swir1', 'swir2'],\n",
    "                         dask_chunks={'time': 1},\n",
    "                         **query)\n",
    "\n",
    "        # Load PQ data matching query without actually loading them using dask\n",
    "        pq_all = dc.load(product = '{}_pq_albers'.format(sensor),\n",
    "                        group_by = 'solar_day',\n",
    "                        fuse_func=ga_pq_fuser, \n",
    "                        dask_chunks={'time': 1},\n",
    "                        **query)\n",
    "        \n",
    "        # Remove Landsat 7 SLC-off from PQ layer if ls7_slc_off = False\n",
    "        if not ls7_slc_off and sensor == 'ls7':\n",
    "            sensor_all = sensor_all.where(sensor_all.time < np.datetime64('2003-05-30'), drop=True) \n",
    "\n",
    "        # Use the tidal mode to compute tide heights for each observation:\n",
    "        print('Computing tidal heights for {} {} observations'.format(len(sensor_all.time), sensor))\n",
    "        obs_datetimes = sensor_all.time.data.astype('M8[s]').astype('O').tolist()\n",
    "        obs_timepoints = [TimePoint(tidepost_lon, tidepost_lat, dt) for dt in obs_datetimes]\n",
    "        obs_predictedtides = predict_tide(obs_timepoints)\n",
    "        obs_tideheights = [predictedtide.tide_m for predictedtide in obs_predictedtides]\n",
    "\n",
    "        # Assign tide heights to the dataset as a new variable\n",
    "        sensor_all['tide_heights'] = xr.DataArray(obs_tideheights, [('time', sensor_all.time)]) \n",
    "\n",
    "        # Append results for each sensor to a dictionary with sensor name as the key\n",
    "        sensor_dict[sensor] = sensor_all\n",
    "        pq_dict[sensor] = pq_all \n",
    "    \n",
    "    except AttributeError:\n",
    "        \n",
    "        print('{} selection failed due to likely lack of data in time_period; skipping'.format(sensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset data by tidal height and epoch time range\n",
    "Use image time-stamps to compute tidal heights at the time of each observation, and take a subset of the entire Landsat and PQ time series based on tide height and epoch date range for each sensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keeping tidal heights of 0.79 m to 1.62 m out of an observed local tidal range of -0.87 m to 1.62 m\n",
      "    Filtering from 1987-01-01 to 1990-01-01 for ls5\n",
      "    Filtering from 1987-01-01 to 1990-01-01 for ls7\n",
      "    Filtering from 1987-01-01 to 1990-01-01 for ls8\n",
      "    Filtering from 1990-01-01 to 1993-01-01 for ls5\n",
      "    Filtering from 1990-01-01 to 1993-01-01 for ls7\n",
      "    Filtering from 1990-01-01 to 1993-01-01 for ls8\n",
      "    Filtering from 1993-01-01 to 1996-01-01 for ls5\n",
      "    Filtering from 1993-01-01 to 1996-01-01 for ls7\n",
      "    Filtering from 1993-01-01 to 1996-01-01 for ls8\n",
      "    Filtering from 1996-01-01 to 1999-01-01 for ls5\n",
      "    Filtering from 1996-01-01 to 1999-01-01 for ls7\n",
      "    Filtering from 1996-01-01 to 1999-01-01 for ls8\n",
      "    Filtering from 1999-01-01 to 2002-01-01 for ls5\n",
      "    Filtering from 1999-01-01 to 2002-01-01 for ls7\n",
      "    Filtering from 1999-01-01 to 2002-01-01 for ls8\n",
      "    Filtering from 2002-01-01 to 2005-01-01 for ls5\n",
      "    Filtering from 2002-01-01 to 2005-01-01 for ls7\n",
      "    Filtering from 2002-01-01 to 2005-01-01 for ls8\n",
      "    Filtering from 2005-01-01 to 2008-01-01 for ls5\n",
      "    Filtering from 2005-01-01 to 2008-01-01 for ls7\n",
      "    Filtering from 2005-01-01 to 2008-01-01 for ls8\n",
      "    Filtering from 2008-01-01 to 2011-01-01 for ls5\n",
      "    Filtering from 2008-01-01 to 2011-01-01 for ls7\n",
      "    Filtering from 2008-01-01 to 2011-01-01 for ls8\n",
      "    Filtering from 2011-01-01 to 2014-01-01 for ls5\n",
      "    Filtering from 2011-01-01 to 2014-01-01 for ls7\n",
      "    Filtering from 2011-01-01 to 2014-01-01 for ls8\n",
      "    Filtering from 2014-01-01 to 2017-01-01 for ls5\n",
      "    Filtering from 2014-01-01 to 2017-01-01 for ls7\n",
      "    Filtering from 2014-01-01 to 2017-01-01 for ls8\n"
     ]
    }
   ],
   "source": [
    "# If any sensor has 0 observations, remove it from the dictionary before proceeding\n",
    "sensor_dict = {key:value for key, value in sensor_dict.items() if len(value.time) > 0}\n",
    "\n",
    "# Calculate max and min tide heights for the entire time series and all sensors\n",
    "obs_min = np.min([sensor_ds.tide_heights.min() for sensor_ds in sensor_dict.values()])\n",
    "obs_max = np.max([sensor_ds.tide_heights.max() for sensor_ds in sensor_dict.values()])\n",
    "obs_range = obs_max - obs_min\n",
    "\n",
    "# Calculate tidal limits used for subsequent data selection\n",
    "sel_min = obs_min + (obs_range * lower_tideheight)\n",
    "sel_max = obs_min + (obs_range * upper_tideheight)\n",
    "print('Keeping tidal heights of {0:.2f} m to {1:.2f} m out of an observed local tidal ' \n",
    "      'range of {2:.2f} m to {3:.2f} m'.format(sel_min, sel_max, obs_min, obs_max))\n",
    "\n",
    "# Create dictionaries to hold filtered sensor data for each epoch\n",
    "sensor_epoch_dict = defaultdict(list)\n",
    "pq_epoch_dict = defaultdict(list)\n",
    "   \n",
    "for epoch, sensor in itertools.product(epochs[:-1], sensor_dict.keys()):\n",
    "                                           \n",
    "    # Select dataset\n",
    "    sensor_all = sensor_dict[sensor]  \n",
    "    pq_all = pq_dict[sensor] \n",
    "    \n",
    "    # Filter to keep only observations that have matching PQ data \n",
    "    time = (sensor_all.time - pq_all.time).time\n",
    "    sensor_subset = sensor_all.sel(time=time)\n",
    "    pq_subset = pq_all.sel(time=time)\n",
    "                                           \n",
    "    # Filter by tidal stage\n",
    "    sensor_subset = sensor_subset.where((sensor_subset.tide_heights >= sel_min) &\n",
    "                                        (sensor_subset.tide_heights <= sel_max), drop = True)\n",
    "    \n",
    "    # Filter pq to same timesteps (this avoids conversion to float by `.where`)\n",
    "    pq_subset = pq_subset.sel(time = sensor_subset.time)\n",
    "\n",
    "    # Identify from and to date strings\n",
    "    from_date = epoch.strftime('%Y-%m-%d')\n",
    "    to_date = (epoch + relativedelta(years=epoch_years)).strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Subset sensor to from and to date for epoch\n",
    "    print('    Filtering from {} to {} for {}'.format(from_date, to_date, sensor))\n",
    "    sensor_subset = sensor_subset.sel(time=slice(from_date, to_date))\n",
    "    pq_subset = pq_subset.sel(time=slice(from_date, to_date))\n",
    "\n",
    "    # Add subsetted data to dicts (one key matching a list of sensor data for each epoch)\n",
    "    sensor_epoch_dict[from_date].append(sensor_subset)\n",
    "    pq_epoch_dict[from_date].append(pq_subset)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine multiple sensors, load data and generate geomedians\n",
    "For each epoch, combine all sensors into one dataset, load the data for the first time using `dask`'s `.compute()`, then composite all timesteps into a single array using a geometric median computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and combining ls5, ls7, ls8 data for 1987-01-01\n",
      "    7 observations combined\n",
      "    Applying PQ mask and setting nodata to NaN\n",
      "    Computing NDWI, MNDWI and AWEI median\n",
      "    Exporting to file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:rasterio._gdal:CPLE_NotSupported in 'RGBA' is an unexpected value for PHOTOMETRIC creation option of type string-select.\n",
      "WARNING:rasterio._gdal:CPLE_IllegalArg in PHOTOMETRIC=RGBA value not recognised, ignoring.  Set the Photometric Interpretation as MINISBLACK.\n",
      "WARNING:rasterio._gdal:CPLE_NotSupported in 'RGBA' is an unexpected value for PHOTOMETRIC creation option of type string-select.\n",
      "WARNING:rasterio._gdal:CPLE_IllegalArg in PHOTOMETRIC=RGBA value not recognised, ignoring.  Set the Photometric Interpretation as MINISBLACK.\n",
      "WARNING:rasterio._gdal:CPLE_NotSupported in 'RGBA' is an unexpected value for PHOTOMETRIC creation option of type string-select.\n",
      "WARNING:rasterio._gdal:CPLE_IllegalArg in PHOTOMETRIC=RGBA value not recognised, ignoring.  Set the Photometric Interpretation as MINISBLACK.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and combining ls5, ls7, ls8 data for 1990-01-01\n",
      "    13 observations combined\n",
      "    Applying PQ mask and setting nodata to NaN\n",
      "    Computing NDWI, MNDWI and AWEI median\n",
      "    Exporting to file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:rasterio._gdal:CPLE_NotSupported in 'RGBA' is an unexpected value for PHOTOMETRIC creation option of type string-select.\n",
      "WARNING:rasterio._gdal:CPLE_IllegalArg in PHOTOMETRIC=RGBA value not recognised, ignoring.  Set the Photometric Interpretation as MINISBLACK.\n",
      "WARNING:rasterio._gdal:CPLE_NotSupported in 'RGBA' is an unexpected value for PHOTOMETRIC creation option of type string-select.\n",
      "WARNING:rasterio._gdal:CPLE_IllegalArg in PHOTOMETRIC=RGBA value not recognised, ignoring.  Set the Photometric Interpretation as MINISBLACK.\n",
      "WARNING:rasterio._gdal:CPLE_NotSupported in 'RGBA' is an unexpected value for PHOTOMETRIC creation option of type string-select.\n",
      "WARNING:rasterio._gdal:CPLE_IllegalArg in PHOTOMETRIC=RGBA value not recognised, ignoring.  Set the Photometric Interpretation as MINISBLACK.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and combining ls5, ls7, ls8 data for 1993-01-01\n",
      "    14 observations combined\n",
      "    Applying PQ mask and setting nodata to NaN\n",
      "    Computing NDWI, MNDWI and AWEI median\n",
      "    Exporting to file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:rasterio._gdal:CPLE_NotSupported in 'RGBA' is an unexpected value for PHOTOMETRIC creation option of type string-select.\n",
      "WARNING:rasterio._gdal:CPLE_IllegalArg in PHOTOMETRIC=RGBA value not recognised, ignoring.  Set the Photometric Interpretation as MINISBLACK.\n",
      "WARNING:rasterio._gdal:CPLE_NotSupported in 'RGBA' is an unexpected value for PHOTOMETRIC creation option of type string-select.\n",
      "WARNING:rasterio._gdal:CPLE_IllegalArg in PHOTOMETRIC=RGBA value not recognised, ignoring.  Set the Photometric Interpretation as MINISBLACK.\n",
      "WARNING:rasterio._gdal:CPLE_NotSupported in 'RGBA' is an unexpected value for PHOTOMETRIC creation option of type string-select.\n",
      "WARNING:rasterio._gdal:CPLE_IllegalArg in PHOTOMETRIC=RGBA value not recognised, ignoring.  Set the Photometric Interpretation as MINISBLACK.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and combining ls5, ls7, ls8 data for 1996-01-01\n",
      "    8 observations combined\n",
      "    Applying PQ mask and setting nodata to NaN\n",
      "    Computing NDWI, MNDWI and AWEI median\n",
      "    Exporting to file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:rasterio._gdal:CPLE_NotSupported in 'RGBA' is an unexpected value for PHOTOMETRIC creation option of type string-select.\n",
      "WARNING:rasterio._gdal:CPLE_IllegalArg in PHOTOMETRIC=RGBA value not recognised, ignoring.  Set the Photometric Interpretation as MINISBLACK.\n",
      "WARNING:rasterio._gdal:CPLE_NotSupported in 'RGBA' is an unexpected value for PHOTOMETRIC creation option of type string-select.\n",
      "WARNING:rasterio._gdal:CPLE_IllegalArg in PHOTOMETRIC=RGBA value not recognised, ignoring.  Set the Photometric Interpretation as MINISBLACK.\n",
      "WARNING:rasterio._gdal:CPLE_NotSupported in 'RGBA' is an unexpected value for PHOTOMETRIC creation option of type string-select.\n",
      "WARNING:rasterio._gdal:CPLE_IllegalArg in PHOTOMETRIC=RGBA value not recognised, ignoring.  Set the Photometric Interpretation as MINISBLACK.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and combining ls5, ls7, ls8 data for 1999-01-01\n",
      "    10 observations combined\n",
      "    Applying PQ mask and setting nodata to NaN\n",
      "    Computing NDWI, MNDWI and AWEI median\n",
      "    Exporting to file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:rasterio._gdal:CPLE_NotSupported in 'RGBA' is an unexpected value for PHOTOMETRIC creation option of type string-select.\n",
      "WARNING:rasterio._gdal:CPLE_IllegalArg in PHOTOMETRIC=RGBA value not recognised, ignoring.  Set the Photometric Interpretation as MINISBLACK.\n",
      "WARNING:rasterio._gdal:CPLE_NotSupported in 'RGBA' is an unexpected value for PHOTOMETRIC creation option of type string-select.\n",
      "WARNING:rasterio._gdal:CPLE_IllegalArg in PHOTOMETRIC=RGBA value not recognised, ignoring.  Set the Photometric Interpretation as MINISBLACK.\n",
      "WARNING:rasterio._gdal:CPLE_NotSupported in 'RGBA' is an unexpected value for PHOTOMETRIC creation option of type string-select.\n",
      "WARNING:rasterio._gdal:CPLE_IllegalArg in PHOTOMETRIC=RGBA value not recognised, ignoring.  Set the Photometric Interpretation as MINISBLACK.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and combining ls5, ls7, ls8 data for 2002-01-01\n",
      "    7 observations combined\n",
      "    Applying PQ mask and setting nodata to NaN\n",
      "    Computing NDWI, MNDWI and AWEI median\n",
      "    Exporting to file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:rasterio._gdal:CPLE_NotSupported in 'RGBA' is an unexpected value for PHOTOMETRIC creation option of type string-select.\n",
      "WARNING:rasterio._gdal:CPLE_IllegalArg in PHOTOMETRIC=RGBA value not recognised, ignoring.  Set the Photometric Interpretation as MINISBLACK.\n",
      "WARNING:rasterio._gdal:CPLE_NotSupported in 'RGBA' is an unexpected value for PHOTOMETRIC creation option of type string-select.\n",
      "WARNING:rasterio._gdal:CPLE_IllegalArg in PHOTOMETRIC=RGBA value not recognised, ignoring.  Set the Photometric Interpretation as MINISBLACK.\n",
      "WARNING:rasterio._gdal:CPLE_NotSupported in 'RGBA' is an unexpected value for PHOTOMETRIC creation option of type string-select.\n",
      "WARNING:rasterio._gdal:CPLE_IllegalArg in PHOTOMETRIC=RGBA value not recognised, ignoring.  Set the Photometric Interpretation as MINISBLACK.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and combining ls5, ls7, ls8 data for 2005-01-01\n",
      "    8 observations combined\n",
      "    Applying PQ mask and setting nodata to NaN\n",
      "    Computing NDWI, MNDWI and AWEI median\n",
      "    Exporting to file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:rasterio._gdal:CPLE_NotSupported in 'RGBA' is an unexpected value for PHOTOMETRIC creation option of type string-select.\n",
      "WARNING:rasterio._gdal:CPLE_IllegalArg in PHOTOMETRIC=RGBA value not recognised, ignoring.  Set the Photometric Interpretation as MINISBLACK.\n",
      "WARNING:rasterio._gdal:CPLE_NotSupported in 'RGBA' is an unexpected value for PHOTOMETRIC creation option of type string-select.\n",
      "WARNING:rasterio._gdal:CPLE_IllegalArg in PHOTOMETRIC=RGBA value not recognised, ignoring.  Set the Photometric Interpretation as MINISBLACK.\n",
      "WARNING:rasterio._gdal:CPLE_NotSupported in 'RGBA' is an unexpected value for PHOTOMETRIC creation option of type string-select.\n",
      "WARNING:rasterio._gdal:CPLE_IllegalArg in PHOTOMETRIC=RGBA value not recognised, ignoring.  Set the Photometric Interpretation as MINISBLACK.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and combining ls5, ls7, ls8 data for 2008-01-01\n",
      "    7 observations combined\n",
      "    Applying PQ mask and setting nodata to NaN\n",
      "    Computing NDWI, MNDWI and AWEI median\n",
      "    Exporting to file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:rasterio._gdal:CPLE_NotSupported in 'RGBA' is an unexpected value for PHOTOMETRIC creation option of type string-select.\n",
      "WARNING:rasterio._gdal:CPLE_IllegalArg in PHOTOMETRIC=RGBA value not recognised, ignoring.  Set the Photometric Interpretation as MINISBLACK.\n",
      "WARNING:rasterio._gdal:CPLE_NotSupported in 'RGBA' is an unexpected value for PHOTOMETRIC creation option of type string-select.\n",
      "WARNING:rasterio._gdal:CPLE_IllegalArg in PHOTOMETRIC=RGBA value not recognised, ignoring.  Set the Photometric Interpretation as MINISBLACK.\n",
      "WARNING:rasterio._gdal:CPLE_NotSupported in 'RGBA' is an unexpected value for PHOTOMETRIC creation option of type string-select.\n",
      "WARNING:rasterio._gdal:CPLE_IllegalArg in PHOTOMETRIC=RGBA value not recognised, ignoring.  Set the Photometric Interpretation as MINISBLACK.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and combining ls5, ls7, ls8 data for 2011-01-01\n",
      "    5 observations combined\n",
      "    Applying PQ mask and setting nodata to NaN\n",
      "    Computing NDWI, MNDWI and AWEI median\n",
      "    Exporting to file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:rasterio._gdal:CPLE_NotSupported in 'RGBA' is an unexpected value for PHOTOMETRIC creation option of type string-select.\n",
      "WARNING:rasterio._gdal:CPLE_IllegalArg in PHOTOMETRIC=RGBA value not recognised, ignoring.  Set the Photometric Interpretation as MINISBLACK.\n",
      "WARNING:rasterio._gdal:CPLE_NotSupported in 'RGBA' is an unexpected value for PHOTOMETRIC creation option of type string-select.\n",
      "WARNING:rasterio._gdal:CPLE_IllegalArg in PHOTOMETRIC=RGBA value not recognised, ignoring.  Set the Photometric Interpretation as MINISBLACK.\n",
      "WARNING:rasterio._gdal:CPLE_NotSupported in 'RGBA' is an unexpected value for PHOTOMETRIC creation option of type string-select.\n",
      "WARNING:rasterio._gdal:CPLE_IllegalArg in PHOTOMETRIC=RGBA value not recognised, ignoring.  Set the Photometric Interpretation as MINISBLACK.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and combining ls5, ls7, ls8 data for 2014-01-01\n",
      "    11 observations combined\n",
      "    Applying PQ mask and setting nodata to NaN\n",
      "    Computing NDWI, MNDWI and AWEI median\n",
      "    Exporting to file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:rasterio._gdal:CPLE_NotSupported in 'RGBA' is an unexpected value for PHOTOMETRIC creation option of type string-select.\n",
      "WARNING:rasterio._gdal:CPLE_IllegalArg in PHOTOMETRIC=RGBA value not recognised, ignoring.  Set the Photometric Interpretation as MINISBLACK.\n",
      "WARNING:rasterio._gdal:CPLE_NotSupported in 'RGBA' is an unexpected value for PHOTOMETRIC creation option of type string-select.\n",
      "WARNING:rasterio._gdal:CPLE_IllegalArg in PHOTOMETRIC=RGBA value not recognised, ignoring.  Set the Photometric Interpretation as MINISBLACK.\n",
      "WARNING:rasterio._gdal:CPLE_NotSupported in 'RGBA' is an unexpected value for PHOTOMETRIC creation option of type string-select.\n",
      "WARNING:rasterio._gdal:CPLE_IllegalArg in PHOTOMETRIC=RGBA value not recognised, ignoring.  Set the Photometric Interpretation as MINISBLACK.\n"
     ]
    }
   ],
   "source": [
    "# Dict to hold output geomedian composits\n",
    "ndwi_dict = {}\n",
    "mndwi_dict = {}\n",
    "awei_dict = {}\n",
    "tideheights_dict = {}\n",
    "\n",
    "for from_date in sensor_epoch_dict.keys():\n",
    "    \n",
    "    # For the first time, actually load data for all sensors and then combine into one dataset  \n",
    "    print('Loading and combining {} data for {}'.format(\", \".join(sensor_dict.keys()), from_date))\n",
    "    sensor_combined = xr.concat([i.compute() for i in sensor_epoch_dict[from_date]], dim='time')\n",
    "    pq_combined = xr.concat([i.compute() for i in pq_epoch_dict[from_date]], dim='time')\n",
    "    print('    {} observations combined'.format(len(sensor_combined.time)))\n",
    "    \n",
    "    # Manually add flag definition back in (it is lost during the concatenation)\n",
    "    pq_combined.pixelquality.attrs['flags_definition'] = pq_all.pixelquality.flags_definition\n",
    "    \n",
    "    # Sort output datasets by date\n",
    "    sensor_combined = sensor_combined.sortby('time')\n",
    "    pq_combined = pq_combined.sortby('time')\n",
    "    \n",
    "    # Identify pixels with no clouds/shadows in either ACCA for Fmask\n",
    "    good_quality = masking.make_mask(pq_combined.pixelquality,\n",
    "                                     cloud_acca='no_cloud',\n",
    "                                     cloud_shadow_acca='no_cloud_shadow',\n",
    "                                     cloud_shadow_fmask='no_cloud_shadow',\n",
    "                                     cloud_fmask='no_cloud',\n",
    "                                     blue_saturated=False,\n",
    "                                     green_saturated=False,\n",
    "                                     red_saturated=False,\n",
    "                                     nir_saturated=False,\n",
    "                                     swir1_saturated=False,\n",
    "                                     swir2_saturated=False,\n",
    "                                     contiguous=True)\n",
    "    \n",
    "    # Apply mask to set all PQ-affected pixels to NaN and set nodata to NaN\n",
    "    print('    Applying PQ mask and setting nodata to NaN')\n",
    "    sensor_combined = sensor_combined.where(good_quality)\n",
    "    sensor_combined = masking.mask_invalid_data(sensor_combined)\n",
    "    \n",
    "    # Compute NDWI\n",
    "    sensor_combined[\"ndwi\"] = (sensor_combined.green - sensor_combined.nir) / (sensor_combined.green + sensor_combined.nir)\n",
    "    sensor_combined[\"mndwi\"] = (sensor_combined.green - sensor_combined.swir1) / (sensor_combined.green + sensor_combined.swir1)\n",
    "    sensor_combined[\"awei\"] = (4 * (sensor_combined.green * 0.0001 - sensor_combined.swir1 * 0.0001) -\n",
    "                              (0.25 * sensor_combined.nir * 0.0001 + 2.75 * sensor_combined.swir2 * 0.0001))\n",
    "\n",
    "    # Compute NDWI composite using all timesteps\n",
    "    print('    Computing NDWI, MNDWI and AWEI median')\n",
    "    ndwi_median = sensor_combined[[\"ndwi\"]].median(dim='time', keep_attrs=True)\n",
    "    mndwi_median = sensor_combined[[\"mndwi\"]].median(dim='time', keep_attrs=True)\n",
    "    awei_median = sensor_combined[[\"awei\"]].median(dim='time', keep_attrs=True)\n",
    "    \n",
    "    # Export to file\n",
    "    print('    Exporting to file')\n",
    "    filename_ndwi = 'output_data/{0}/{0}_{1}_ndwi.tif'.format(study_area, from_date)\n",
    "    filename_mndwi = 'output_data/{0}/{0}_{1}_mndwi.tif'.format(study_area, from_date)\n",
    "    filename_awei = 'output_data/{0}/{0}_{1}_awei.tif'.format(study_area, from_date)\n",
    "    write_geotiff(filename=filename_ndwi, dataset=ndwi_median)\n",
    "    write_geotiff(filename=filename_mndwi, dataset=mndwi_median)\n",
    "    write_geotiff(filename=filename_awei, dataset=awei_median)\n",
    "    \n",
    "    # Assign to dict\n",
    "    ndwi_dict[from_date] = ndwi_median\n",
    "    mndwi_dict[from_date] = mndwi_median\n",
    "    awei_dict[from_date] = awei_median\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract waterlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1987-01-01\n",
      "Extracting contour 0\n",
      "\n",
      "Exporting contour shapefile to output_data/ard_test_comp/ard_test_comp_ndwi_1987-01-01.shp\n",
      "1990-01-01\n",
      "Extracting contour 0\n",
      "\n",
      "Exporting contour shapefile to output_data/ard_test_comp/ard_test_comp_ndwi_1990-01-01.shp\n",
      "1993-01-01\n",
      "Extracting contour 0\n",
      "\n",
      "Exporting contour shapefile to output_data/ard_test_comp/ard_test_comp_ndwi_1993-01-01.shp\n",
      "1996-01-01\n",
      "Extracting contour 0\n",
      "\n",
      "Exporting contour shapefile to output_data/ard_test_comp/ard_test_comp_ndwi_1996-01-01.shp\n",
      "1999-01-01\n",
      "Extracting contour 0\n",
      "\n",
      "Exporting contour shapefile to output_data/ard_test_comp/ard_test_comp_ndwi_1999-01-01.shp\n",
      "2002-01-01\n",
      "Extracting contour 0\n",
      "\n",
      "Exporting contour shapefile to output_data/ard_test_comp/ard_test_comp_ndwi_2002-01-01.shp\n",
      "2005-01-01\n",
      "Extracting contour 0\n",
      "\n",
      "Exporting contour shapefile to output_data/ard_test_comp/ard_test_comp_ndwi_2005-01-01.shp\n",
      "2008-01-01\n",
      "Extracting contour 0\n",
      "\n",
      "Exporting contour shapefile to output_data/ard_test_comp/ard_test_comp_ndwi_2008-01-01.shp\n",
      "2011-01-01\n",
      "Extracting contour 0\n",
      "\n",
      "Exporting contour shapefile to output_data/ard_test_comp/ard_test_comp_ndwi_2011-01-01.shp\n",
      "2014-01-01\n",
      "Extracting contour 0\n",
      "\n",
      "Exporting contour shapefile to output_data/ard_test_comp/ard_test_comp_ndwi_2014-01-01.shp\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold = 0\n",
    "metric = 'ndwi'\n",
    "\n",
    "for date, data in ndwi_dict.items():\n",
    "    \n",
    "    print(date)\n",
    "    \n",
    "    # Prepare attributes as input to contour extract\n",
    "    attribute_data = {'date': [date[0:4]]}\n",
    "    attribute_dtypes = {'date': 'int'}\n",
    "    \n",
    "    # Extract contours with custom attribute fields:\n",
    "    contour_dict = SpatialTools.contour_extract(z_values=[threshold],\n",
    "                                   ds_array=data[metric],\n",
    "                                   ds_crs='epsg:3577',\n",
    "                                   ds_affine=data.geobox.transform,\n",
    "                                   output_shp=f'output_data/{study_area}/{study_area}_{metric}_{date}.shp',\n",
    "                                   attribute_data=attribute_data,\n",
    "                                   attribute_dtypes=attribute_dtypes)\n",
    "    \n",
    "# Combine all shapefiles into one file\n",
    "import glob\n",
    "shapefiles = glob.glob(f'output_data/{study_area}/{study_area}_{metric}_*01-01.shp')\n",
    "gdf = pd.concat([gpd.read_file(shp) for shp in shapefiles], sort=False).pipe(gpd.GeoDataFrame)\n",
    "\n",
    "# Save as combined shapefile\n",
    "gdf = gdf.reset_index()[['date', 'geometry']].sort_values('date')\n",
    "gdf['date'] = gdf['date'].astype(np.float32)\n",
    "gdf.crs = 'epsg:3577'\n",
    "gdf.to_file(f'output_data/{study_area}/{study_area}_{metric}_combined.shp')\n",
    "\n",
    "gdf['date'].dtype\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create tidal modelled vs observed plot\n",
    "Create a plot comparing selected Landsat observations to all Landsat observations and the entire tidal history of the study area:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each hour between start and end of time series, predict tide and add to list\n",
    "all_times = date_range(start, end, 1, 'hours')\n",
    "tp_model = [TimePoint(tidepost_lon, tidepost_lat, dt) for dt in all_times]\n",
    "tides_model = [tide.tide_m for tide in predict_tide(tp_model)]\n",
    "\n",
    "# Covert to dataframe of modelled dates and tidal heights\n",
    "modelled_df = pd.DataFrame({'tide_heights': tides_model}, index=pd.DatetimeIndex(all_times))\n",
    "\n",
    "# Return dataframe of previously-generated observed dates and tidal heights\n",
    "observed_df = xr.concat([i.tide_heights for i in sensor_dict.values()], dim='time').to_dataframe()   \n",
    "\n",
    "# Set up plot\n",
    "fig = plt.figure(figsize=(10, 4))\n",
    "plt.margins(0)\n",
    "fig.axes[0].spines['right'].set_visible(False)\n",
    "fig.axes[0].spines['top'].set_visible(False)\n",
    "fig.axes[0].yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "plt.ylabel('Tide height (m)')\n",
    "\n",
    "plt.annotate('Tidepost: {} E, {} S ({})'.format(tidepost_lon, tidepost_lat, study_area), \n",
    "             xy=(0, 1.2), xycoords='axes fraction', \n",
    "             xytext=(5, -3), textcoords='offset points',\n",
    "             fontsize=10, verticalalignment='top', fontweight='bold') \n",
    "\n",
    "# Plot modelled values as grey background\n",
    "plt.plot(modelled_df.index, modelled_df.tide_heights,\n",
    "         color='gainsboro', linewidth=0.6, zorder=1, label='OTPS model')\n",
    "\n",
    "# Plot all observations as black points\n",
    "plt.scatter(observed_df.index, observed_df.tide_heights,\n",
    "            s=4, color='darkgrey', marker='o', zorder=2, label='All observations')\n",
    "\n",
    "# Plot selected observations in red by clipping observed_df to min and max tide selection\n",
    "selected_df = observed_df[observed_df.tide_heights.between(sel_min-0.001, sel_max+0.001)]\n",
    "plt.scatter(selected_df.index, selected_df.tide_heights,\n",
    "            s=10, color='black', marker='o', zorder=2, label='Selected observations')\n",
    "\n",
    "# Plot horizontal lines defining border of selected tidal range\n",
    "plt.axhline(y=sel_min, color='red', alpha=0.2) \n",
    "plt.axhline(y=sel_max, color='red', alpha=0.2) \n",
    "\n",
    "# Add vertical lines and annotation defining each epoch\n",
    "for epoch in epochs[:-1]:\n",
    "    \n",
    "    # Compute from and to date strings\n",
    "    from_date = epoch.strftime('%Y-%m-%d')\n",
    "    to_date = (epoch + relativedelta(years=epoch_years)).strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Add vertical line and epoch titles\n",
    "    plt.axvline(x=epoch, color='red', alpha=0.2)\n",
    "    plt.annotate('{} to {}'.format(from_date, to_date), \n",
    "                 xy=(epoch, modelled_df.tide_heights.max()), \n",
    "                 xytext=(5, 10), textcoords='offset points', fontsize=7) \n",
    "    \n",
    "# Add legend\n",
    "plt.legend(bbox_to_anchor=(0.975, 1.2), loc=1, borderaxespad=0, ncol=3, \n",
    "           handletextpad=0.1, frameon=False, columnspacing=0.3, fontsize=7)\n",
    "\n",
    "# Export plot\n",
    "filename = 'figures/{0}/{0}_tideobs.png'.format(study_area)\n",
    "print('    Exporting to {}'.format(filename))\n",
    "# fig.savefig(filename, dpi=300, bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
