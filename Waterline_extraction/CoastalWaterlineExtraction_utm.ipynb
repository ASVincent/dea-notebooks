{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract coastal waterlines across time\n",
    "**What does this notebook do?** \n",
    "\n",
    "This notebooks demonstrates how to tidally tag remotely sensed imagery using the [OSU Tidal Prediction Software or OTPS](http://volkov.oce.orst.edu/tides/otps.html) model, create geomedian composites for a given set of epochs and tidal range, and extract waterline contours from the composite layers for each epoch. \n",
    "\n",
    "**Requirements:** \n",
    "\n",
    "You need to run the following commands from the command line prior to launching jupyter notebooks from the same terminal so that the required libraries and paths are set:\n",
    "\n",
    "`module use /g/data/v10/public/modules/modulefiles` \n",
    "\n",
    "`module load dea/20180515`  *(currently using an older version of `dea` due to a bug in `xr.concat`; will be reverted to `module load dea` in future)*\n",
    "\n",
    "`module load otps`\n",
    "\n",
    "If you find an error or bug in this notebook, please either create an 'Issue' in the Github repository, or fix it yourself and create a 'Pull' request to contribute the updated notebook back into the repository (See the repository [README](https://github.com/GeoscienceAustralia/dea-notebooks/blob/master/README.rst) for instructions on creating a Pull request).\n",
    "\n",
    "**Date:** September 2018\n",
    "\n",
    "**Author:** Robbi Bishop-Taylor"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "**Tags**: :index:`tidal_model`, :index:`OTPS`, :index:`tidal_tagging`, :index:`predict_tide`, :index:`composites`, :index:`dask`, :index:`write_geotiff`,  :index:`filmstrip_plot`, :index:`geopandas`, :index:`point_in_polygon`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import datacube\n",
    "import itertools\n",
    "import warnings\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from otps import TimePoint\n",
    "from otps import predict_tide\n",
    "from datetime import datetime, timedelta\n",
    "from datacube.utils import geometry\n",
    "from datacube.utils.geometry import CRS\n",
    "from datacube_stats.statistics import GeoMedian\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from datacube.helpers import ga_pq_fuser\n",
    "from datacube.helpers import write_geotiff\n",
    "from datacube.storage import masking\n",
    "from shapely.geometry import Point\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "sys.path.append('../10_Scripts')\n",
    "import SpatialTools, DEAPlotting\n",
    "\n",
    "# For nicer notebook plotting, hide warnings (comment out for real analysis)\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "# Create datacube instance\n",
    "# dc = datacube.Datacube(app='Tidal geomedian filmstrips')\n",
    "from datacube.model import Range\n",
    "dc = datacube.Datacube(config='/home/561/rt1527/.ard-interoperability_tmp.conf')\n",
    "\n",
    "\n",
    "\n",
    "def date_range(start_date, end_date, increment, period):\n",
    "    \n",
    "    \"\"\"Generate dates seperated by given time increment/period\"\"\"\n",
    "    \n",
    "    result = []\n",
    "    nxt = start_date\n",
    "    delta = relativedelta(**{period:increment})\n",
    "    while nxt <= end_date:\n",
    "        result.append(nxt)\n",
    "        nxt += delta\n",
    "    return result\n",
    "\n",
    "def ds_to_rgb(ds, bands=['red', 'green', 'blue'], reflect_stand=4000):\n",
    "    \n",
    "    \"\"\"Converts an xarray dataset to a three band array\"\"\"\n",
    "    \n",
    "    rawimg = np.transpose(ds[bands].to_array().data, [1, 2, 0])\n",
    "    img_toshow = (rawimg / reflect_stand).clip(0, 1)\n",
    "    return img_toshow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geomedian filmstrip parameters\n",
    "Set the area, time period  and sensors of interest, and tide limits and epoch length used to produce each geomedian composite. This is the only cell that needs to be edited to run the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"width:100%;\"><div style=\"position:relative;width:100%;height:0;padding-bottom:60%;\"><iframe src=\"data:text/html;charset=utf-8;base64,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\" style=\"position:absolute;width:100%;height:100%;left:0;top:0;border:none !important;\" allowfullscreen webkitallowfullscreen mozallowfullscreen></iframe></div></div>"
      ],
      "text/plain": [
       "<folium.folium.Map at 0x7f61ccc28eb8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Set up centre of study area and buffer size in metres for data extraction\n",
    "# study_area = 'narrabeen_utm'  # name used as prefix for output files\n",
    "# lat, lon = -33.72, 151.3006  # centre of study area\n",
    "# buffer = 2000  # metre units to extend region of interest on each side of centre point\n",
    "\n",
    "# # Set up centre of study area and buffer size in metres for data extraction\n",
    "# study_area = 'avoca_utm'  # name used as prefix for output files\n",
    "# lat, lon = -33.463600, 151.44  # centre of study area\n",
    "# buffer = 1000  # metre units to extend region of interest on each side of centre point\n",
    "\n",
    "# study_area = 'stuart_utm'  # name used as prefix for output files\n",
    "# lat, lon = -12.255, 131.865  # centre of study area\n",
    "# buffer = 7000  # metre units to extend region of interest on each side of centre point\n",
    "# ratio = 3\n",
    "# output_crs = 'EPSG:32752'\n",
    "\n",
    "# study_area = 'cabarita'  # name used as prefix for output files\n",
    "# lat, lon = -28.29, 153.575  # centre of study area\n",
    "# buffer = 9000  # metre units to extend region of interest on each side of centre point\n",
    "# ratio = 0.3\n",
    "# output_crs = 'EPSG:32756'\n",
    "\n",
    "# study_area = 'avoca'  # name used as prefix for output files\n",
    "# lat, lon = -33.4, 151.47  # centre of study area\n",
    "# buffer = 15000  # metre units to extend region of interest on each side of centre point\n",
    "# ratio = 1\n",
    "# output_crs = 'EPSG:32756'\n",
    "\n",
    "# study_area = 'quinnsbeach'  # name used as prefix for output files\n",
    "# lat, lon = -31.665, 115.689 # centre of study area\n",
    "# buffer = 4000  # metre units to extend region of interest on each side of centre point\n",
    "# ratio = 0.8\n",
    "# output_crs = 'EPSG:32750'\n",
    "\n",
    "study_area = 'stuart_utm'  # name used as prefix for output files\n",
    "lat, lon = -12.19, 131.78  # centre of study area\n",
    "buffer = 17000  # metre units to extend region of interest on each side of centre point\n",
    "ratio = 3.6\n",
    "output_crs = 'EPSG:32752'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Set up compositing parameters\n",
    "time_period = ('2018-01-01', '2019-01-01')  # Total date range to import data from\n",
    "sensors = ['ls5', 'ls7', 'ls8']  # Landsat sensors from which to import data from\n",
    "ls7_slc_off = True # Whether to include striped > May 31 2003 Landsat 7 SLC-off data\n",
    "lower_tideheight = 0.4 # Minimum proportion of the observed tidal range to include \n",
    "upper_tideheight = 1.0  # Maximum proportion of the observed tidal range to include \n",
    "epoch_years = 1  # Length of each epoch used to compute geomedians (i.e. 5 years = 1988 to 1993)\n",
    "\n",
    "# Set up query\n",
    "x, y = geometry.point(lon, lat, CRS('WGS84')).to_crs(CRS('EPSG:3577')).points[0]\n",
    "query = {'x': (x - buffer * ratio, x + buffer * ratio),  # \n",
    "         'y': (y - buffer, y + buffer),\n",
    "         'time': time_period,\n",
    "         'crs': 'EPSG:3577'}\n",
    "\n",
    "DEAPlotting.display_map(x=query['x'], y=query['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = {'x': (-14956.828215981925, 58643.17178401808),\n",
    "#  'y': (-1329934.5257599277, -1265934.5257599277),\n",
    "#  'time': ('1996-01-01', '1996-12-31'),\n",
    "#  'crs': 'EPSG:3577'}\n",
    "\n",
    "# sensor_all = dc.load(product = 'ls5_ard', \n",
    "#                      group_by = 'solar_day', \n",
    "#                      output_crs = 'EPSG:32752',\n",
    "#                      measurements = ['nbart_red', 'nbart_blue', 'nbart_green', 'nbart_nir', 'nbart_swir_1', 'nbart_swir_2', 'fmask'],\n",
    "#                      gqa_iterative_mean_xy=(0, 1),    # tolerate no more than a one pixel shift\n",
    "#                      align = (15, 15),  # fix for 15m NW offset in collection upgrade\n",
    "#                      resolution = (-100, 100),\n",
    "#                      **query)\n",
    "\n",
    "# # Fix invalid\n",
    "# sensor_all = sensor_all.where(sensor_all != -999.0)\n",
    "# sensor_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dc.find_datasets(product='ls5_ard', **query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sensor_all.nbart_swir_1.plot(figsize=(10, 10), col='time', col_wrap = 4)\n",
    "\n",
    "# DEAPlotting.rgb(sensor_all, bands=['nbart_red', 'nbart_green', 'nbart_blue'], col='time', percentile_stretch=(0.05, 0.75))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Return observations for each sensor for the entire time series\n",
    "Use `dask` to lazily load Landsat data and PQ for each sensor for the entire time series. No data is actually loaded here: this is saved until after the layers have been filtered by tidal stage and date to save processing time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1 epochs: 2018-01-01\n",
      "Using tidepost coordinates: 131.78 E, -12.19 S\n",
      "\n",
      "ls5 selection failed due to likely lack of data in time_period; skipping\n",
      "Computing tidal heights for 17 ls7 observations\n",
      "Computing tidal heights for 42 ls8 observations\n"
     ]
    }
   ],
   "source": [
    "# If output data and figure directories doesn't exist, create them\n",
    "if not os.path.isdir('output_data/{}/'.format(study_area)):\n",
    "    os.makedirs('output_data/{}/'.format(study_area))\n",
    "    \n",
    "if not os.path.isdir('figures/{}/'.format(study_area)):\n",
    "    os.makedirs('figures/{}/'.format(study_area))\n",
    "\n",
    "# Create list of epochs between start and end of time_period in datetime format\n",
    "start = datetime.strptime(time_period[0], '%Y-%m-%d')\n",
    "end = datetime.strptime(time_period[1], '%Y-%m-%d')\n",
    "epochs = date_range(start, end, epoch_years, 'years')\n",
    "\n",
    "# Print list of epochs\n",
    "epochs_strings = [epoch.strftime('%Y-%m-%d') for epoch in epochs][:-1]\n",
    "print('Processing {} epochs: {}'.format(len(epochs_strings), ', '.join(epochs_strings)))\n",
    "\n",
    "# # Identify tide point by importing ITEM polygons\n",
    "# polygons_gpd = gpd.read_file('/g/data/r78/intertidal/GA_native_tidal_model.shp')\n",
    "# polygon_gpd = polygons_gpd[polygons_gpd.intersects(Point(lon, lat))]\n",
    "# tidepost_lon, tidepost_lat = polygon_gpd[['lon', 'lat']].values[0]\n",
    "tidepost_lat, tidepost_lon  = lat, lon\n",
    "print('Using tidepost coordinates: {} E, {} S\\n'.format(tidepost_lon, tidepost_lat))\n",
    "\n",
    "# Output dicts to hold entire time-series for each sensor\n",
    "sensor_dict = {}\n",
    "\n",
    "# For each sensor, dask load data and compute tide heights for each sensor\n",
    "for sensor in sensors:\n",
    "    \n",
    "    # The following code will fail if there is no data for the sensor in time_period;\n",
    "    # this try statement catches this and skips the affected sensor    \n",
    "    try:\n",
    "    \n",
    "        # Return observations matching query without actually loading them using dask\n",
    "        sensor_all = dc.load(product = '{}_ard'.format(sensor), \n",
    "                             group_by = 'solar_day', \n",
    "                             output_crs = output_crs,\n",
    "                             measurements = ['nbart_red', 'nbart_blue', 'nbart_green', 'nbart_nir', 'nbart_swir_1', 'nbart_swir_2', 'fmask'],\n",
    "                             resolution = (-30, 30),\n",
    "                             dask_chunks={'time': 1},\n",
    "                             gqa_iterative_mean_xy=(0, 1),    # tolerate no more than a one pixel shift\n",
    "                             align = (15, 15),  # fix for 15m NW offset in collection upgrade\n",
    "                             fuse_func=None,\n",
    "                             **query)\n",
    "\n",
    "        \n",
    "        # Remove Landsat 7 SLC-off from PQ layer if ls7_slc_off = False\n",
    "        if not ls7_slc_off and sensor == 'ls7':\n",
    "            sensor_all = sensor_all.where(sensor_all.time < np.datetime64('2003-05-30'), drop=True) \n",
    "\n",
    "        # Use the tidal mode to compute tide heights for each observation:\n",
    "        print('Computing tidal heights for {} {} observations'.format(len(sensor_all.time), sensor))\n",
    "        obs_datetimes = sensor_all.time.data.astype('M8[s]').astype('O').tolist()\n",
    "        obs_timepoints = [TimePoint(tidepost_lon, tidepost_lat, dt) for dt in obs_datetimes]\n",
    "        obs_predictedtides = predict_tide(obs_timepoints)\n",
    "        obs_tideheights = [predictedtide.tide_m for predictedtide in obs_predictedtides]\n",
    "\n",
    "        # Assign tide heights to the dataset as a new variable\n",
    "        sensor_all['tide_heights'] = xr.DataArray(obs_tideheights, [('time', sensor_all.time)]) \n",
    "\n",
    "        # Append results for each sensor to a dictionary with sensor name as the key\n",
    "        sensor_all = sensor_all.rename({'nbart_red': 'red', 'nbart_blue': 'blue', 'nbart_green': 'green', \n",
    "                                        'nbart_nir': 'nir', 'nbart_swir_1': 'swir1', 'nbart_swir_2': 'swir2'})\n",
    "        sensor_dict[sensor] = sensor_all\n",
    "    \n",
    "    except AttributeError:\n",
    "        \n",
    "        print('{} selection failed due to likely lack of data in time_period; skipping'.format(sensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset data by tidal height and epoch time range\n",
    "Use image time-stamps to compute tidal heights at the time of each observation, and take a subset of the entire Landsat and PQ time series based on tide height and epoch date range for each sensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keeping tidal heights of -0.16 m to 1.18 m out of an observed local tidal range of -1.04 m to 1.18 m\n",
      "    Filtering from 2018-01-01 to 2019-01-01 for ls7\n",
      "    Filtering from 2018-01-01 to 2019-01-01 for ls8\n"
     ]
    }
   ],
   "source": [
    "# If any sensor has 0 observations, remove it from the dictionary before proceeding\n",
    "sensor_dict = {key:value for key, value in sensor_dict.items() if len(value.time) > 0}\n",
    "\n",
    "# Calculate max and min tide heights for the entire time series and all sensors\n",
    "obs_min = np.min([sensor_ds.tide_heights.min() for sensor_ds in sensor_dict.values()])\n",
    "obs_max = np.max([sensor_ds.tide_heights.max() for sensor_ds in sensor_dict.values()])\n",
    "obs_range = obs_max - obs_min\n",
    "\n",
    "# Calculate tidal limits used for subsequent data selection\n",
    "sel_min = obs_min + (obs_range * lower_tideheight)\n",
    "sel_max = obs_min + (obs_range * upper_tideheight)\n",
    "print('Keeping tidal heights of {0:.2f} m to {1:.2f} m out of an observed local tidal ' \n",
    "      'range of {2:.2f} m to {3:.2f} m'.format(sel_min, sel_max, obs_min, obs_max))\n",
    "\n",
    "sel_min = xr.concat([sensor_ds.tide_heights for sensor_ds in sensor_dict.values()], dim='time').median().item()\n",
    "\n",
    "# Create dictionaries to hold filtered sensor data for each epoch\n",
    "sensor_epoch_dict = defaultdict(list)\n",
    "pq_epoch_dict = defaultdict(list)\n",
    "   \n",
    "for epoch, sensor in itertools.product(epochs[:-1], sensor_dict.keys()):\n",
    "                                           \n",
    "    # Select dataset\n",
    "    sensor_subset = sensor_dict[sensor]  \n",
    "    len_before = len(sensor_subset.time)\n",
    "    \n",
    "    # Filter by tidal stage\n",
    "    sensor_subset = sensor_subset.where((sensor_subset.tide_heights >= sel_min) &\n",
    "                                        (sensor_subset.tide_heights <= sel_max), drop = True)\n",
    "    len_after = len(sensor_subset.time)\n",
    "    \n",
    "\n",
    "    # Identify from and to date strings\n",
    "    from_date = epoch.strftime('%Y-%m-%d')\n",
    "    to_date = (epoch + relativedelta(years=epoch_years)).strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Subset sensor to from and to date for epoch\n",
    "    print('    Filtering from {} to {} for {}'.format(from_date, to_date, sensor))\n",
    "    sensor_subset = sensor_subset.sel(time=slice(from_date, to_date))\n",
    "\n",
    "    # Add subsetted data to dicts (one key matching a list of sensor data for each epoch)\n",
    "    sensor_epoch_dict[from_date].append(sensor_subset)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine multiple sensors, load data and generate geomedians\n",
    "For each epoch, combine all sensors into one dataset, load the data for the first time using `dask`'s `.compute()`, then composite all timesteps into a single array using a geometric median computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and combining ls7, ls8 data for 2018-01-01\n",
      "    Applying PQ mask and setting nodata to NaN\n",
      "    Computing NDWI, MNDWI and AWEI median\n",
      "    Exporting to file\n"
     ]
    }
   ],
   "source": [
    "# Dict to hold output geomedian composites\n",
    "ndwi_dict = {}\n",
    "mndwi_dict = {}\n",
    "awei_ns_dict = {}\n",
    "awei_s_dict = {}\n",
    "tideheights_dict = {}\n",
    "\n",
    "for from_date in sensor_epoch_dict.keys():\n",
    "    \n",
    "    # For the first time, actually load data for all sensors and then combine into one dataset  \n",
    "    print('Loading and combining {} data for {}'.format(\", \".join(sensor_dict.keys()), from_date))\n",
    "    sensor_combined = xr.concat([i.compute() for i in sensor_epoch_dict[from_date]], dim='time')\n",
    "    \n",
    "    # Sort output datasets by date\n",
    "    sensor_combined = sensor_combined.sortby('time')\n",
    "    \n",
    "    # Identify pixels with valid data\n",
    "    good_quality = np.isin(sensor_combined['fmask'], test_elements=(0, 2, 3), invert=True)\n",
    "    good_quality = sensor_combined['fmask'].where(good_quality).notnull()\n",
    "    \n",
    "    # Apply mask to set all PQ-affected pixels to NaN and set nodata to NaN\n",
    "    print('    Applying PQ mask and setting nodata to NaN')\n",
    "    sensor_combined = sensor_combined.where(good_quality)\n",
    "    sensor_combined = masking.mask_invalid_data(sensor_combined)\n",
    "    \n",
    "    # Compute NDWI\n",
    "    sensor_combined[\"ndwi\"] = (sensor_combined.green - sensor_combined.nir) / (sensor_combined.green + sensor_combined.nir)\n",
    "    sensor_combined[\"mndwi\"] = (sensor_combined.green - sensor_combined.swir1) / (sensor_combined.green + sensor_combined.swir1)\n",
    "    sensor_combined[\"awei_ns\"] = (4 * (sensor_combined.green * 0.0001 - sensor_combined.swir1 * 0.0001) -\n",
    "                              (0.25 * sensor_combined.nir * 0.0001 + 2.75 * sensor_combined.swir2 * 0.0001))\n",
    "    sensor_combined[\"awei_s\"] = (sensor_combined.blue * 0.0001 + 2.5 * sensor_combined.green * 0.0001 - \n",
    "                                 1.5 * (sensor_combined.nir * 0.0001 + sensor_combined.swir1 * 0.0001) - 0.25 * sensor_combined.swir2 * 0.0001)\n",
    "\n",
    "    # Compute NDWI composite using all timesteps\n",
    "    print('    Computing NDWI, MNDWI and AWEI median')\n",
    "    ndwi_median = sensor_combined[[\"ndwi\"]].median(dim='time', keep_attrs=True)\n",
    "    mndwi_median = sensor_combined[[\"mndwi\"]].median(dim='time', keep_attrs=True)\n",
    "    awei_ns_median = sensor_combined[[\"awei_ns\"]].median(dim='time', keep_attrs=True)\n",
    "    awei_s_median = sensor_combined[[\"awei_s\"]].median(dim='time', keep_attrs=True)\n",
    "    \n",
    "    # Export to file\n",
    "    print('    Exporting to file')\n",
    "    filename_ndwi = 'output_data/{0}/{0}_{1}_ndwi.tif'.format(study_area, from_date)\n",
    "    filename_mndwi = 'output_data/{0}/{0}_{1}_mndwi.tif'.format(study_area, from_date)\n",
    "    filename_awei_ns = 'output_data/{0}/{0}_{1}_awei_ns.tif'.format(study_area, from_date)\n",
    "    filename_awei_s = 'output_data/{0}/{0}_{1}_awei_s.tif'.format(study_area, from_date)\n",
    "    write_geotiff(filename=filename_ndwi, dataset=ndwi_median)\n",
    "    write_geotiff(filename=filename_mndwi, dataset=mndwi_median)\n",
    "    write_geotiff(filename=filename_awei_ns, dataset=awei_ns_median)\n",
    "    write_geotiff(filename=filename_awei_s, dataset=awei_s_median)\n",
    "    \n",
    "    # Assign to dict\n",
    "    ndwi_dict[from_date] = ndwi_median\n",
    "    mndwi_dict[from_date] = mndwi_median\n",
    "    awei_ns_dict[from_date] = awei_ns_median\n",
    "    awei_s_dict[from_date] = awei_s_median\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract waterlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-01\n",
      "Operating in single array, multiple z-values mode\n",
      "    Extracting contour 0.1\n",
      "Exporting contour shapefile to output_data/stuart_utm/stuart_utm_awei_s_2018-01-01.shp\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.1\n",
    "metric = 'awei_s'\n",
    "\n",
    "from skimage import filters\n",
    "\n",
    "for date, data in awei_s_dict.items():\n",
    "    \n",
    "    print(date)\n",
    "    \n",
    "    # Prepare attributes as input to contour extract\n",
    "    attribute_data = {'year': [date[0:4]]}\n",
    "    attribute_dtypes = {'year': 'int'}\n",
    "    \n",
    "    try:\n",
    "\n",
    "#         threshold = filters.threshold_otsu(data[metric].values)\n",
    "\n",
    "        # Extract contours with custom attribute fields:\n",
    "        contour_dict = SpatialTools.contour_extract(z_values=[threshold],\n",
    "                                       ds_array=data[metric],\n",
    "                                       ds_crs=output_crs,\n",
    "                                       ds_affine=data.geobox.transform,\n",
    "                                       output_shp=f'output_data/{study_area}/{study_area}_{metric}_{date}.shp',\n",
    "                                       attribute_data=attribute_data,\n",
    "                                       min_vertices=500,\n",
    "                                       attribute_dtypes=attribute_dtypes)\n",
    "    except:\n",
    "        print('none')\n",
    "    \n",
    "# Combine all shapefiles into one file\n",
    "import glob\n",
    "shapefiles = glob.glob(f'output_data/{study_area}/{study_area}_{metric}_*01-01.shp')\n",
    "gdf = pd.concat([gpd.read_file(shp) for shp in shapefiles], sort=False).pipe(gpd.GeoDataFrame)\n",
    "\n",
    "# Save as combined shapefile\n",
    "gdf = gdf.reset_index()[['year', 'geometry']].sort_values('year')\n",
    "gdf['year'] = gdf['year'].astype(np.int16)\n",
    "gdf.crs = output_crs\n",
    "gdf.to_file(f'output_data/{study_area}/{study_area}_{metric}_combined.shp')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation against Narrabeen beach width data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_area"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# from pyproj import Proj, transform\n",
    "# from shapely.geometry import Point, LineString\n",
    "# import geopandas as gpd\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# def dms2dd(s):\n",
    "#     # example: s = \"0°51'56.29\"\n",
    "#     degrees, minutes, seconds = re.split('[°\\'\"]+', s)\n",
    "#     if float(degrees) > 0:\n",
    "#         dd = float(degrees) + float(minutes)/60 + float(seconds)/(60*60)\n",
    "#     else:\n",
    "#         dd = float(degrees) - float(minutes)/60 - float(seconds)/(60*60);\n",
    "#     return dd\n",
    "\n",
    "# def dist_angle(lon, lat, dist, angle):\n",
    "#     lon_2 = lon + dist *  np.sin(angle * np.pi / 180)\n",
    "#     lat_2 = lat + dist *  np.cos(angle * np.pi / 180)\n",
    "#     return pd.Series({'y2': lat_2, 'x2': lon_2})\n",
    "\n",
    "\n",
    "# # Import data and parse DMS to DD\n",
    "# data = \"PF1 -33°42'20.65 151°18'16.30 118.42\\nPF2 -33°42'33.45 151°18'10.33 113.36\\nPF4 -33°43'01.55 151°17'58.84 100.26\\nPF6 -33°43'29.81 151°17'58.65 83.65\\nPF8 -33°43'55.94 151°18'06.47 60.48\"\n",
    "# coords = pd.read_csv(pd.compat.StringIO(data), sep=' ', names=['site', 'y', 'x', 'angle'])\n",
    "# coords['x'] = [dms2dd(i) for i in coords.x]\n",
    "# coords['y'] = [dms2dd(i) for i in coords.y]\n",
    "\n",
    "# # Extend survey lines out from start coordinates using supplied angle\n",
    "# coords_end = coords.apply(lambda x: dist_angle(x.x, x.y, 0.005, x.angle), axis = 1)\n",
    "# coords = pd.concat([coords, coords_end], axis=1).drop('angle', axis=1)\n",
    "\n",
    "# # Reproject coords to Albers and create geodataframe\n",
    "# inProj = Proj(init='epsg:4326')\n",
    "# outProj = Proj(init='epsg:32756')\n",
    "# coords['x'], coords['y'] = transform(inProj,outProj,coords.x.values,coords.y.values)\n",
    "# coords['x2'], coords['y2'] = transform(inProj,outProj,coords.x2.values,coords.y2.values)\n",
    "# coords['geometry'] = coords.apply(lambda x: LineString([Point(x.x, x.y), Point(x.x2, x.y2)]), axis = 1)\n",
    "# transects_gdf = gpd.GeoDataFrame(coords, geometry='geometry').set_index('site')\n",
    "# transects_gdf.crs = 'EPSG:32756'\n",
    "\n",
    "# # Export to file\n",
    "# transects_gdf.reset_index().to_file('narrabeen_transects.shp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_list = []\n",
    "\n",
    "# for transect_id in ['PF1', 'PF2', 'PF4', 'PF6', 'PF8']:\n",
    "\n",
    "#     ###########\n",
    "#     # Landsat #\n",
    "#     ###########\n",
    "\n",
    "#     # Import contours\n",
    "#     contours_gpf = gpd.read_file('output_data/narrabeen_utm/narrabeen_utm_ndwi_combined.shp').set_index('date')\n",
    "#     contours_gdf = contours_gpf.drop([1987.0, 2016.0, 2017.0])\n",
    "\n",
    "#     # Extract intersections for transect \n",
    "#     intersect_gdf = gpd.GeoDataFrame(geometry = contours_gdf.intersection(transects_gdf.loc[transect_id].geometry))\n",
    "#     intersect_gdf.crs = 'EPSG:32756'\n",
    "# #     intersect_gdf.reset_index().to_file('intersect_transect1.shp')\n",
    "\n",
    "#     # Compute distances\n",
    "#     landsat_beachwidths = intersect_gdf.distance(Point(transects_gdf.loc[transect_id].geometry.coords[0]))\n",
    "#     landsat_beachwidths = landsat_beachwidths.rename('Landsat beach width (m)')\n",
    "\n",
    "\n",
    "#     ##############\n",
    "#     # Validation #\n",
    "#     ##############\n",
    "\n",
    "#     profiles_df = pd.read_csv('/g/data/r78/rt1527/dea-notebooks/Waterline_extraction/validation/Narrabeen/Narrabeen_Profiles.csv', \n",
    "#                               skiprows=1, names=['site', 'date', 'distance', 'elevation', 'flag'])\n",
    "\n",
    "#     # Set to datetime\n",
    "#     profiles_df['date'] = pd.to_datetime(profiles_df['date'], format='%d/%m/%Y')\n",
    "\n",
    "#     # Restrict to years\n",
    "#     profiles_df = profiles_df[(profiles_df.date.dt.year > 1987) & (profiles_df.date.dt.year < 2016)]\n",
    "\n",
    "#     # Use linear interpolation to find distance along beach of 0 elevation \n",
    "#     from scipy.interpolate import interp1d\n",
    "#     profiles_interp = profiles_df.groupby(['site', 'date']).apply(lambda x: interp1d(x.elevation, x.distance)(0) if \n",
    "#                                                                     x.elevation.min() < 0 else np.nan)  #.plot()\n",
    "\n",
    "#     profiles_interp = profiles_interp.loc[transect_id].astype(np.float)\n",
    "#     validation_beachwidths = profiles_interp.groupby(profiles_interp.index.year).mean()\n",
    "#     validation_beachwidths_std = profiles_interp.groupby(profiles_interp.index.year).std()\n",
    "#     validation_beachwidths = validation_beachwidths.rename('Validation beach width (m)')\n",
    "#     validation_beachwidths_std = validation_beachwidths_std.rename('Validation beach width (stdev)')\n",
    "\n",
    "#     # Combine into a single dataframe and append to output list\n",
    "#     landsat_validation_df = pd.DataFrame([validation_beachwidths, validation_beachwidths_std, landsat_beachwidths]).T\n",
    "#     landsat_validation_df['site'] = transect_id\n",
    "#     landsat_validation_df['RMSE'] = ((landsat_validation_df['Validation beach width (m)'] - landsat_validation_df['Landsat beach width (m)']) ** 2).mean() ** .5\n",
    "#     out_list.append(landsat_validation_df)\n",
    "    \n",
    "# # Plot\n",
    "# import seaborn as sns \n",
    "# combined_df_wide = pd.concat(out_list).reset_index()\n",
    "# combined_df_wide['Validation beach width (min stdev)'] = combined_df_wide['Validation beach width (m)'] - combined_df_wide['Validation beach width (stdev)']\n",
    "# combined_df_wide['Validation beach width (max stdev)'] = combined_df_wide['Validation beach width (m)'] + combined_df_wide['Validation beach width (stdev)']\n",
    "# combined_df_wide =  combined_df_wide.drop(['Validation beach width (stdev)'], axis=1)\n",
    "# combined_df_wide = combined_df_wide.iloc[:, [0, 1, 5, 6, 2, 3, 4]]\n",
    "\n",
    "# combined_df = pd.melt(combined_df_wide, id_vars=['site', 'date', 'RMSE'], value_name='Beach width (m)')\n",
    "# g = sns.catplot(data=combined_df, x='date', y='Beach width (m)', hue='variable', col='site', kind='point', legend=False, col_wrap=3, \n",
    "#                 palette=['#1f77b4', '#a8d2f0', '#a8d2f0', '#ff7f0e'], markers=[\".\", \"\", \"\", \".\"], linestyles=['-', ':', ':', '-'])\n",
    "# g.set_xticklabels(rotation=90)\n",
    "\n",
    "# axes = g.axes.flatten()\n",
    "# for i, ax in enumerate(axes):\n",
    "#     current_title = ax.get_title()\n",
    "#     current_rmse = np.round(combined_df.groupby('site').RMSE.max()[i], 2)\n",
    "#     ax.set_title(f'{current_title}\\nRMSE = {np.round(current_rmse, 2)} m')\n",
    "# plt.legend(loc='upper right')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.lmplot(data=combined_df_wide, x='Validation beach width (m)', y='Landsat beach width (m)', height=10, aspect=1.0)\n",
    "# plt.plot(np.linspace(0, 150), np.linspace(0, 150), 'black', linestyle='dashed')\n",
    "# plt.ylim([10,150])\n",
    "# plt.xlim([10,150])\n",
    "\n",
    "# print(((combined_df_wide['Validation beach width (m)'] - combined_df_wide['Landsat beach width (m)']) ** 2).mean() ** .5)\n",
    "# import scipy.stats\n",
    "# print(scipy.stats.linregress(combined_df_wide['Validation beach width (m)'], combined_df_wide['Landsat beach width (m)']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.lmplot(data=combined_df_wide, x='Validation beach width (m)', y='Landsat beach width (m)',hue='site', height=7, aspect=1.0)\n",
    "# plt.plot(np.linspace(0, 150), np.linspace(0, 150), 'black', linestyle='dashed')\n",
    "# plt.ylim([10,150]); plt.xlim([10,150]);\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
