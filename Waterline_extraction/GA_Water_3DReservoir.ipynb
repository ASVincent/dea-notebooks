{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derive waterbody relative topography using Landsat\n",
    "\n",
    "This notebook demonstrates how to load Landsat time series data, compute a water index, generate a rolling median water index composites, extract contours along the land-water boundary, and finally interpolate between contours to produce a 3D relative topographic surface. This relative topography could be easily calibrated to obtain absolute bathymetry (and accordingly, volume estimates) with a simple GPS transect from the highest to the deepest part of the lake during a dry period.\n",
    "\n",
    "**Author**: Robbi Bishop-Taylor\n",
    "\n",
    "**Date**: 30 October 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supress Warning \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import datacube\n",
    "import sys\n",
    "import glob\n",
    "import gdal\n",
    "import affine\n",
    "import fiona\n",
    "import collections\n",
    "import rasterio\n",
    "import scipy\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from rasterio.features import shapes\n",
    "from shapely import geometry\n",
    "from shapely.geometry import MultiLineString, mapping\n",
    "from rasterstats import zonal_stats\n",
    "from skimage import measure\n",
    "from skimage import filters\n",
    "from skimage import exposure\n",
    "\n",
    "from datacube.utils import geometry\n",
    "from datacube.utils.geometry import CRS\n",
    "\n",
    "\n",
    "dc = datacube.Datacube()\n",
    "\n",
    "\n",
    "def contour_extract(z_values, ds_array, ds_crs, ds_affine, output_shp=None, min_vertices=2,\n",
    "                    attribute_data=None, attribute_dtypes=None):\n",
    "\n",
    "    \"\"\"\n",
    "    Uses `skimage.measure.find_contours` to extract contour lines from a two-dimensional array.\n",
    "    Contours are extracted as a dictionary of xy point arrays for each contour z-value, and optionally as\n",
    "    line shapefile with one feature per contour z-value.\n",
    "    The `attribute_data` and `attribute_dtypes` parameters can be used to pass custom attributes to the output\n",
    "    shapefile.\n",
    "    Last modified: September 2018\n",
    "    Author: Robbi Bishop-Taylor\n",
    "    :param z_values:\n",
    "        A list of numeric contour values to extract from the array.\n",
    "    :param ds_array:\n",
    "        A two-dimensional array from which contours are extracted. This can be a numpy array or xarray DataArray.\n",
    "        If an xarray DataArray is used, ensure that the array has one two dimensions (e.g. remove the time dimension\n",
    "        using either `.isel(time=0)` or `.squeeze('time')`).\n",
    "    :param ds_crs:\n",
    "        Either a EPSG string giving the coordinate system of the array (e.g. 'EPSG:3577'), or a crs\n",
    "        object (e.g. from an xarray dataset: `xarray_ds.geobox.crs`).\n",
    "    :param ds_affine:\n",
    "        Either an affine object from a rasterio or xarray object (e.g. `xarray_ds.geobox.affine`), or a gdal-derived\n",
    "        geotransform object (e.g. `gdal_ds.GetGeoTransform()`) which will be converted to an affine.\n",
    "    :param min_vertices:\n",
    "        An optional integer giving the minimum number of vertices required for a contour to be extracted. The default\n",
    "        (and minimum) value is 2, which is the smallest number required to produce a contour line (i.e. a start and\n",
    "        end point). Higher values remove smaller contours, potentially removing noise from the output dataset.\n",
    "    :param output_shp:\n",
    "        An optional string giving a path and filename for the output shapefile. Defaults to None, which\n",
    "        does not generate a shapefile.\n",
    "    :param attribute_data:\n",
    "        An optional dictionary of lists used to define attributes/fields to add to the shapefile. Dict keys give\n",
    "        the name of the shapefile attribute field, while dict values must be lists of the same length as `z_values`.\n",
    "        For example, if `z_values=[0, 10, 20]`, then `attribute_data={'type: [1, 2, 3]}` can be used to create a\n",
    "        shapefile field called 'type' with a value for each contour in the shapefile. The default is None, which\n",
    "        produces a default shapefile field called 'z_value' with values taken directly from the `z_values` parameter\n",
    "        and formatted as a 'float:9.2'.\n",
    "    :param attribute_dtypes:\n",
    "        An optional dictionary giving the output dtype for each shapefile attribute field that is specified by\n",
    "        `attribute_data`. For example, `attribute_dtypes={'type: 'int'}` can be used to set the 'type' field to an\n",
    "        integer dtype. The dictionary should have the same keys/field names as declared in `attribute_data`.\n",
    "        Valid values include 'int', 'str', 'datetime, and 'float:X.Y', where X is the minimum number of characters\n",
    "        before the decimal place, and Y is the number of characters after the decimal place.\n",
    "    :return:\n",
    "        A dictionary with contour z-values as the dict key, and a list of xy point arrays as dict values.\n",
    "    :example:\n",
    "    >>> # Import modules\n",
    "    >>> import sys\n",
    "    >>> import datacube\n",
    "    >>> # Import external dea-notebooks functions using relative link to Scripts directory\n",
    "    >>> sys.path.append('../10_Scripts')\n",
    "    >>> import SpatialTools\n",
    "    >>> # Set up datacube instance\n",
    "    >>> dc = datacube.Datacube(app='Contour extraction')\n",
    "    >>> # Define an elevation query\n",
    "    >>> elevation_query = {'lat': (-35.25, -35.35),\n",
    "    ...                    'lon': (149.05, 149.17),\n",
    "    ...                    'output_crs': 'EPSG:3577',\n",
    "    ...                    'resolution': (-25, 25)}\n",
    "    >>> # Import sample elevation data\n",
    "    >>> elevation_data = dc.load(product='srtm_dem1sv1_0', **elevation_query)\n",
    "    >>> # Remove the time dimension so that array is two-dimensional\n",
    "    >>> elevation_2d = elevation_data.dem_h.squeeze('time')\n",
    "    >>> # Extract contours\n",
    "    >>> contour_dict = SpatialTools.contour_extract(z_values=[600, 700, 800],\n",
    "    ...                                             ds_array=elevation_2d,\n",
    "    ...                                             ds_crs=elevation_2d.geobox.crs,\n",
    "    ...                                             ds_affine=elevation_2d.geobox.affine,\n",
    "    ...                                             output_shp='extracted_contours.shp')\n",
    "    Extracting contour 600\n",
    "    Extracting contour 700\n",
    "    Extracting contour 800\n",
    "    <BLANKLINE>\n",
    "    Exporting contour shapefile to extracted_contours.shp\n",
    "    \"\"\"\n",
    "\n",
    "    # First test that input array has only two dimensions:\n",
    "    if len(ds_array.shape) == 2:\n",
    "\n",
    "        # Obtain affine object from either rasterio/xarray affine or a gdal geotransform:\n",
    "        if type(ds_affine) != affine.Affine:\n",
    "\n",
    "            ds_affine = affine.Affine.from_gdal(*ds_affine)\n",
    "\n",
    "        ####################\n",
    "        # Extract contours #\n",
    "        ####################\n",
    "\n",
    "        # Output dict to hold contours for each offset\n",
    "        contours_dict = collections.OrderedDict()\n",
    "\n",
    "        for z_value in z_values:\n",
    "\n",
    "            # Extract contours and convert output array pixel coordinates into arrays of real world Albers coordinates.\n",
    "            # We need to add (0.5 x the pixel size) to x values and subtract (-0.5 * pixel size) from y values to\n",
    "            # correct coordinates to give the centre point of pixels, rather than the top-left corner\n",
    "            print(f'Extracting contour {z_value}')\n",
    "            ps = ds_affine[0]  # Compute pixel size\n",
    "            contours_geo = [np.column_stack(ds_affine * (i[:, 1], i[:, 0])) + np.array([0.5 * ps, -0.5 * ps]) for i in\n",
    "                            find_contours(ds_array, z_value)]\n",
    "\n",
    "            # For each array of coordinates, drop any xy points that have NA\n",
    "            contours_nona = [i[~np.isnan(i).any(axis=1)] for i in contours_geo]\n",
    "\n",
    "            # Drop 0 length and add list of contour arrays to dict\n",
    "            contours_withdata = [i for i in contours_nona if len(i) >= min_vertices]\n",
    "\n",
    "            # If there is data for the contour, add to dict:\n",
    "            if len(contours_withdata) > 0:\n",
    "                contours_dict[z_value] = contours_withdata\n",
    "            else:\n",
    "                print(f'    No data for contour {z_value}; skipping')\n",
    "\n",
    "        #######################\n",
    "        # Export to shapefile #\n",
    "        #######################\n",
    "\n",
    "        # If a shapefile path is given, generate shapefile\n",
    "        if output_shp:\n",
    "\n",
    "            print(f'\\nExporting contour shapefile to {output_shp}')\n",
    "\n",
    "            # If attribute fields are left empty, default to including a single z-value field based on `z_values`\n",
    "            if not attribute_data:\n",
    "\n",
    "                # Default field uses two decimal points by default\n",
    "                attribute_data = {'z_value': z_values}\n",
    "                attribute_dtypes = {'z_value': 'float:9.2'}\n",
    "\n",
    "            # Set up output multiline shapefile properties\n",
    "            schema = {'geometry': 'MultiLineString',\n",
    "                      'properties': attribute_dtypes}\n",
    "\n",
    "            # Create output shapefile for writing\n",
    "            with fiona.open(output_shp, 'w',\n",
    "                            crs={'init': str(ds_crs), 'no_defs': True},\n",
    "                            driver='ESRI Shapefile',\n",
    "                            schema=schema) as output:\n",
    "\n",
    "                # Write each shapefile to the dataset one by one\n",
    "                for i, (z_value, contours) in enumerate(contours_dict.items()):\n",
    "\n",
    "                    # Create multi-string object from all contour coordinates\n",
    "                    contour_multilinestring = MultiLineString(contours)\n",
    "\n",
    "                    # Get attribute values for writing\n",
    "                    attribute_vals = {field_name: field_vals[i] for field_name, field_vals in attribute_data.items()}\n",
    "\n",
    "                    # Write output shapefile to file with z-value field\n",
    "                    output.write({'properties': attribute_vals,\n",
    "                                  'geometry': mapping(contour_multilinestring)})\n",
    "\n",
    "        # Return dict of contour arrays\n",
    "        return contours_dict\n",
    "\n",
    "    else:\n",
    "        print(f'The input `ds_array` has shape {ds_array.shape}. Please input a two-dimensional array (if your '\n",
    "              f'input array has a time dimension, remove it using `.isel(time=0)` or `.squeeze(\\'time\\')`)')\n",
    "\n",
    "        \n",
    "# Extract vertex coordinates and heights from geopandas\n",
    "def contours_to_arrays(gdf, col):\n",
    "    \n",
    "    coords_zvals = []\n",
    "    \n",
    "    for i in range(1, len(gdf)):\n",
    "        \n",
    "        val = gdf.iloc[i][col]\n",
    "    \n",
    "        try:\n",
    "            coords = np.concatenate([np.vstack(x.coords.xy).T for x in gdf.iloc[i].geometry])\n",
    "            \n",
    "        except:\n",
    "            coords = np.vstack(gdf.iloc[i].geometry.coords.xy).T\n",
    "\n",
    "        coords_zvals.append(np.column_stack((coords, np.full(np.shape(coords)[0], fill_value=val))))\n",
    "    \n",
    "    return np.concatenate(coords_zvals)\n",
    "\n",
    "\n",
    "def interpolate_timeseries(ds, freq='7D', method='linear'):\n",
    "    \n",
    "    \"\"\"\n",
    "    Interpolate new data between each existing xarray timestep at a given\n",
    "    frequency. For example, `freq='7D'` will interpolate new values at weekly\n",
    "    intervals from the start time of the xarray dataset to the end time. \n",
    "    `freq='24H'` will interpolate new values for each day, etc.\n",
    "    \n",
    "    :param ds:\n",
    "        The xarray dataset to interpolate new time-step observations for.\n",
    "        \n",
    "    :param freq:\n",
    "        An optional string giving the frequency at which to interpolate new time-step \n",
    "        observations. Defaults to '7D' which interpolates new values at weekly intervals; \n",
    "        for a full list of options refer to Panda's list of offset aliases: \n",
    "        https://pandas.pydata.org/pandas-docs/stable/timeseries.html#timeseries-offset-aliases\n",
    "        \n",
    "    :param method:\n",
    "        An optional string giving the interpolation method to use to generate new time-steps.\n",
    "        Default is 'linear'; options are {'linear', 'nearest'} for multidimensional arrays and\n",
    "        {'linear', 'nearest', 'zero', 'slinear', 'quadratic', 'cubic'} for 1-dimensional arrays.\n",
    "        \n",
    "    :return:\n",
    "        A matching xarray dataset covering the same time period as `ds`, but with an \n",
    "        interpolated for each time-step given by `freq`.\n",
    "        \n",
    "    \"\"\"    \n",
    "    \n",
    "    # Use pandas to generate dates from start to end of ds at a given frequency\n",
    "    start_time = ds.isel(time=0).time.values.item() \n",
    "    end_time = ds.isel(time=-1).time.values.item()    \n",
    "    from_to = pd.date_range(start=start_time, end=end_time, freq=freq)\n",
    "    \n",
    "    # Use these dates to linearly interpolate new data for each new date\n",
    "    print('Interpolating {} time-steps at {} intervals'.format(len(from_to), freq))\n",
    "    return ds.interp(coords={'time': from_to}, method=method)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contour extraction and interpolation parameters\n",
    "min_vertices = 2  # This can be used to remove noise by dropping contours with less than X vertices\n",
    "guassian_sigma = 1  # Controls amount of smoothing to apply to interpolated raster. Higher = smoother\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "name='test'\n",
    "latitude_extents = (-30.1944649466, -30.133245012)\n",
    "longitude_extents = (147.239668043, 147.294627242)\n",
    "time_extents = ('2000-01-01', '2018-11-01')\n",
    "\n",
    "# name='test2'\n",
    "# latitude_extents = (-30.1633720285, -30.127021544)\n",
    "# longitude_extents = (147.306395967, 147.422629533)\n",
    "# time_extents = ('2013-01-01', '2018-11-01')\n",
    "\n",
    "query = dict(latitude=latitude_extents, longitude=longitude_extents, time=time_extents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Landsat data and mask by cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import external functions from dea-notebooks using relative link to Scripts\n",
    "sys.path.append('../10_Scripts')\n",
    "import DEADataHandling\n",
    "\n",
    "landsat_dataset = DEADataHandling.load_clearlandsat(dc=dc, query=query, sensors=['ls8'],\n",
    "                                                   bands_of_interest=['red', 'green', 'blue', 'swir1'],  # ['swir1', 'nir', 'green'], \n",
    "                                                   masked_prop=0, \n",
    "                                                   mask_pixel_quality=True,\n",
    "                                                   mask_invalid_data=True,\n",
    "                                                   ls7_slc_off=False)\n",
    "\n",
    "# Plot single time step\n",
    "landsat_dataset[['red', 'green', 'blue']].isel(time=20).to_array().plot.imshow(robust=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute water index for each timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MNDWI water index (gives best results in turbid inland waters)\n",
    "landsat_dataset['mndwi'] = ((landsat_dataset.green - landsat_dataset.swir1) / \n",
    "                            (landsat_dataset.green + landsat_dataset.swir1))\n",
    "    \n",
    "# landsat_dataset['ndwi'] = (landsat_dataset.green - landsat_dataset.nir) / \\\n",
    "#                            (landsat_dataset.green + landsat_dataset.nir)\n",
    "    \n",
    "# landsat_dataset['awei'] = (4 * (landsat_dataset.green * 0.0001 - landsat_dataset.swir1 * 0.0001) -\n",
    "#                            (0.25 * landsat_dataset.nir * 0.0001 + 2.75 * landsat_dataset.swir2 * 0.0001))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landsat_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute percentages of valid data and inundation per timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mask of max extent of water (land = 0, water = 1) and create new layer with\n",
    "# NaN for all pixels outside max extent area\n",
    "max_water_mask = ~((landsat_dataset.mndwi.fillna(-1) < 0).min(dim='time'))\n",
    "mndwi_water_masked = landsat_dataset.mndwi.where(max_water_mask)\n",
    "\n",
    "# Calculate the valid data percentage for each time step by dividing the number of \n",
    "# non-NaN pixels in timestep by the total number of pixels in the max extent water layer\n",
    "landsat_dataset['data_perc'] = (mndwi_water_masked.count(dim=['x', 'y']) / \n",
    "                               (max_water_mask).sum())\n",
    "\n",
    "# Calculate innundation percent\n",
    "inundation_perc = ((mndwi_water_masked > 0).sum(dim=['x', 'y']) /  \n",
    "                   mndwi_water_masked.count(dim=['x', 'y']))\n",
    "landsat_dataset['inundation_perc'] = inundation_perc\n",
    "\n",
    "# Sort in reverse order by inundation percentage (from wet to dry)\n",
    "landsat_dataset = landsat_dataset.sortby('inundation_perc', ascending=False)\n",
    "\n",
    "\n",
    "# Restrict to scenes with greater than 20% valid data and select variables for further analysis\n",
    "to_keep = landsat_dataset.data_perc > 0.2\n",
    "cleaned_subset = landsat_dataset[['mndwi', 'data_perc', 'inundation_perc']].sel(time = to_keep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (cleaned_subset.sortby('time').isel(time=[27])).mndwi.plot(vmin=0, vmax=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot only observations with greater than 20% valid data\n",
    "timeseries_subset = landsat_dataset.sel(time = to_keep).inundation_perc\n",
    "\n",
    "# # Interpolate to one point per week, then take a rolling mean to smooth line for plotting\n",
    "timeseries_subset = interpolate_timeseries(timeseries_subset.sortby('time'), freq='3D', method='linear')\n",
    "timeseries_subset = timeseries_subset.rolling(time=5, min_periods=1).mean()\n",
    "timeseries_subset.plot()\n",
    "\n",
    "# Export to text file\n",
    "timeseries_subset_df = timeseries_subset.to_dataframe(name='innundation_perc')\n",
    "timeseries_subset_df['date'] = timeseries_subset_df.index.floor('d')\n",
    "timeseries_subset_df.set_index('date')\n",
    "timeseries_subset_df.to_csv('relative_topography/{}_timeseries.csv'.format(name))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute median NDWI composites using a rolling 20 observation median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = len(cleaned_subset.time)\n",
    "print('{} timesteps in total'.format(timesteps))\n",
    "\n",
    "# Create output list to hold NDWI median composites\n",
    "out = []\n",
    "\n",
    "for i in np.arange(0, timesteps, 1):\n",
    "    \n",
    "    # identify min and max index to extract rolling median\n",
    "    min_index = max(i - 20, 0)\n",
    "    max_index = min(i + 20, timesteps)\n",
    "    print('Creating median NDWI composite using indices {} to {}'.format(min_index, max_index))\n",
    "    \n",
    "    # Take median of dates that match indexes and add to list\n",
    "    median_dataset = cleaned_subset.isel(time=slice(min_index, max_index)).median(dim='time')\n",
    "    out.append(median_dataset)\n",
    "out\n",
    "\n",
    "# Combine each median composite into a single xarray dataset\n",
    "combined = xr.concat(out, dim='inundation_perc').sortby('inundation_perc')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract contours from each median composite observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "observations = combined.inundation_perc\n",
    "\n",
    "for i, observation in enumerate(observations):    \n",
    "\n",
    "    cleaned_subset_i = combined.sel(inundation_perc=observation)\n",
    "    \n",
    "    # Compute area\n",
    "    area = cleaned_subset_i.inundation_perc.item() * 100 \n",
    "    \n",
    "    # Prepare attributes as input to contour extract\n",
    "    attribute_data = {'in_perc': [area]}\n",
    "    attribute_dtypes = {'in_perc': 'float'}\n",
    "    \n",
    "    # Set threshold\n",
    "    thresh = 0\n",
    "\n",
    "    # Extract contours with custom attribute fields:\n",
    "    contour_dict = contour_extract(z_values=[thresh],\n",
    "                                   ds_array=cleaned_subset_i.mndwi,\n",
    "                                   ds_crs='epsg:3577',\n",
    "                                   ds_affine=landsat_dataset.geobox.transform,\n",
    "                                   output_shp='relative_topography/{}_{}.shp'.format(name, i),\n",
    "                                   min_vertices=50,  \n",
    "                                   attribute_data=attribute_data,\n",
    "                                   attribute_dtypes=attribute_dtypes)\n",
    "\n",
    "        \n",
    "# Combine all shapefiles into one file\n",
    "shapefiles = glob.glob('relative_topography/{}_*.shp'.format(name))\n",
    "gdf = pd.concat([gpd.read_file(shp) for shp in shapefiles], sort=False).pipe(gpd.GeoDataFrame)\n",
    "\n",
    "# Save as combined shapefile\n",
    "gdf = gdf.reset_index()[['in_perc', 'geometry']].sort_values('in_perc')\n",
    "gdf.crs = 'epsg:3577'\n",
    "gdf.to_file(f'relative_topography/{name}_combined.shp')\n",
    "\n",
    "# Plot contours\n",
    "fig, ax = plt.subplots(figsize=(16, 16))\n",
    "gdf.plot(ax=ax, column='in_perc', cmap='viridis')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract x, y and z points for interpolation\n",
    "all_contours = contours_to_arrays(gdf=gdf, col='in_perc')\n",
    "points_xy = all_contours[:, [1, 0]]\n",
    "values_elev = all_contours[:, 2]\n",
    "\n",
    "# Create grid to interpolate into\n",
    "x_size, _, upleft_x, _, y_size, upleft_y =  landsat_dataset.geobox.transform[0:6]\n",
    "xcols = len(landsat_dataset.x)\n",
    "yrows = len(landsat_dataset.y)\n",
    "bottomright_x = upleft_x + (x_size * xcols)\n",
    "bottomright_y = upleft_y + (y_size * yrows)\n",
    "yrows, xcols = landsat_dataset.red.shape[1:]\n",
    "grid_y, grid_x = np.mgrid[upleft_y:bottomright_y:1j * yrows, upleft_x:bottomright_x:1j * xcols]\n",
    "\n",
    "# Interpolate x, y and z values using linear/TIN interpolation\n",
    "out = scipy.interpolate.griddata(points_xy, values_elev, (grid_y, grid_x), method='linear')\n",
    "\n",
    "# Set areas outside of NDWI composites to highest innundation percentage\n",
    "test = (combined.mndwi > 0).max(dim='inundation_perc')\n",
    "out[~test] = np.nanmax(out)\n",
    "out[np.isnan(out)] = np.nanmax(out)\n",
    "\n",
    "# Apply guassian blur to smooth transitions between z values (optional)\n",
    "# out = filters.gaussian(out, sigma=guassian_sigma)\n",
    "out = exposure.rescale_intensity(out, out_range=(timeseries_subset.min().item() - 0.001, \n",
    "                                                 timeseries_subset.max().item() + 0.001))\n",
    "\n",
    "# Plot interpolated surface\n",
    "fig, ax = plt.subplots(figsize=(16, 16))\n",
    "ax.imshow(out, cmap='magma_r', extent=[upleft_x, bottomright_x, bottomright_y, upleft_y])\n",
    "# gdf.plot(ax=ax, edgecolor='white', linewidth=0.5, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(combined.mndwi > 0).isel(inundation_perc=170).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export DEM and RGB arrays to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {'driver': 'GTiff',\n",
    "         'width': xcols,\n",
    "         'height': yrows,\n",
    "         'count': 1,\n",
    "         'dtype': rasterio.float64,\n",
    "         'crs': 'EPSG:3577',\n",
    "         'transform': landsat_dataset.geobox.transform,\n",
    "         'nodata': -9999}\n",
    "\n",
    "with rasterio.open('relative_topography/{}_dem.tif'.format(name), 'w', **kwargs) as target:\n",
    "    target.write_band(1, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a single cloud free image  with low inundation\n",
    "data_array = landsat_dataset.sel(time = (landsat_dataset.data_perc > 0.8) & \n",
    "             (landsat_dataset.inundation_perc < 0.2)).mean(dim='time')[['red', 'green', 'blue']].to_array().values\n",
    "\n",
    "# Optimise colours using a percentile stretch\n",
    "rgb_array = np.transpose(data_array, [1, 2, 0])\n",
    "p_low, p_high = np.nanpercentile(rgb_array, [2, 98])\n",
    "img_toshow = exposure.rescale_intensity(rgb_array, in_range=(p_low, p_high), out_range=(0, 1))\n",
    "fig, ax = plt.subplots(figsize=(16, 16))\n",
    "ax.imshow(img_toshow)\n",
    "\n",
    "# Change dtype to int16 scaled between 0 and 10000 to save disk space\n",
    "img_toshow = (img_toshow * 10000).astype(rasterio.int16)\n",
    "\n",
    "kwargs = {'driver': 'GTiff',\n",
    "         'width': xcols,\n",
    "         'height': yrows,\n",
    "         'count': 3,\n",
    "         'dtype': rasterio.int16,\n",
    "         'crs': 'EPSG:3577',\n",
    "         'transform': landsat_dataset.geobox.transform,\n",
    "         'nodata': -9999}\n",
    "\n",
    "with rasterio.open('relative_topography/{}_rgb.tif'.format(name), 'w', **kwargs) as target:\n",
    "    target.write(np.transpose(img_toshow, [2, 0, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
